{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujan-bandla/srujan_INFO5731_Fall2022/blob/main/In_class_exercise/In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXwahJvfLUqP"
      },
      "source": [
        "## The second In-class-exercise (09/22/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchLNHfhLUqR"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezxZYNDLLUqR"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "DgGjfLQfLUqS",
        "outputId": "7ce4c4a9-18ee-4747-b5d1-f9a71b570746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here: What is the review of APPLE iPhone 13 (Starlight, 128 GB).\\nWhether it has positive reviews or negative reviews. \\nChecking the reviews of APPLE iPhone 13 and analysing it if we can purchase it or not.\\nReviews and Ratings are required to analyse the product which are given by customers who bought the APPLE iPhone 13. \\nWe are using Flipkart website for checking the reviews and this contains the reviews and rating of the product. \\nWe can analyze the reviews by considering the words and rating out in them using some common words.\\n\\nSteps for Collecting and Saving Data:\\nI have used the BeautifulSoup library to extract the information from the website.\\nI have extracted the reviews by using the classname and then appended to the empty list.\\nTo extract 1000 reviews I have itearted 40 times as each page contains 10 reviews and \\nI have generated the url dynamically while iterating\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here: What is the review of APPLE iPhone 13 (Starlight, 128 GB).\n",
        "Whether it has positive reviews or negative reviews. \n",
        "Checking the reviews of APPLE iPhone 13 and analysing it if we can purchase it or not.\n",
        "Reviews and Ratings are required to analyse the product which are given by customers who bought the APPLE iPhone 13. \n",
        "We are using Flipkart website for checking the reviews and this contains the reviews and rating of the product. \n",
        "We can analyze the reviews by considering the words and rating out in them using some common words.\n",
        "\n",
        "Steps for Collecting and Saving Data:\n",
        "I have used the BeautifulSoup library to extract the information from the website.\n",
        "I have extracted the reviews by using the classname and then appended to the empty list.\n",
        "To extract 1000 reviews I have itearted 40 times as each page contains 10 reviews and \n",
        "I have generated the url dynamically while iterating\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfx5K9VELUqT"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "x2e4eYjVLUqT",
        "outputId": "de431fdc-5b3c-49af-bb13-19e0597c3301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data frame is 1020\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Glimpse of Review Rating\n",
              "0             Brilliant      5\n",
              "1             Fabulous!      5\n",
              "2      Perfect product!      5\n",
              "3             Must buy!      5\n",
              "4     Worth every penny      5\n",
              "...                 ...    ...\n",
              "1015          Brilliant      5\n",
              "1016          Must buy!      5\n",
              "1017  Terrific purchase      5\n",
              "1018          Just wow!      5\n",
              "1019          Fabulous!      5\n",
              "\n",
              "[1020 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc31faa6-71d8-4d9f-9525-039eedf4fcb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Glimpse of Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Brilliant</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fabulous!</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Perfect product!</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Must buy!</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Worth every penny</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>Brilliant</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>Must buy!</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>Terrific purchase</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>Just wow!</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>Fabulous!</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc31faa6-71d8-4d9f-9525-039eedf4fcb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc31faa6-71d8-4d9f-9525-039eedf4fcb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc31faa6-71d8-4d9f-9525-039eedf4fcb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "main_text = [] # List to store Review headings\n",
        "sub_text =[] #List to store reviews\n",
        "for number in range(102):\n",
        "  link = \"https://www.flipkart.com/apple-iphone-13-starlight-128-gb/product-reviews/itmc9604f122ae7f?pid=MOBG6VF5ADKHKXFX&lid=LSTMOBG6VF5ADKHKXFX4LCPEV&marketplace=FLIPKART\" + str(number) # Generating link dynamically\n",
        "  page = requests.get(link) # Accessing the webpage\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "  main_reviews = soup.find_all(class_='_2-N8zT') # Getting the Review Heading by using the class name\n",
        "  text_reviews = soup.find_all(class_='_3LWZlK _1BLPMq') # Getting the full reviews by using the class name\n",
        "  for ele, sub_ele in zip(main_reviews, text_reviews) : # Iterating through the list\n",
        "      main_text.append(ele.text) #Appending to empty list\n",
        "      sub_text.append(sub_ele.text)\n",
        "apple_review_df = pd.DataFrame(list(zip(main_text, sub_text)), columns =['Glimpse of Review', 'Rating'])  # Creating Dataframe\n",
        "print(\"Length of data frame is {0}\".format(len(apple_review_df)))\n",
        "\n",
        "apple_review_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apple_review_df.to_csv('data.csv', index=False)"
      ],
      "metadata": {
        "id": "mLZjr9SsmrTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_SJgNM_LUqT"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PMqhTUl5LUqU",
        "outputId": "59a4c79b-2b1e-4173-8934-4a35db6b8527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Abstract : The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.\n",
            "\n",
            "Paper ID : 2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c \tArticle : Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems\n",
            "Author(s) : ['Aurélien Géron']\n",
            "Year : 2017 \n",
            "Abstract : Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details\n",
            "\n",
            "Paper ID : 5d433da6d0f143f20936379910104d2bb139d4ae \tArticle : ilastik: interactive machine learning for (bio)image analysis\n",
            "Author(s) : ['Stuart E. Berg', 'D. Kutra', 'Thorben Kroeger', 'C. Straehle', 'Bernhard X. Kausler', 'C. Haubold', 'Martin Schiegg', 'J. Aleš', 'T. Beier', 'Markus Rudy', 'Kemal Eren', 'Jaime I Cervantes', 'Buote Xu', 'Fynn Beuttenmueller', 'A. Wolny', 'Chong Zhang', 'U. Köthe', 'F. Hamprecht', 'A. Kreshuk']\n",
            "Year : 2019 \n",
            "Abstract : We present ilastik, an easy-to-use interactive tool that brings machine-learning-based (bio)image analysis to end users without substantial computational expertise. It contains pre-defined workflows for image segmentation, object classification, counting and tracking. Users adapt the workflows to the problem at hand by interactively providing sparse training annotations for a nonlinear classifier. ilastik can process data in up to five dimensions (3D, time and number of channels). Its computational back end runs operations on-demand wherever possible, allowing for interactive prediction on data larger than RAM. Once the classifiers are trained, ilastik workflows can be applied to new data from the command line without further user interaction. We describe all ilastik workflows in detail, including three case studies and a discussion on the expected performance.ilastik is an user-friendly interactive tool for machine-learning-based image segmentation, object classification, counting and tracking.\n",
            "\n",
            "Paper ID : f86f1748d1b6d22870f4347fd5d65314ba800583 \tArticle : Reconciling modern machine-learning practice and the classical bias–variance trade-off\n",
            "Author(s) : ['Mikhail Belkin', 'Daniel J. Hsu', 'Siyuan Ma', 'Soumik Mandal']\n",
            "Year : 2019 \n",
            "Abstract : Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.\n",
            "\n",
            "Paper ID : 3df952d4a724655f7520ff95d4b2cef90fff0cae \tArticle : Techniques for interpretable machine learning\n",
            "Author(s) : ['Mengnan Du', 'Ninghao Liu', 'Xia Hu']\n",
            "Year : 2020 \n",
            "Abstract : Uncovering the mysterious ways machine learning models make decisions.\n",
            "\n",
            "Paper ID : 1b0f4bd3872bb590d457990ac2b26b29f770fc44 \tArticle : Explainable machine learning in deployment\n",
            "Author(s) : ['Umang Bhatt', 'Alice Xiang', 'Shubham Sharma', 'Adrian Weller', 'Ankur Taly', 'Yunhan Jia', 'Joydeep Ghosh', 'R. Puri', 'J. Moura', 'P. Eckersley']\n",
            "Year : 2020 \n",
            "Abstract : Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.\n",
            "\n",
            "Paper ID : 4eca52f892f288c0b33b74aa4cfed56ed968fb4e \tArticle : Explainable Machine Learning for Scientific Insights and Discoveries\n",
            "Author(s) : ['R. Roscher', 'B. Bohn', 'Marco F. Duarte', 'J. Garcke']\n",
            "Year : 2020 \n",
            "Abstract : Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.\n",
            "\n",
            "Paper ID : 696b388ee6221c6dbcfd647a06883b2bfee773d9 \tArticle : Universal Differential Equations for Scientific Machine Learning\n",
            "Author(s) : ['Christopher Rackauckas', 'Yingbo Ma', 'Julius Martensen', 'Collin Warner', 'K. Zubov', 'R. Supekar', 'Dominic J. Skinner', 'A. Ramadhan']\n",
            "Year : 2020 \n",
            "Abstract : \n",
            " In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring \"big data\". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.\n",
            "\n",
            "Paper ID : f9a855ae59579d16dca6a5133cd8daddd3305582 \tArticle : A Survey on Distributed Machine Learning\n",
            "Author(s) : ['Joost Verbraeken', 'Matthijs Wolting', 'Jonathan Katzy', 'Jeroen Kloppenburg', 'Tim Verbelen', 'Jan S. Rellermeyer']\n",
            "Year : 2020 \n",
            "Abstract : The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.\n",
            "------------------------------------Extracting Page #7------------------------------------\n",
            "\n",
            "Paper ID : 09e85ad84c4ed40461340ac1bd5fadbd2a5b2340 \tArticle : Integrating Physics-Based Modeling with Machine Learning: A Survey\n",
            "Author(s) : ['J. Willard', 'X. Jia', 'Shaoming Xu', 'M. Steinbach', 'Vipin Kumar']\n",
            "Year : 2020 \n",
            "Abstract : In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.\n",
            "\n",
            "Paper ID : 3119ea9c7ad7a5e044dc7c267329a4bbf00d0158 \tArticle : A Survey of Optimization Methods From a Machine Learning Perspective\n",
            "Author(s) : ['Shiliang Sun', 'Zehui Cao', 'Han Zhu', 'Jing Zhao']\n",
            "Year : 2020 \n",
            "Abstract : Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.\n",
            "\n",
            "Paper ID : 7feb0fc888cd55360949554db032d7d1cba9e947 \tArticle : Programs for Machine Learning\n",
            "Author(s) : ['S. Salzberg', 'Alberto Maria Segre']\n",
            "Year : 1994 \n",
            "Abstract : Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.\n",
            "\n",
            "Paper ID : f70b2f20be241f445a61f33c4b8e76e554760340 \tArticle : Software Engineering for Machine Learning: A Case Study\n",
            "Author(s) : ['Saleema Amershi', 'A. Begel', 'C. Bird', 'R. DeLine', 'H. Gall', 'Ece Kamar', 'Nachiappan Nagappan', 'Besmira Nushi', 'T. Zimmermann']\n",
            "Year : 2019 \n",
            "Abstract : Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.\n",
            "\n",
            "Paper ID : adade3149b2a6177296de352f003471eefa958b8 \tArticle : The Impact of Machine Learning on Economics\n",
            "Author(s) : ['S. Athey']\n",
            "Year : 2019 \n",
            "Abstract : This paper provides an assessment of the early contributions of machine learning to economics, as well as predictions about its future contributions. It begins by briefly overviewing some themes from the literature on machine learning, and then draws some contrasts with traditional approaches to estimating the impact of counterfactual policies in economics. Next, we review some of the initial “off-the-shelf” applications of machine learning to economics, including applications in analyzing text and images. We then describe new types of questions that have been posed surrounding the application of machine learning to policy problems, including “prediction policy problems,” as well as considerations of fairness and manipulability. We present some highlights from the emerging econometric literature combining machine learning and causal inference. Finally, we overview a set of broader predictions about the future impact of machine learning on economics, including its impacts on the nature of collaboration, funding, research tools, and research questions.\n",
            "\n",
            "Paper ID : b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0 \tArticle : Machine learning for molecular simulation\n",
            "Author(s) : ['F. Noé', 'A. Tkatchenko', 'K. Müller', 'C. Clementi']\n",
            "Year : 2020 \n",
            "Abstract : Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation. Expected final online publication date for the Annual Review of Physical Chemistry, Volume 71 is April 20, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\n",
            "\n",
            "Paper ID : 739769f4862753fc80057194456d758d2a148ee3 \tArticle : Extreme Learning Machine for Regression and Multiclass Classification\n",
            "Author(s) : ['G. Huang', 'Hongming Zhou', 'Xiaojian Ding', 'Rui Zhang']\n",
            "Year : 2012 \n",
            "Abstract : Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.\n",
            "\n",
            "Paper ID : fee8f63972906214b77f16cfeca0b93ee8f36ba2 \tArticle : Fairness in Machine Learning: A Survey\n",
            "Author(s) : ['Simon Caton', 'C. Haas']\n",
            "Year : 2020 \n",
            "Abstract : As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.\n",
            "\n",
            "Paper ID : b6c4c7477d88fc6fa2cf04f5ece7b34b8e6f0eb6 \tArticle : Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent\n",
            "Author(s) : ['Yudong Chen', 'Lili Su', 'Jiaming Xu']\n",
            "Year : 2019 \n",
            "Abstract : We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.\n",
            "\n",
            "Paper ID : 7ae2783a9196fb4bc2a610ae812d19722daddce5 \tArticle : Applications of machine learning to machine fault diagnosis: A review and roadmap\n",
            "Author(s) : ['Y. Lei', 'Bin Yang', 'Xinwei Jiang', 'Feng Jia', 'Naipeng Li', 'A. Nandi']\n",
            "Year : 2020 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #8------------------------------------\n",
            "\n",
            "Paper ID : d1e701665e73faa648cb15473952576f40e8e122 \tArticle : The Machine‐Learning Approach\n",
            "Author(s) : ['N. Srivastava', 'C. Verma', 'Rabia Aziz Musheer']\n",
            "Year : 2020 \n",
            "Abstract : For past several years, microarray technology has attracted tremendous interest for both scientific community and industry. Recently, the applications of microarrays include gene discovery, disease diagnosis and prognosis, drug discovery, etc. High dimensional data with small sample size is the main problem that generate the application of dimension reduction in microarray data analysis. It is seen that SVM, ANN and NB have recently gained wide popularity for cancer classification problems. An efficient and reliable method of dimension reduction plays an important role to improve the performance of SVM, ANN and NB, when applied for classification of high dimensional microarray data. In this book, we applied different combinations of feature selection / extraction methods, as a novel hybrid dimension reduction method for SVM, ANN and NB classifiers. The obtained results are compared with other popular published dimension reduction methods for SVM, NB and ANN classifiers.\n",
            "\n",
            "Paper ID : 9e27190f2d9b2167d4a66b88696def4585072fd5 \tArticle : SoilGrids250m: Global gridded soil information based on machine learning\n",
            "Author(s) : ['T. Hengl', 'Jorge Mendes de Jesus', 'G. Heuvelink', 'Maria Ruiperez González', 'M. Kilibarda', 'Aleksandar Blagotić', 'W. Shangguan', 'Marvin N. Wright', 'X. Geng', 'B. Bauer-Marschallinger', 'M. Guevara', 'R. Vargas', 'R. MacMillan', 'N. Batjes', 'J. Leenaars', 'E. Ribeiro', 'I. Wheeler', 'S. Mantel', 'B. Kempen']\n",
            "Year : 2017 \n",
            "Abstract : This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.\n",
            "\n",
            "Paper ID : 26ae952599aa9ba5815a80356024258247fc2b10 \tArticle : Data mining: practical machine learning tools and techniques with Java implementations\n",
            "Author(s) : ['I. Witten', 'Eibe Frank']\n",
            "Year : 2002 \n",
            "Abstract : 1. What's It All About? 2. Input: Concepts, Instances, Attributes 3. Output: Knowledge Representation 4. Algorithms: The Basic Methods 5. Credibility: Evaluating What's Been Learned 6. Implementations: Real Machine Learning Schemes 7. Moving On: Engineering The Input And Output 8. Nuts And Bolts: Machine Learning Algorithms In Java 9. Looking Forward\n",
            "\n",
            "Paper ID : e0408181bccb7e3754dd5e6785ec47d8beb8b6bd \tArticle : Machine Learning for High-Speed Corner Detection\n",
            "Author(s) : ['E. Rosten', 'T. Drummond']\n",
            "Year : 2006 \n",
            "Abstract : Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time. By comparison neither the Harris detector (120%) nor the detection stage of SIFT (300%) can operate at full frame rate. \n",
            " \n",
            "Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.\n",
            "\n",
            "Paper ID : db0cc2f21b20cbc0ab8946090967399c25709614 \tArticle : Practical Secure Aggregation for Privacy-Preserving Machine Learning\n",
            "Author(s) : ['Keith Bonawitz', 'Vladimir Ivanov', 'Ben Kreuter', 'Antonio Marcedone', 'H. B. McMahan', 'Sarvar Patel', 'D. Ramage', 'Aaron Segal', 'Karn Seth']\n",
            "Year : 2017 \n",
            "Abstract : We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.\n",
            "\n",
            "Paper ID : da118b8aa99699edd7609fbbd081d5b93bc2e87b \tArticle : Automatic differentiation in machine learning: a survey\n",
            "Author(s) : ['A. G. Baydin', 'Barak A. Pearlmutter', 'Alexey Radul', 'J. Siskind']\n",
            "Year : 2017 \n",
            "Abstract : Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply “auto-diff”, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until \n",
            "very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other’s results. Despite its \n",
            "relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names “dynamic computational \n",
            "graphs” and “differentiable programming”. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main imple- \n",
            "mentation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms “autodiff”, “automatic differentiation”, and “symbolic differentiation” as these are encountered more and more in machine learning settings.\n",
            "\n",
            "Paper ID : c292e473b3825eeb9db03c70b2e1c033aea190d5 \tArticle : Machine learning for molecular and materials science\n",
            "Author(s) : ['K. Butler', 'Daniel W. Davies', 'H. Cartwright', 'O. Isayev', 'A. Walsh']\n",
            "Year : 2018 \n",
            "Abstract : Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.Recent progress in machine learning in the chemical sciences and future directions in this field are discussed.\n",
            "\n",
            "Paper ID : f2e07ccce9a89f7c018bd62d390240e8ee0b0045 \tArticle : Machine learning with Python\n",
            "Author(s) : ['Pedro Ferreira', 'C. Simons']\n",
            "Year : 2017 \n",
            "Abstract : This presentation is a case study taken from the travel and holiday industry. Paxport/Multicom, based in UK and Sweden, have recently adopted a recommendation system for holiday accommodation bookings. Machine learning techniques such as Collaborative Filtering have been applied using Python (3.5.1), with Jupyter (4.0.6) as the main framework. Data scale and sparsity present significant challenges in the case study, and so the effectiveness of various techniques are described as well as the performance of Python-based libraries such as Python Data Analysis Library (Pandas), and Scikit-learn (built on NumPy, SciPy and matplotlib). The presentation is suitable for all levels of programmers.\n",
            "\n",
            "Paper ID : 47f2f60995e4393ad689233ee870e1e0707d4d60 \tArticle : Automated Machine Learning: Methods, Systems, Challenges\n",
            "Author(s) : ['F. Hutter', 'Lars Kotthoff', 'J. Vanschoren']\n",
            "Year : 2019 \n",
            "Abstract : This open access book presents the first comprehensive overview of general methods in Automatic Machine Learning (AutoML), collects descriptions of existing systems based on these methods, and discusses the first international challenge of AutoML systems. The book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use AutoML in their work. The recent success of commercial ML applications and the rapid growth of the field has created a high demand for off-the-shelf ML methods that can be used easily and without expert knowledge. Many of the recent machine learning successes crucially rely on human experts, who select appropriate ML architectures (deep learning architectures or more traditional ML workflows) and their hyperparameters; however the field of AutoML targets a progressive automation of machine learning, based on principles from optimization and machine learning itself.\n",
            "\n",
            "Paper ID : 7e7eb0f93c9550d7336f4bbfad5fe89604295705 \tArticle : Quantum Machine Learning in Feature Hilbert Spaces.\n",
            "Author(s) : ['M. Schuld', 'N. Killoran']\n",
            "Year : 2019 \n",
            "Abstract : A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.\n",
            "------------------------------------Extracting Page #9------------------------------------\n",
            "\n",
            "Paper ID : f75b70c9d7078724b592ec3e21de705e7b6ff73f \tArticle : Double/Debiased Machine Learning for Treatment and Structural Parameters\n",
            "Author(s) : ['V. Chernozhukov', 'D. Chetverikov', 'Mert Demirer', 'E. Duflo', 'Christian Hansen', 'W. Newey', 'J. Robins']\n",
            "Year : 2017 \n",
            "Abstract : We revisit the classic semiparametric problem of inference on a low dimensional parameter θ_0 in the presence of high-dimensional nuisance parameters η_0. We depart from the classical setting by allowing for η_0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate η_0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η_0 cause a heavy bias in estimators of θ_0 that are obtained by naively plugging ML estimators of η_0 into estimating equations for θ_0. This bias results in the naive estimator failing to be N^(-1/2) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ_0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ_0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N^(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.\n",
            "\n",
            "Paper ID : 1b225474e7a5794f98cdfbde8b12ccbc56799409 \tArticle : Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\n",
            "Author(s) : ['Wieland Brendel', 'Jonas Rauber', 'M. Bethge']\n",
            "Year : 2018 \n",
            "Abstract : Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .\n",
            "\n",
            "Paper ID : caf9e0fa2c340fb07cef8d547ea8849508e5c358 \tArticle : Empirical Asset Pricing Via Machine Learning\n",
            "Author(s) : ['Shihao Gu', 'B. Kelly', 'D. Xiu']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            " We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.\n",
            " Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.\n",
            "\n",
            "Paper ID : 0522fdd8efe0e4baa5602d1384047a5705613765 \tArticle : Supervised Machine Learning: A Review of Classification Techniques\n",
            "Author(s) : ['S. Kotsiantis']\n",
            "Year : 2007 \n",
            "Abstract : The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.\n",
            "\n",
            "Paper ID : c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7 \tArticle : Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting\n",
            "Author(s) : ['Samuel Yeom', 'Irene Giacomelli', 'Matt Fredrikson', 'S. Jha']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.\n",
            "\n",
            "Paper ID : 262c0e54370dfc03a7ad53d79930568d18dd448c \tArticle : Speeding Up Distributed Machine Learning Using Codes\n",
            "Author(s) : ['Kangwook Lee', 'Maximilian Lam', 'Ramtin Pedarsani', 'Dimitris Papailiopoulos', 'K. Ramchandran']\n",
            "Year : 2018 \n",
            "Abstract : Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({\\alpha + \\frac {1}{n}}\\right)\\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n) \\simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.\n",
            "\n",
            "Paper ID : d75356e2bf674902a06a14bb55d18ee88af5b4bb \tArticle : Machine Learning Methods That Economists Should Know About\n",
            "Author(s) : ['S. Athey', 'G. Imbens']\n",
            "Year : 2019 \n",
            "Abstract : We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.\n",
            "\n",
            "Paper ID : d7701e78e0bfc92b03a89582e80cfb751ac03f26 \tArticle : Explaining Explanations: An Overview of Interpretability of Machine Learning\n",
            "Author(s) : ['Leilani H. Gilpin', 'David Bau', 'Ben Z. Yuan', 'Ayesha Bajwa', 'Michael A. Specter', 'Lalana Kagal']\n",
            "Year : 2018 \n",
            "Abstract : There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.\n",
            "\n",
            "Paper ID : 61306b52c2d292928f7cbb2f2ef5711d15a2566c \tArticle : ABY3: A Mixed Protocol Framework for Machine Learning\n",
            "Author(s) : ['Payman Mohassel', 'Peter Rindal']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning is widely used to produce models for a range of applications and is increasingly offered as a service by major technology companies. However, the required massive data collection raises privacy concerns during both training and prediction stages. In this paper, we design and implement a general framework for privacy-preserving machine learning and use it to obtain new solutions for training linear regression, logistic regression and neural network models. Our protocols are in a three-server model wherein data owners secret share their data among three servers who train and evaluate models on the joint data using three-party computation (3PC). Our main contribution is a new and complete framework ($\\textABY ^3$) for efficiently switching back and forth between arithmetic, binary, and Yao 3PC which is of independent interest. Many of the conversions are based on new techniques that are designed and optimized for the first time in this paper. We also propose new techniques for fixed-point multiplication of shared decimal values that extends beyond the three-party case, and customized protocols for evaluating piecewise polynomial functions. We design variants of each building block that is secure against \\em malicious adversaries who deviate arbitrarily. We implement our system in C++. Our protocols are up to \\em four orders of magnitude faster than the best prior work, hence significantly reducing the gap between privacy-preserving and plaintext training.\n",
            "\n",
            "Paper ID : 2b7f9117eb6608a58be4c078ca3d69c0e5ccb875 \tArticle : SecureML: A System for Scalable Privacy-Preserving Machine Learning\n",
            "Author(s) : ['Payman Mohassel', 'Yupeng Zhang']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.\n",
            "------------------------------------Extracting Page #10------------------------------------\n",
            "\n",
            "Paper ID : 0273507eb05f1135f3a05f9c7adc9a56f12c7c5c \tArticle : Recent advances and applications of machine learning in solid-state materials science\n",
            "Author(s) : ['Jonathan Schmidt', 'Mário R. G. Marques', 'S. Botti', 'M. Marques']\n",
            "Year : 2019 \n",
            "Abstract : One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure–property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.\n",
            "\n",
            "Paper ID : 48ddd9101a90fe65e3061de69626741b843ff5e4 \tArticle : The use of the area under the ROC curve in the evaluation of machine learning algorithms\n",
            "Author(s) : ['A. Bradley']\n",
            "Year : 1997 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 6bc43977fb11cceed0b9aa55b23c6dd29dd9a132 \tArticle : Correlation-based Feature Selection for Machine Learning\n",
            "Author(s) : ['M. Hall']\n",
            "Year : 2003 \n",
            "Abstract : A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. On natural domains, CFS typically eliminated well over half the features. In most cases, classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set. Feature selection degraded machine learning performance in cases where some features were eliminated which were highly predictive of very small areas of the instance space. Further experiments compared CFS with a wrapper—a well known approach to feature selection that employs the target learning algorithm to evaluate feature sets. In many cases CFS gave comparable results to the wrapper, and in general, outperformed the wrapper on small datasets. CFS executes many times faster than the wrapper, which allows it to scale to larger datasets. Two methods of extending CFS to handle feature interaction are presented and experimentally evaluated. The first considers pairs of features and the second incorporates iii feature weights calculated by the RELIEF algorithm. Experiments on artificial domains showed that both methods were able to identify interacting features. On natural domains, the pairwise method gave more reliable results than using weights provided by RELIEF.\n",
            "\n",
            "Paper ID : b5904cd5dbf73b8d5ff13517de490c292d877ee0 \tArticle : Applications of machine learning in drug discovery and development\n",
            "Author(s) : ['J. Vamathevan', 'D. Clark', 'Paul Czodrowski', 'I. Dunham', 'Edgardo Ferran', 'George Lee', 'Bin Li', 'A. Madabhushi', 'P. Shah', 'M. Spitzer', 'Shanrong Zhao']\n",
            "Year : 2019 \n",
            "Abstract : Drug discovery and development pipelines are long, complex and depend on numerous factors. Machine learning (ML) approaches provide a set of tools that can improve discovery and decision making for well-specified questions with abundant, high-quality data. Opportunities to apply ML occur in all stages of drug discovery. Examples include target validation, identification of prognostic biomarkers and analysis of digital pathology data in clinical trials. Applications have ranged in context and methodology, with some approaches yielding accurate predictions and insights. The challenges of applying ML lie primarily with the lack of interpretability and repeatability of ML-generated results, which may limit their application. In all areas, systematic and comprehensive high-dimensional data still need to be generated. With ongoing efforts to tackle these issues, as well as increasing awareness of the factors needed to validate ML approaches, the application of ML can promote data-driven decision making and has the potential to speed up the process and reduce failure rates in drug discovery and development.Machine learning has been applied to numerous stages in the drug discovery pipeline. Here, Vamathevan and colleagues discuss the most useful techniques and how machine learning can promote data-driven decision making in drug discovery and development. They highlight major hurdles in the field, such as the required data characteristics for applying machine learning, which will need to be solved as machine learning matures.\n",
            "\n",
            "Paper ID : a9cbbef8f4426329d0687025b34287c35bdd8b38 \tArticle : Machine learning and the physical sciences\n",
            "Author(s) : ['G. Carleo', 'I. Cirac', 'K. Cranmer', 'L. Daudet', 'M. Schuld', 'Naftali Tishby', 'Leslie Vogt-Maranto', \"Lenka Zdeborov'a\"]\n",
            "Year : 2019 \n",
            "Abstract : Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.\n",
            "\n",
            "Paper ID : 62df84d6a4d26f95e4714796c2337c9848cc13b5 \tArticle : MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems\n",
            "Author(s) : ['Tianqi Chen', 'Mu Li', 'Yutian Li', 'Min Lin', 'Naiyan Wang', 'Minjie Wang', 'Tianjun Xiao', 'Bing Xu', 'Chiyuan Zhang', 'Zheng Zhang']\n",
            "Year : 2015 \n",
            "Abstract : MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. \n",
            "This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.\n",
            "\n",
            "Paper ID : 3b900b13a8ae7814c1fb00960ef1da66c4580859 \tArticle : Ensemble Methods in Machine Learning\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 2000 \n",
            "Abstract : Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.\n",
            "\n",
            "Paper ID : 21dfbc88b21b27fe8a245ab1df98edd45f655ae7 \tArticle : Machine Learning in Medicine\n",
            "Author(s) : ['A. Rajkomar', 'Jeffrey Dean', 'I. Kohane']\n",
            "Year : 2019 \n",
            "Abstract : Machine Learning in Medicine In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. The...\n",
            "\n",
            "Paper ID : 46c266b3d1274dacd7fce27ee8cb4d587f087a58 \tArticle : Machine Learning Interpretability: A Survey on Methods and Metrics\n",
            "Author(s) : ['D. V. Carvalho', 'E. M. Pereira', 'Jaime S. Cardoso']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.\n",
            "\n",
            "Paper ID : 57fdd4d1b69e7640c54eef4e8fbba2beb6449019 \tArticle : Definitions, methods, and applications in interpretable machine learning\n",
            "Author(s) : ['W. James Murdoch', 'Chandan Singh', 'Karl Kumbier', 'R. Abbasi-Asl', 'Bin Yu']\n",
            "Year : 2019 \n",
            "Abstract : Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.\n",
            "------------------------------------Extracting Page #11------------------------------------\n",
            "\n",
            "Paper ID : 57fdd4d1b69e7640c54eef4e8fbba2beb6449019 \tArticle : Definitions, methods, and applications in interpretable machine learning\n",
            "Author(s) : ['W. James Murdoch', 'Chandan Singh', 'Karl Kumbier', 'R. Abbasi-Asl', 'Bin Yu']\n",
            "Year : 2019 \n",
            "Abstract : Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.\n",
            "\n",
            "Paper ID : 46c266b3d1274dacd7fce27ee8cb4d587f087a58 \tArticle : Machine Learning Interpretability: A Survey on Methods and Metrics\n",
            "Author(s) : ['D. V. Carvalho', 'E. M. Pereira', 'Jaime S. Cardoso']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.\n",
            "\n",
            "Paper ID : c8ac6060d34179871b81ecd19621c63360347f8e \tArticle : Comparing different supervised machine learning algorithms for disease prediction\n",
            "Author(s) : ['S. Uddin', 'Arif Khan', 'Md Ekramul Hossain', 'M. Moni']\n",
            "Year : 2019 \n",
            "Abstract : BackgroundSupervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study ai7ms to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction.MethodsIn this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction.ResultsWe found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naïve Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53%. This was followed by SVM which topped in 41% of the studies it was considered.ConclusionThis study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.\n",
            "\n",
            "Paper ID : 998039a4876edc440e0cabb0bc42239b0eb29644 \tArticle : Tackling Climate Change with Machine Learning\n",
            "Author(s) : ['D. Rolnick', 'P. Donti', 'L. Kaack', 'K. Kochanski', 'Alexandre Lacoste', 'K. Sankaran', 'A. Ross', 'Nikola Milojevic-Dupont', 'Natasha Jaques', 'Anna Waldman-Brown', 'A. Luccioni', 'Tegan Maharaj', 'Evan D. Sherwin', 'S. Mukkavilli', 'Konrad Paul Kording', 'Carla P. Gomes', 'Andrew Y. Ng', 'D. Hassabis', 'John C. Platt', 'F. Creutzig', 'J. Chayes', 'Yoshua Bengio']\n",
            "Year : 2019 \n",
            "Abstract : Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.\n",
            "\n",
            "Paper ID : 730fba26faa7f91dc6742a0c3521eb439670a825 \tArticle : Machine-learning-guided directed evolution for protein engineering\n",
            "Author(s) : ['Kevin Kaichuang Yang', 'Zachary Wu', 'F. Arnold']\n",
            "Year : 2019 \n",
            "Abstract : Protein engineering through machine-learning-guided directed evolution enables the optimization of protein functions. Machine-learning approaches predict how sequence maps to function in a data-driven manner without requiring a detailed model of the underlying physics or biological pathways. Such methods accelerate directed evolution by learning from the properties of characterized variants and using that information to select sequences that are likely to exhibit improved properties. Here we introduce the steps required to build machine-learning sequence–function models and to use those models to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to the use of machine learning for protein engineering, as well as the current literature and applications of this engineering paradigm. We illustrate the process with two case studies. Finally, we look to future opportunities for machine learning to enable the discovery of unknown protein functions and uncover the relationship between protein sequence and function.This review provides an overview of machine learning techniques in protein engineering and illustrates the underlying principles with the help of case studies.\n",
            "\n",
            "Paper ID : 638e41912f314c74436205aa8d332dca963ab1dc \tArticle : Parameterized quantum circuits as machine learning models\n",
            "Author(s) : ['M. Benedetti', 'Erika Lloyd', 'Stefan H. Sack', 'Mattia Fiorentini']\n",
            "Year : 2019 \n",
            "Abstract : Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.\n",
            "\n",
            "Paper ID : ac644a74a0ebc8cfbe1b0af8120004909828d283 \tArticle : Adversarial attacks on medical machine learning\n",
            "Author(s) : ['S. G. Finlayson', 'John Bowers', 'Joichi Ito', 'J. Zittrain', 'Andrew Beam', 'I. Kohane']\n",
            "Year : 2019 \n",
            "Abstract : Emerging vulnerabilities demand new conversations With public and academic attention increasingly focused on the new role of machine learning in the health information economy, an unusual and no-longer-esoteric category of vulnerabilities in machine-learning systems could prove important. These vulnerabilities allow a small, carefully designed change in how inputs are presented to a system to completely alter its output, causing it to confidently arrive at manifestly wrong conclusions. These advanced techniques to subvert otherwise-reliable machine-learning systems—so-called adversarial attacks—have, to date, been of interest primarily to computer science researchers (1). However, the landscape of often-competing interests within health care, and billions of dollars at stake in systems' outputs, implies considerable problems. We outline motivations that various players in the health care system may have to use adversarial attacks and begin a discussion of what to do about them. Far from discouraging continued innovation with medical machine learning, we call for active engagement of medical, technical, legal, and ethical experts in pursuit of efficient, broadly available, and effective health care that machine learning will enable.\n",
            "\n",
            "Paper ID : fbf9812f29156024ec693b4633a21303eead309d \tArticle : Machine learning algorithm validation with a limited sample size\n",
            "Author(s) : ['Andrius Vabalas', 'E. Gowen', 'E. Poliakoff', 'A. Casson']\n",
            "Year : 2019 \n",
            "Abstract : Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.\n",
            "\n",
            "Paper ID : 5d093bd376ba63495ea442241bc8bc2f0ff30c2b \tArticle : Machine learning in medicine: a practical introduction\n",
            "Author(s) : ['Jenni A. M. Sidey-Gibbons', 'C. Sidey-Gibbons']\n",
            "Year : 2019 \n",
            "Abstract : BackgroundFollowing visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data.MethodsWe demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples.We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment.ResultsThe trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble.ConclusionsWe use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.\n",
            "\n",
            "Paper ID : a89fa53a26b66fcbac7dd2e3524715eeb284f513 \tArticle : Do no harm: a roadmap for responsible machine learning for health care\n",
            "Author(s) : ['J. Wiens', 'S. Saria', 'M. Sendak', 'M. Ghassemi', 'V. Liu', 'Finale Doshi-Velez', 'K. Jung', 'K. Heller', 'David C. Kale', 'Mohammed Saeed', 'P. Ossorio', 'Sonoo Thadaney-Israni', 'A. Goldenberg']\n",
            "Year : 2019 \n",
            "Abstract : Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).In this Perspective, the authors present a framework, context and guidelines for accelerating the translation of machine-learning-based interventions in health care.\n",
            "------------------------------------Extracting Page #12------------------------------------\n",
            "\n",
            "Paper ID : d294d5246e0dd8ed8bd9ec9d24a01fd4ece4fb3c \tArticle : A Detailed Investigation and Analysis of Using Machine Learning Techniques for Intrusion Detection\n",
            "Author(s) : ['P. Mishra', 'V. Varadharajan', 'U. Tupakula', 'E. Pilli']\n",
            "Year : 2019 \n",
            "Abstract : Intrusion detection is one of the important security problems in todays cyber world. A significant number of techniques have been developed which are based on machine learning approaches. However, they are not very successful in identifying all types of intrusions. In this paper, a detailed investigation and analysis of various machine learning techniques have been carried out for finding the cause of problems associated with various machine learning techniques in detecting intrusive activities. Attack classification and mapping of the attack features is provided corresponding to each attack. Issues which are related to detecting low-frequency attacks using network attack dataset are also discussed and viable methods are suggested for improvement. Machine learning techniques have been analyzed and compared in terms of their detection capability for detecting the various category of attacks. Limitations associated with each category of them are also discussed. Various data mining tools for machine learning have also been included in the paper. At the end, future directions are provided for attack detection using machine learning techniques.\n",
            "\n",
            "Paper ID : 241d9b16e09f77368797c44493eafbadd675da5b \tArticle : Fairness in Machine Learning\n",
            "Author(s) : ['L. Oneto', 'S. Chiappa']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning based systems are reaching society at large and in many aspects of everyday life. This phenomenon has been accompanied by concerns about the ethical issues that may arise from the adoption of these technologies. ML fairness is a recently established area of machine learning that studies how to ensure that biases in the data and model inaccuracies do not lead to models that treat individuals unfavorably on the basis of characteristics such as e.g. race, gender, disabilities, and sexual or political orientation. In this manuscript, we discuss some of the limitations present in the current reasoning about fairness and in methods that deal with it, and describe some work done by the authors to address them. More specifically, we show how causal Bayesian networks can play an important role to reason about and deal with fairness, especially in complex unfairness scenarios. We describe how optimal transport theory can be leveraged to develop methods that impose constraints on the full shapes of distributions corresponding to different sensitive attributes, overcoming the limitation of most approaches that approximate fairness desiderata by imposing constraints on the lower order moments or other functions of those distributions. We present a unified framework that encompasses methods that can deal with different settings and fairness criteria, and that enjoys strong theoretical guarantees. We introduce an approach to learn fair representations that can generalize to unseen tasks. Finally, we describe a technique that accounts for legal restrictions about the use of sensitive attributes.\n",
            "\n",
            "Paper ID : a42e380e1b8aafecb3b1e338a8a9a579c6a5a40f \tArticle : A Survey of Machine Learning Techniques Applied to Software Defined Networking (SDN): Research Issues and Challenges\n",
            "Author(s) : ['Jun-feng Xie', 'F. Yu', 'Tao Huang', 'Renchao Xie', 'Jiang Liu', 'Chen-meng Wang', 'Yunjie Liu']\n",
            "Year : 2019 \n",
            "Abstract : In recent years, with the rapid development of current Internet and mobile communication technologies, the infrastructure, devices and resources in networking systems are becoming more complex and heterogeneous. In order to efficiently organize, manage, maintain and optimize networking systems, more intelligence needs to be deployed. However, due to the inherently distributed feature of traditional networks, machine learning techniques are hard to be applied and deployed to control and operate networks. Software defined networking (SDN) brings us new chances to provide intelligence inside the networks. The capabilities of SDN (e.g., logically centralized control, global view of the network, software-based traffic analysis, and dynamic updating of forwarding rules) make it easier to apply machine learning techniques. In this paper, we provide a comprehensive survey on the literature involving machine learning algorithms applied to SDN. First, the related works and background knowledge are introduced. Then, we present an overview of machine learning algorithms. In addition, we review how machine learning algorithms are applied in the realm of SDN, from the perspective of traffic classification, routing optimization, quality of service/quality of experience prediction, resource management and security. Finally, challenges and broader perspectives are discussed.\n",
            "\n",
            "Paper ID : 67df7bf02fe2d618c7c18448c2668a526dc4d423 \tArticle : Machine Learning at Facebook: Understanding Inference at the Edge\n",
            "Author(s) : ['Carole-Jean Wu', 'D. Brooks', 'Kevin Chen', 'Douglas Chen', 'Sy Choudhury', 'Marat Dukhan', 'Kim M. Hazelwood', 'Eldad Isaac', 'Yangqing Jia', 'Bill Jia', 'Tommer Leyvand', 'Hao Lu', 'Yang Lu', 'Lin Qiao', 'Brandon Reagen', 'Joe Spisak', 'Fei Sun', 'Andrew Tulloch', 'Péter Vajda', 'Xiaodong Wang', 'Yanghan Wang', 'Bram Wasti', 'Yiming Wu', 'Ran Xian', 'S. Yoo', 'Peizhao Zhang']\n",
            "Year : 2019 \n",
            "Abstract : At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized datacenter infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a datadriven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms.\n",
            "\n",
            "Paper ID : 97f4a6f87f258053f2677504647696f1803c6794 \tArticle : How to Read Articles That Use Machine Learning: Users' Guides to the Medical Literature.\n",
            "Author(s) : ['Yun Liu', 'Po-Hsuan Cameron Chen', 'Jonathan Krause', 'L. Peng']\n",
            "Year : 2019 \n",
            "Abstract : In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.\n",
            "\n",
            "Paper ID : e67121cd31e95fba6c892724e619323ad7564b03 \tArticle : A Survey of Deep Learning and Its Applications: A New Paradigm to Machine Learning\n",
            "Author(s) : ['Shaveta Dargan', 'Munish Kumar', 'M. Ayyagari', 'G. Kumar']\n",
            "Year : 2019 \n",
            "Abstract : AbstractNowadays, deep learning is a current and a stimulating field of machine learning. Deep learning is the most effective, supervised, time and cost efficient machine learning approach. Deep learning is not a restricted learning approach, but it abides various procedures and topographies which can be applied to an immense speculum of complicated problems. The technique learns the illustrative and differential features in a very stratified way. Deep learning methods have made a significant breakthrough with appreciable performance in a wide variety of applications with useful security tools. It is considered to be the best choice for discovering complex architecture in high-dimensional data by employing back propagation algorithm. As deep learning has made significant advancements and tremendous performance in numerous applications, the widely used domains of deep learning are business, science and government which further includes adaptive testing, biological image classification, computer vision, cancer detection, natural language processing, object detection, face recognition, handwriting recognition, speech recognition, stock market analysis, smart city and many more. This paper focuses on the concepts of deep learning, its basic and advanced architectures, techniques, motivational aspects, characteristics and the limitations. The paper also presents the major differences between the deep learning, classical machine learning and conventional learning approaches and the major challenges ahead. The main intention of this paper is to explore and present chronologically, a comprehensive survey of the major applications of deep learning covering variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects.\n",
            "\n",
            "\n",
            "Paper ID : 05c5b732fb92546c7d6eeabfadb5c14610d07373 \tArticle : Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning\n",
            "Author(s) : ['G. Lemaître', 'Fernando Nogueira', 'Christos K. Aridas']\n",
            "Year : 2017 \n",
            "Abstract : Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: this https URL.\n",
            "\n",
            "Paper ID : 3784b73a1f392160523400ec0309191c0a96d86f \tArticle : MLlib: Machine Learning in Apache Spark\n",
            "Author(s) : ['Xiangrui Meng', 'Joseph K. Bradley', 'Burak Yavuz', 'Evan R. Sparks', 'S. Venkataraman', 'Davies Liu', 'Jeremy Freeman', 'D. B. Tsai', 'Manish Amde', 'Sean Owen', 'Doris Xin', 'Reynold Xin', 'M. Franklin', 'R. Zadeh', 'M. Zaharia', 'Ameet S. Talwalkar']\n",
            "Year : 2016 \n",
            "Abstract : Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.\n",
            "\n",
            "Paper ID : 18aaf8664bbae17a7d20bcb422c36e5d52201aa5 \tArticle : Machine learning-assisted directed protein evolution with combinatorial libraries\n",
            "Author(s) : ['Zachary Wu', 'S. Kan', 'Russell D. Lewis', 'Bruce J. Wittmann', 'F. Arnold']\n",
            "Year : 2019 \n",
            "Abstract : Significance Proteins often function poorly when used outside their natural contexts; directed evolution can be used to engineer them to be more efficient in new roles. We propose that the expense of experimentally testing a large number of protein variants can be decreased and the outcome can be improved by incorporating machine learning with directed evolution. Simulations on an empirical fitness landscape demonstrate that the expected performance improvement is greater with this approach. Machine learning-assisted directed evolution from a single parent produced enzyme variants that selectively synthesize the enantiomeric products of a new-to-nature chemical transformation. By exploring multiple mutations simultaneously, machine learning efficiently navigates large regions of sequence space to identify improved proteins and also produces diverse solutions to engineering problems. To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning into the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine-learning models trained on tested variants provide a fast method for testing sequence space computationally. We validated this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (i.e., stereodivergence) of a new-to-nature carbene Si–H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93% and 79% ee (enantiomeric excess). By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.\n",
            "\n",
            "Paper ID : 4f2b9cb774489c1a600c224c75edb8da07a24064 \tArticle : Machine-learning reprogrammable metasurface imager\n",
            "Author(s) : ['Lianlin Li', 'Hengxin Ruan', 'Che Liu', 'Ying Li', 'Ya Shuang', 'A. Alú', 'C. Qiu', 'T. Cui']\n",
            "Year : 2019 \n",
            "Abstract : Conventional microwave imagers usually require either time-consuming data acquisition, or complicated reconstruction algorithms for data post-processing, making them largely ineffective for complex in-situ sensing and monitoring. Here, we experimentally report a real-time digital-metasurface imager that can be trained in-situ to generate the radiation patterns required by machine-learning optimized measurement modes. This imager is electronically reprogrammed in real time to access the optimized solution for an entire data set, realizing storage and transfer of full-resolution raw data in dynamically varying scenes. High-accuracy image coding and recognition are demonstrated in situ for various image sets, including hand-written digits and through-wall body gestures, using a single physical hardware imager, reprogrammed in real time. Our electronically controlled metasurface imager opens new venues for intelligent surveillance, fast data acquisition and processing, imaging at various frequencies, and beyond.Conventional imagers require time-consuming data acquisition, or complicated reconstruction algorithms for data post-processing. Here, the authors demonstrate a real-time digital-metasurface imager that can be trained in-situ to show high accuracy image coding and recognition for various image sets.\n",
            "------------------------------------Extracting Page #13------------------------------------\n",
            "\n",
            "Paper ID : 4f2b9cb774489c1a600c224c75edb8da07a24064 \tArticle : Machine-learning reprogrammable metasurface imager\n",
            "Author(s) : ['Lianlin Li', 'Hengxin Ruan', 'Che Liu', 'Ying Li', 'Ya Shuang', 'A. Alú', 'C. Qiu', 'T. Cui']\n",
            "Year : 2019 \n",
            "Abstract : Conventional microwave imagers usually require either time-consuming data acquisition, or complicated reconstruction algorithms for data post-processing, making them largely ineffective for complex in-situ sensing and monitoring. Here, we experimentally report a real-time digital-metasurface imager that can be trained in-situ to generate the radiation patterns required by machine-learning optimized measurement modes. This imager is electronically reprogrammed in real time to access the optimized solution for an entire data set, realizing storage and transfer of full-resolution raw data in dynamically varying scenes. High-accuracy image coding and recognition are demonstrated in situ for various image sets, including hand-written digits and through-wall body gestures, using a single physical hardware imager, reprogrammed in real time. Our electronically controlled metasurface imager opens new venues for intelligent surveillance, fast data acquisition and processing, imaging at various frequencies, and beyond.Conventional imagers require time-consuming data acquisition, or complicated reconstruction algorithms for data post-processing. Here, the authors demonstrate a real-time digital-metasurface imager that can be trained in-situ to show high accuracy image coding and recognition for various image sets.\n",
            "\n",
            "Paper ID : 9583ac53a19cdf0db81fef6eb0b63e66adbe2324 \tArticle : Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent\n",
            "Author(s) : ['P. Blanchard', 'El Mahdi El Mhamdi', 'R. Guerraoui', 'J. Stainer']\n",
            "Year : 2017 \n",
            "Abstract : We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.\n",
            "\n",
            "Paper ID : dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd \tArticle : Troubling Trends in Machine Learning Scholarship\n",
            "Author(s) : ['Zachary Chase Lipton', 'J. Steinhardt']\n",
            "Year : 2019 \n",
            "Abstract : Flawed scholarship threatens to mislead the public and stymie future research by compromising ML’s intellectual foundations. Indeed, many of these problems have recurred cyclically throughout the history of AI and, more broadly, in scientific research. In 1976, Drew McDermott chastised the AI community for abandoning self-discipline, warning prophetically that \"if we can’t criticize ourselves, someone else will save us the trouble.\" The current strength of machine learning owes to a large body of rigorous research to date, both theoretical and empirical. By promoting clear scientific thinking and communication, our community can sustain the trust and investment it currently enjoys.\n",
            "\n",
            "Paper ID : 5fc5f45932104eb1f905aa316067e485938c1b01 \tArticle : Causality for Machine Learning\n",
            "Author(s) : ['B. Scholkopf']\n",
            "Year : 2019 \n",
            "Abstract : Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.\n",
            "\n",
            "Paper ID : daf468f001c3a5c6f9e667417becb94fa83efb2f \tArticle : Exploiting machine learning for end-to-end drug discovery and development\n",
            "Author(s) : ['S. Ekins', 'A. C. Puhl', 'Kimberley M. Zorn', 'T. Lane', 'Daniel P. Russo', 'Jennifer J Klein', 'A. Hickey', 'A. Clark']\n",
            "Year : 2019 \n",
            "Abstract : A variety of machine learning methods such as naive Bayesian, support vector machines and more recently deep neural networks are demonstrating their utility for drug discovery and development. These leverage the generally bigger datasets created from high-throughput screening data and allow prediction of bioactivities for targets and molecular properties with increased levels of accuracy. We have only just begun to exploit the potential of these techniques but they may already be fundamentally changing the research process for identifying new molecules and/or repurposing old drugs. The integrated application of such machine learning models for end-to-end (E2E) application is broadly relevant and has considerable implications for developing future therapies and their targeting.This Perspective describes the application of machine learning models in the design, synthesis and characterisation of molecules at different stages in the drug discovery and development process.\n",
            "\n",
            "Paper ID : 8fd8fe76b3fc76adb47b1f1597e2e182a8280225 \tArticle : Survey on SDN based network intrusion detection system using machine learning approaches\n",
            "Author(s) : ['N. Sultana', 'N. Chilamkurti', 'Wei Peng', 'Rabei Alhadad']\n",
            "Year : 2019 \n",
            "Abstract : Software Defined Networking Technology (SDN) provides a prospect to effectively detect and monitor network security problems ascribing to the emergence of the programmable features. Recently, Machine Learning (ML) approaches have been implemented in the SDN-based Network Intrusion Detection Systems (NIDS) to protect computer networks and to overcome network security issues. A stream of advanced machine learning approaches – the deep learning technology (DL) commences to emerge in the SDN context. In this survey, we reviewed various recent works on machine learning (ML) methods that leverage SDN to implement NIDS. More specifically, we evaluated the techniques of deep learning in developing SDN-based NIDS. In the meantime, in this survey, we covered tools that can be used to develop NIDS models in SDN environment. This survey is concluded with a discussion of ongoing challenges in implementing NIDS using ML/DL and future works.\n",
            "\n",
            "Paper ID : 62ccd99a65bfc7c735ae1f33b75b107665de95df \tArticle : Federated Machine Learning\n",
            "Author(s) : ['Qiang Yang', 'Yang Liu', 'Tianjian Chen', 'Yongxin Tong']\n",
            "Year : 2019 \n",
            "Abstract : Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.\n",
            "\n",
            "Paper ID : 218062f45c15f39bc8f4fb2c930ddf20b5809b11 \tArticle : Machine Learning Testing: Survey, Landscapes and Horizons\n",
            "Author(s) : ['J Zhang', 'M. Harman', 'Lei Ma', 'Yang Liu']\n",
            "Year : 2022 \n",
            "Abstract : This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.\n",
            "\n",
            "Paper ID : d0ab11de3077490c80a08abd0fb8827bac84c454 \tArticle : MoleculeNet: A Benchmark for Molecular Machine Learning\n",
            "Author(s) : ['Zhenqin Wu', 'Bharath Ramsundar', 'Evan N. Feinberg', 'Joseph Gomes', 'Caleb Geniesse', 'Aneesh S. Pappu', 'K. Leswing', 'V. Pande']\n",
            "Year : 2017 \n",
            "Abstract : Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.\n",
            "\n",
            "Paper ID : b3de1062d8a462dfdc2938558258f8884abe9f4e \tArticle : Implementation of machine-learning classification in remote sensing: an applied review\n",
            "Author(s) : ['Aaron E. Maxwell', 'T. Warner', 'Fang Fang']\n",
            "Year : 2018 \n",
            "Abstract : ABSTRACT Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.\n",
            "------------------------------------Extracting Page #14------------------------------------\n",
            "\n",
            "Paper ID : 960ba564e9e598d864dff38d2f3d0bad1b319ead \tArticle : A Survey of Machine Learning for Big Code and Naturalness\n",
            "Author(s) : ['Miltiadis Allamanis', 'Earl T. Barr', 'Premkumar T. Devanbu', 'Charles Sutton']\n",
            "Year : 2018 \n",
            "Abstract : Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.\n",
            "\n",
            "Paper ID : 88a97c8ef539589c55a6fe869c243792e470d6a3 \tArticle : Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective\n",
            "Author(s) : ['Kim M. Hazelwood', 'Sarah Bird', 'D. Brooks', 'Soumith Chintala', 'Utku Diril', 'Dmytro Dzhulgakov', 'Mohamed Fawzy', 'Bill Jia', 'Yangqing Jia', 'Aditya Kalro', 'James Law', 'Kevin Lee', 'Jason Lu', 'P. Noordhuis', 'M. Smelyanskiy', 'Liang Xiong', 'Xiaodong Wang']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.\n",
            "\n",
            "Paper ID : 236dfdeb4511754cf71ba220ac569b11973502cd \tArticle : Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey\n",
            "Author(s) : ['Hongyu Liu', 'Bo Lang']\n",
            "Year : 2019 \n",
            "Abstract : Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.\n",
            "\n",
            "Paper ID : 9d75cc322a4e06d0a3a868cb91b04219a289c12c \tArticle : Machine Learning: An Applied Econometric Approach\n",
            "Author(s) : ['S. Mullainathan', 'Jann Spiess']\n",
            "Year : 2017 \n",
            "Abstract : Machines are increasingly doing \"intelligent\" things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.\n",
            "\n",
            "Paper ID : eed9fa4483cab37eacd59db0fac4b1441431ee85 \tArticle : Tensor Decomposition for Signal Processing and Machine Learning\n",
            "Author(s) : ['N. Sidiropoulos', 'L. Lathauwer', 'Xiao Fu', 'Kejun Huang', 'E. Papalexakis', 'C. Faloutsos']\n",
            "Year : 2017 \n",
            "Abstract : Tensors or <italic>multiway arrays</italic> are functions of three or more indices <inline-formula> <tex-math notation=\"LaTeX\">$(i,j,k,\\ldots)$</tex-math></inline-formula>—similar to matrices (two-way arrays), which are functions of two indices <inline-formula><tex-math notation=\"LaTeX\">$(r,c)$</tex-math></inline-formula> for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth <italic>and depth</italic> that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.\n",
            "\n",
            "Paper ID : e9239469aba4bccf3e36d1c27894721e8dbefc44 \tArticle : Foundations of Machine Learning\n",
            "Author(s) : ['M. Mohri', 'Afshin Rostamizadeh', 'Ameet S. Talwalkar']\n",
            "Year : 2012 \n",
            "Abstract : This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.\n",
            "\n",
            "Paper ID : 8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8 \tArticle : Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification\n",
            "Author(s) : ['Ignacio Arganda-Carreras', 'V. Kaynig', 'C. Rueden', 'K. Eliceiri', 'Johannes E. Schindelin', 'A. Cardona', 'H. Seung']\n",
            "Year : 2017 \n",
            "Abstract : Summary: State‐of‐the‐art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time‐consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user‐designed image features or classifiers. Availability and Implementation: TWS is distributed as open‐source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable_Weka_Segmentation. Contact: ignacio.arganda@ehu.eus Supplementary information: Supplementary data are available at Bioinformatics online.\n",
            "\n",
            "Paper ID : 2aadb938af2f77a6ad9321ff873c1c9b9a579fcb \tArticle : A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection\n",
            "Author(s) : ['Lalu Banoth', 'M. S. Teja', 'M. Saicharan', 'N. Chandra']\n",
            "Year : 2017 \n",
            "Abstract : Cyber security is that the body of technologies, processes and practices designed to safeguard networks, computers, programs and knowledge from attack, harm or unauthorized access. During a computing context, the term security implies cyber security. This survey paper describes a targeted literature survey of machine learning (ML) and data processing (DM) strategies for cyber analytics in support of intrusion detection. This paper focuses totally on cyber intrusion detection as it applies to wired networks. With a wired network, associate oppose must experience many layers of defense at firewalls and operative systems, or gain physical access to the network. The quality of ML/DM algorithms is addressed, discussion of challenges for victimization ML/DM for cyber security is conferred, and some recommendations on once to use a given methodology area unit provided.\n",
            "\n",
            "Paper ID : 0359bba5112d472206d82ddb29947f2d634bb0cc \tArticle : Introduction to machine learning\n",
            "Author(s) : ['Ethem Alpaydin']\n",
            "Year : 2004 \n",
            "Abstract : The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods. Adaptive Computation and Machine Learning series\n",
            "\n",
            "Paper ID : 7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7 \tArticle : Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning\n",
            "Author(s) : ['T. Yarkoni', 'Jacob Westfall']\n",
            "Year : 2017 \n",
            "Abstract : Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.\n",
            "------------------------------------Extracting Page #15------------------------------------\n",
            "\n",
            "Paper ID : 7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7 \tArticle : Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning\n",
            "Author(s) : ['T. Yarkoni', 'Jacob Westfall']\n",
            "Year : 2017 \n",
            "Abstract : Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.\n",
            "\n",
            "Paper ID : c5c3fa019574988c479474c32f34debecd12e8d1 \tArticle : Procedural Content Generation via Machine Learning (PCGML)\n",
            "Author(s) : ['A. Summerville', 'Sam Snodgrass', 'Matthew J. Guzdial', 'Christoffer Holmgård', 'Amy K. Hoover', 'Aaron Isaksen', 'Andy Nealen', 'J. Togelius']\n",
            "Year : 2018 \n",
            "Abstract : This survey explores procedural content generation via machine learning (PCGML), defined as the generation of game content using machine learning models trained on existing content. As the importance of PCG for game development increases, researchers explore new avenues for generating high-quality content with or without human involvement; this paper addresses the relatively new paradigm of using machine learning (in contrast with search-based, solver-based, and constructive methods). We focus on what is most often considered functional game content, such as platformer levels, game maps, interactive fiction stories, and cards in collectible card games, as opposed to cosmetic content, such as sprites and sound effects. In addition to using PCG for autonomous generation, cocreativity, mixed-initiative design, and compression, PCGML is suited for repair, critique, and content analysis because of its focus on modeling existing content. We discuss various data sources and representations that affect the generated content. Multiple PCGML methods are covered, including neural networks: long short-term memory networks, autoencoders, and deep convolutional networks; Markov models: $n$-grams and multi-dimensional Markov chains; clustering; and matrix factorization. Finally, we discuss open problems in PCGML, including learning from small data sets, lack of training data, multilayered learning, style-transfer, parameter tuning, and PCG as a game mechanic.\n",
            "\n",
            "Paper ID : 05d20fda297c9afb347214bd1693bd049674e0c6 \tArticle : Machine Learning for the Geosciences: Challenges and Opportunities\n",
            "Author(s) : ['A. Karpatne', 'I. Ebert‐Uphoff', 'S. Ravela', 'H. Babaie', 'Vipin Kumar']\n",
            "Year : 2019 \n",
            "Abstract : Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML)—that has been widely successful in commercial domains—offers immense potential to contribute to problems in geosciences. However, geoscience applications introduce novel challenges for ML due to combinations of geoscience properties encountered in every problem, requiring novel research in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their common properties. We then describe some of the common categories of geoscience problems where machine learning can play a role, discussing the challenges faced by existing ML methods and opportunities for novel ML research. We conclude by discussing some of the cross-cutting research themes in machine learning that are applicable across several geoscience problems, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.\n",
            "\n",
            "Paper ID : 78aa018ee7d52360e15d103390ea1cdb3a0beb41 \tArticle : Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples\n",
            "Author(s) : ['Nicolas Papernot', 'P. Mcdaniel', 'Ian J. Goodfellow']\n",
            "Year : 2016 \n",
            "Abstract : Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.\n",
            "\n",
            "Paper ID : 59d9318f07331ec15e54fe2a4218bc4a5c247a38 \tArticle : Foolbox: A Python toolbox to benchmark the robustness of machine learning models\n",
            "Author(s) : ['Jonas Rauber', 'Wieland Brendel', 'M. Bethge']\n",
            "Year : 2017 \n",
            "Abstract : Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io. In 2013, Szegedy et al. demonstrated that minimal perturbations, often almost imperceptible to humans, can have devastating effects on machine predictions. These so-called adversarial perturbations thus demonstrate a striking difference between human and machine perception. As a result, adversarial perturbations have been subject to many Equal contribution Centre for Integrative Neuroscience, University of Tübingen, Germany Bernstein Center for Computational Neuroscience, Tübingen, Germany International Max Planck Research School for Intelligent Systems, Tübingen, Germany Max Planck Institute for Biological Cybernetics, Tübingen, Germany Institute for Theoretical Physics, University of Tübingen, Germany. Correspondence to: Jonas Rauber <jonas.rauber@bethgelab.org>. Reliable Machine Learning in the Wild Workshop, 34 th International Conference on Machine Learning, Sydney, Australia, 2017. studies concerning the generation of such perturbations and strategies to protect machine learning models such as deep neural networks against them. A practical definition of the robustness R of a model, first used by Szegedy et al. (2013), is the average size of the minimum adversarial perturbation ρ(x) across many samples x, R = 〈ρ(x)〉 x where (1) ρ(x) = min δ d(x,x+ δ) s.t. x+ δ is adversarial (2) and d(·) is some distance measure. Unfortunately, finding the global minimum adversarial perturbation is close to impossible in any practical setting, and we thus employ heuristic attacks to find a suitable approximation. Such heuristics, however, can fail, in which case we could easily be mislead to believe that a model is robust (Brendel & Bethge, 2017). Our best strategy is thus to employ as many attacks as possible, and to use the minimal perturbation found across all attacks as an approximation to the true global minimum. At the moment, however, such a strategy is severely obstructed by two problems: first, the code for most known attack methods is either not available at all, or only available for one particular deep learning framework. Second, implementations of the same attack often differ in many details and are thus not directly comparable. Foolbox improves upon the existing Python package cleverhans by Papernot et al. (2016b) in three important aspects: 1. It interfaces with most popular machine learning frameworks such as PyTorch, Keras, TensorFlow, Theano, Lasagne and MXNet and provides a straight forward way to add support for other frameworks, 2. it provides reference implementations for more than 15 adversarial attacks with a simple and consistent API, and 3. it supports many different criteria for adversarial examples, including custom ones. This technical report is structured as follows: In section 1 we provide an overview over Foolbox and demonstrate Foolbox: A Python toolbox to benchmark the robustness of machine learning models how to benchmark a model and report the result. In section 2 we describe the adversarial attack methods that are implemented in Foolbox and explain the internal hyperparameter tuning.\n",
            "\n",
            "Paper ID : be1496e9620089b377ef631692478f5034ee95b8 \tArticle : Machine learning applications in epilepsy\n",
            "Author(s) : ['B. Abbasi', 'D. Goldenholz']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre‐surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.\n",
            "\n",
            "Paper ID : 6e23398447a022fb9495c44fa80e9de593a574bc \tArticle : Machine Learning in Agriculture: A Review\n",
            "Author(s) : ['Konstantinos G. Liakos', 'P. Busato', 'D. Moshou', 'S. Pearson', 'D. Bochtis']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.\n",
            "\n",
            "Paper ID : 8db8166249dfb94dd8d52f88d27917b5755ae049 \tArticle : A Quick Review of Machine Learning Algorithms\n",
            "Author(s) : ['Susmita Ray']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning is predominantly an area of Artificial Intelligence which has been a key component of digitalization solutions that has caught major attention in the digital arena. In this paper author intends to do a brief review of various machine learning algorithms which are most frequently used and therefore are the most popular ones. The author intends to highlight the merits and demerits of the machine learning algorithms from their application perspective to aid in an informed decision making towards selecting the appropriate learning algorithm to meet the specific requirement of the application.\n",
            "\n",
            "Paper ID : 1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c \tArticle : The Marginal Value of Adaptive Gradient Methods in Machine Learning\n",
            "Author(s) : ['Ashia C. Wilson', 'R. Roelofs', 'Mitchell Stern', 'Nathan Srebro', 'B. Recht']\n",
            "Year : 2017 \n",
            "Abstract : Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.\n",
            "\n",
            "Paper ID : 0ec5868250cbc4eb668d2cb943fab1fe976d2d78 \tArticle : Machine Learning\n",
            "Author(s) : ['S. Kulkarni', 'V. Gurupur', 'S. Fernandes']\n",
            "Year : 2020 \n",
            "Abstract : Introduction and overview of machine learning and its applications. Unsupervised and supervised learning. Discriminative and generative models. Prediction. Generalization. Classification. Nearest neighbors. Naïve Bayes. Discriminant analysis. Cross-validation. Model selection. Overfitting. Bootstrap. Regression. Regularization. Ridge regression. Lasso. Variable Selection. Binary and multi-class regression. Dimension reduction. PCA. ICA. Kernel smoothers. Support Vector Machines. Decision trees. Gaussian processes. Mixture models.\n",
            "------------------------------------Extracting Page #16------------------------------------\n",
            "\n",
            "Paper ID : 1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c \tArticle : The Marginal Value of Adaptive Gradient Methods in Machine Learning\n",
            "Author(s) : ['Ashia C. Wilson', 'R. Roelofs', 'Mitchell Stern', 'Nathan Srebro', 'B. Recht']\n",
            "Year : 2017 \n",
            "Abstract : Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.\n",
            "\n",
            "Paper ID : 175e37bca3762b3a52c6a0e153060b98a251d061 \tArticle : Inverse molecular design using machine learning: Generative models for matter engineering\n",
            "Author(s) : ['Benjamín Sánchez-Lengeling', 'Alán Aspuru-Guzik']\n",
            "Year : 2018 \n",
            "Abstract : The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials.\n",
            "\n",
            "Paper ID : efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea \tArticle : Big Data and Machine Learning in Health Care.\n",
            "Author(s) : ['Andrew Beam', 'I. Kohane']\n",
            "Year : 2018 \n",
            "Abstract : Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT\n",
            "\n",
            "Paper ID : ab0a6b82c359ce6d4c2425080ffee632cb46fced \tArticle : MoleculeNet: a benchmark for molecular machine learning† †Electronic supplementary information (ESI) available. See DOI: 10.1039/c7sc02664a\n",
            "Author(s) : ['Zhenqin Wu', 'Bharath Ramsundar', 'Evan N. Feinberg', 'Joseph Gomes', 'Caleb Geniesse', 'Aneesh S. Pappu', 'K. Leswing', 'V. Pande']\n",
            "Year : 2018 \n",
            "Abstract : A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.\n",
            "\n",
            "Paper ID : 39361b3507c9f8b0a97780568b645f80a208d78a \tArticle : Machine Learning and Deep Learning Methods for Cybersecurity\n",
            "Author(s) : ['Yang Xin', 'Lingshuang Kong', 'Zhi Liu', 'Yuling Chen', 'Yanmiao Li', 'Hongliang Zhu', 'M. Gao', 'Haixia Hou', 'Chunhua Wang']\n",
            "Year : 2018 \n",
            "Abstract : With the development of the Internet, cyber-attacks are changing rapidly and the cyber security situation is not optimistic. This survey report describes key literature surveys on machine learning (ML) and deep learning (DL) methods for network analysis of intrusion detection and provides a brief tutorial description of each ML/DL method. Papers representing each method were indexed, read, and summarized based on their temporal or thermal correlations. Because data are so important in ML/DL methods, we describe some of the commonly used network datasets used in ML/DL, discuss the challenges of using ML/DL for cybersecurity and provide suggestions for research directions.\n",
            "\n",
            "Paper ID : adfc508b9b3d4fc3903aa383a290dc68fb8bbe5a \tArticle : Implementing Machine Learning in Health Care - Addressing Ethical Challenges.\n",
            "Author(s) : ['D. Char', 'N. Shah', 'D. Magnus']\n",
            "Year : 2018 \n",
            "Abstract : Implementing Machine Learning in Health Care We need to consider the ethical challenges inherent in implementing machine learning in health care if its benefits are to be realized. Some of these ch...\n",
            "\n",
            "Paper ID : 0f5476c9629f8093e8ba8c6a41868415c6a7f2f1 \tArticle : Stealing Hyperparameters in Machine Learning\n",
            "Author(s) : ['Binghui Wang', 'N. Gong']\n",
            "Year : 2018 \n",
            "Abstract : Hyperparameters are critical in machine learning, as different hyperparameters often result in models with significantly different performance. Hyperparameters may be deemed confidential because of their commercial value and the confidentiality of the proprietary algorithms that the learner uses to learn them. In this work, we propose attacks on stealing the hyperparameters that are learned by a learner. We call our attacks hyperparameter stealing attacks. Our attacks are applicable to a variety of popular machine learning algorithms such as ridge regression, logistic regression, support vector machine, and neural network. We evaluate the effectiveness of our attacks both theoretically and empirically. For instance, we evaluate our attacks on Amazon Machine Learning. Our results demonstrate that our attacks can accurately steal hyperparameters. We also study countermeasures. Our results highlight the need for new defenses against our hyperparameter stealing attacks for certain machine learning algorithms.\n",
            "\n",
            "Paper ID : 29524f145db94cab2336da99f157e869d805dead \tArticle : SoK: Security and Privacy in Machine Learning\n",
            "Author(s) : ['Nicolas Papernot', 'P. Mcdaniel', 'Arunesh Sinha', 'Michael P. Wellman']\n",
            "Year : 2018 \n",
            "Abstract : Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, à la PAC theory, will foster a science of security and privacy in ML.\n",
            "\n",
            "Paper ID : 4f2baff3195b6fc43a38e3e869496dab9fe9dbc3 \tArticle : Delayed Impact of Fair Machine Learning\n",
            "Author(s) : ['Lydia T. Liu', 'Sarah Dean', 'Esther Rolf', 'Max Simchowitz', 'Moritz Hardt']\n",
            "Year : 2018 \n",
            "Abstract : Static classification has been the predominant focus of the study of fairness in machine learning. While most models do not consider how decisions change populations over time, it is conventional wisdom that fairness criteria promote the long-term well-being of groups they aim to protect. This work studies the interaction of static fairness criteria with temporal indicators of well-being. We show a simple one-step feedback model in which common criteria do not generally promote improvement over time, and may in fact cause harm. Our results highlight the importance of temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.\n",
            "\n",
            "Paper ID : d5125164c7fec457d1442cce807a3436841715d0 \tArticle : Machine Learning Approaches for Clinical Psychology and Psychiatry.\n",
            "Author(s) : ['D. Dwyer', 'P. Falkai', 'N. Koutsouleris']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined.\n",
            "------------------------------------Extracting Page #17------------------------------------\n",
            "\n",
            "Paper ID : 5939ac3b5a9d64d8371ee179751351d7698637df \tArticle : Using Machine Learning to Advance Personality Assessment and Theory\n",
            "Author(s) : ['W. Bleidorn', 'C. Hopwood']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.\n",
            "\n",
            "Paper ID : 7d065e649e3bfc7d6d36166f50eab37b8404eae0 \tArticle : Interpretable Machine Learning in Healthcare\n",
            "Author(s) : ['M. Ahmad', 'A. Teredesai', 'C. Eckert']\n",
            "Year : 2018 \n",
            "Abstract : This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.\n",
            "\n",
            "Paper ID : 8d7238eea00059deb446b6309bcae2901c966049 \tArticle : Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.\n",
            "Author(s) : ['Z. Obermeyer', 'E. Emanuel']\n",
            "Year : 2016 \n",
            "Abstract : The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.\n",
            "\n",
            "Paper ID : 0cc6dbfd929bc816d507527993f55f9b4e88615d \tArticle : Machine learning & artificial intelligence in the quantum domain: a review of recent progress\n",
            "Author(s) : ['V. Dunjko', 'H. Briegel']\n",
            "Year : 2018 \n",
            "Abstract : Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research—quantum information versus machine learning (ML) and artificial intelligence (AI)—have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our ‘big data’ world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement—exploring what ML/AI can do for quantum physics and vice versa—researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.\n",
            "\n",
            "Paper ID : 161ce338538f94b0b9be51ae2336db0aa4b012e5 \tArticle : Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data\n",
            "Author(s) : ['M. Gianfrancesco', 'S. Tamang', 'J. Yazdany', 'G. Schmajuk']\n",
            "Year : 2018 \n",
            "Abstract : A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.\n",
            "\n",
            "Paper ID : 5fdc2223709079ba5c0f78661cdf66cec2173258 \tArticle : Current Applications and Future Impact of Machine Learning in Radiology.\n",
            "Author(s) : ['G. Choy', 'O. Khalilzadeh', 'Mark H. Michalski', 'Synho Do', 'A. Samir', 'O. Pianykh', 'J. R. Geis', 'P. Pandharipande', 'J. Brink', 'K. Dreyer']\n",
            "Year : 2018 \n",
            "Abstract : Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed.\n",
            "\n",
            "Paper ID : 920e561cee4fd4888212ed127d51aa09c02be6c3 \tArticle : Advances in Financial Machine Learning\n",
            "Author(s) : ['M. L. Prado']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. Readers will learn how to structure Big data in a way that is amenable to ML algorithms; how to conduct research with ML algorithms on that data; how to use supercomputing methods; how to backtest your discoveries while avoiding false positives. The book addresses real-life problems faced by practitioners on a daily basis, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their particular setting. Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance.\n",
            "\n",
            "Paper ID : 8285e1b5536ce11d55462ae757f61c75ec6773c6 \tArticle : The Frontiers of Fairness in Machine Learning\n",
            "Author(s) : ['A. Chouldechova', 'Aaron Roth']\n",
            "Year : 2018 \n",
            "Abstract : The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.\n",
            "\n",
            "Paper ID : 2a944564c2466883ec14a6f6ef461f0e34d21b38 \tArticle : Fairness in Machine Learning: Lessons from Political Philosophy\n",
            "Author(s) : ['R. Binns']\n",
            "Year : 2018 \n",
            "Abstract : What does it mean for a machine learning model to be `fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise `fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.\n",
            "\n",
            "Paper ID : d4de528645fdfc6d954364a8e6eeeed9480ccfa2 \tArticle : Machine Learning for Networking: Workflow, Advances and Opportunities\n",
            "Author(s) : ['Mowei Wang', 'Yong Cui', 'Xin Wang', 'Shihan Xiao', 'Junchen Jiang']\n",
            "Year : 2018 \n",
            "Abstract : Recently, machine learning has been used in every possible field to leverage its amazing power. For a long time, the networking and distributed computing system is the key infrastructure to provide efficient computational resources for machine learning. Networking itself can also benefit from this promising technology. This article focuses on the application of MLN, which can not only help solve the intractable old network questions but also stimulate new network applications. In this article, we summarize the basic workflow to explain how to apply machine learning technology in the networking domain. Then we provide a selective survey of the latest representative advances with explanations of their design principles and benefits. These advances are divided into several network design objectives and the detailed information of how they perform in each step of MLN workflow is presented. Finally, we shed light on the new opportunities in networking design and community building of this new inter-discipline. Our goal is to provide a broad research guideline on networking with machine learning to help motivate researchers to develop innovative algorithms, standards and frameworks.\n",
            "------------------------------------Extracting Page #18------------------------------------\n",
            "\n",
            "Paper ID : fa9906b466bbbff3a8c206b499cd34323a91d1b2 \tArticle : Points of Significance: Statistics versus machine learning\n",
            "Author(s) : ['D. Bzdok', 'Naomi Altman', 'M. Krzywinski']\n",
            "Year : 2018 \n",
            "Abstract : Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns.\n",
            "\n",
            "Paper ID : 66ce6a5b7f3e888f4ab75054bb9cf0271bb6f012 \tArticle : A strategy to apply machine learning to small datasets in materials science\n",
            "Author(s) : ['Ying Zhang', 'Chen Ling']\n",
            "Year : 2018 \n",
            "Abstract : There is growing interest in applying machine learning techniques in the research of materials science. However, although it is recognized that materials datasets are typically smaller and sometimes more diverse compared to other fields, the influence of availability of materials data on training machine learning models has not yet been studied, which prevents the possibility to establish accurate predictive rules using small materials datasets. Here we analyzed the fundamental interplay between the availability of materials data and the predictive capability of machine learning models. Instead of affecting the model precision directly, the effect of data size is mediated by the degree of freedom (DoF) of model, resulting in the phenomenon of association between precision and DoF. The appearance of precision–DoF association signals the issue of underfitting and is characterized by large bias of prediction, which consequently restricts the accurate prediction in unknown domains. We proposed to incorporate the crude estimation of property in the feature space to establish ML models using small sized materials data, which increases the accuracy of prediction without the cost of higher DoF. In three case studies of predicting the band gap of binary semiconductors, lattice thermal conductivity, and elastic properties of zeolites, the integration of crude estimation effectively boosted the predictive capability of machine learning models to state-of-art levels, demonstrating the generality of the proposed strategy to construct accurate machine learning models using small materials dataset.MACHINE LEARNING: Dealing with small datasetsMachine learning can be useful for materials prediction if crude estimations of the outcome are integrated in the code. Machine learning has been attracting tremendous attention lately due to its predictive power; evidence suggests it is directly proportional to the size of the available datasets. Machine learning can be useful in predicting new materials and novel properties, but materials sets tend to be smaller and more diverse than other fields. Ying Zhang and Chen Ling from the Toyota Research Institute of North America report that these small datasets affect the freedom of the algorithms and thus limit their predictive capabilities. In order to counterbalance the effect, they suggest introducing in the code crude estimations of the targeted property, obtained by other means.\n",
            "\n",
            "Paper ID : a450156bd8a1e3b808f93e959ae9e79983c5751b \tArticle : Controlling an organic synthesis robot with machine learning to search for new reactivity\n",
            "Author(s) : ['J. Granda', 'Liva Donina', 'Vincenza Dragone', 'D. Long', 'L. Cronin']\n",
            "Year : 2018 \n",
            "Abstract : The discovery of chemical reactions is an inherently unpredictable and time-consuming process1. An attractive alternative is to predict reactivity, although relevant approaches, such as computer-aided reaction design, are still in their infancy2. Reaction prediction based on high-level quantum chemical methods is complex3, even for simple molecules. Although machine learning is powerful for data analysis4,5, its applications in chemistry are still being developed6. Inspired by strategies based on chemists’ intuition7, we propose that a reaction system controlled by a machine learning algorithm may be able to explore the space of chemical reactions quickly, especially if trained by an expert8. Here we present an organic synthesis robot that can perform chemical reactions and analysis faster than they can be performed manually, as well as predict the reactivity of possible reagent combinations after conducting a small number of experiments, thus effectively navigating chemical reaction space. By using machine learning for decision making, enabled by binary encoding of the chemical inputs, the reactions can be assessed in real time using nuclear magnetic resonance and infrared spectroscopy. The machine learning system was able to predict the reactivity of about 1,000 reaction combinations with accuracy greater than 80 per cent after considering the outcomes of slightly over 10 per cent of the dataset. This approach was also used to calculate the reactivity of published datasets. Further, by using real-time data from our robot, these predictions were followed up manually by a chemist, leading to the discovery of four reactions.A robot instructed by a machine learning algorithm and coupled with real-time spectroscopic systems provides fast and accurate reaction outcome predictions and reactivity assessments, leading to the discovery of new reactions.\n",
            "\n",
            "Paper ID : 4e75a145f59400a8d646db770cc396de87d6b9d8 \tArticle : Image Reconstruction is a New Frontier of Machine Learning\n",
            "Author(s) : ['Ge Wang', 'J. C. Ye', 'K. Mueller', 'J. Fessler']\n",
            "Year : 2018 \n",
            "Abstract : Over past several years, machine learning, or more generally artificial intelligence, has generated overwhelming research interest and attracted unprecedented public attention. As tomographic imaging researchers, we share the excitement from our imaging perspective [item 1) in the Appendix], and organized this special issue dedicated to the theme of “Machine learning for image reconstruction.” This special issue is a sister issue of the special issue published in May 2016 of this journal with the theme “Deep learning in medical imaging” [item 2) in the Appendix]. While the previous special issue targeted medical image processing/analysis, this special issue focuses on data-driven tomographic reconstruction. These two special issues are highly complementary, since image reconstruction and image analysis are two of the main pillars for medical imaging. Together we cover the whole workflow of medical imaging: from tomographic raw data/features to reconstructed images and then extracted diagnostic features/readings.\n",
            "\n",
            "Paper ID : 6a6ad9eb495739f4c80e7c09598720c3d5c5dff7 \tArticle : Federated Learning: Collaborative Machine Learning without\n",
            "Centralized Training Data\n",
            "Author(s) : ['Abhishek V A', 'Binny S', 'Johan T R', 'Nithin Raj', 'Vishal Thomas']\n",
            "Year : 2022 \n",
            "Abstract : Federated learning (also known as collaborative learning) is a machine learning technique that trains\n",
            "an algorithm without transferring data samples across numerous decentralized edge devices or\n",
            "servers. This strategy differs from standard centralized machine learning techniques in which all local\n",
            "datasets are uploaded to a single server, as well as more traditional decentralized alternatives, which\n",
            "frequently presume that local data samples are uniformly distributed.\n",
            "Federated learning allows several actors to collaborate on the development of a single, robust\n",
            "machine learning model without sharing data, allowing crucial issues such as data privacy, data\n",
            "security, data access rights, and access to heterogeneous data to be addressed. Defence,\n",
            "telecommunications, internet of things, and pharmaceutical industries are just a few of the sectors\n",
            "where it has applications.\n",
            "\n",
            "Paper ID : ad8faf6376e0b0a636f2a5210abc70c41d3d8a91 \tArticle : Quantum machine learning\n",
            "Author(s) : ['J. Biamonte', 'P. Wittek', 'Nicola Pancotti', 'P. Rebentrost', 'N. Wiebe', 'S. Lloyd']\n",
            "Year : 2017 \n",
            "Abstract : Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.\n",
            "\n",
            "Paper ID : 775a4e375cc79b53b94e37fa3eedff481823e4a6 \tArticle : Efficient and Robust Automated Machine Learning\n",
            "Author(s) : ['Matthias Feurer', 'Aaron Klein', 'Katharina Eggensperger', 'Jost Tobias Springenberg', 'Manuel Blum', 'F. Hutter']\n",
            "Year : 2015 \n",
            "Abstract : The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.\n",
            "\n",
            "Paper ID : c46c0fc81d41d9561643d55f41626c5583b0b25e \tArticle : Machine Learning in Medicine.\n",
            "Author(s) : ['R. Deo']\n",
            "Year : 2015 \n",
            "Abstract : Spurred by advances in processing power, memory, storage, and an unprecedented wealth of data, computers are being asked to tackle increasingly complex learning tasks, often with astonishing success. Computers have now mastered a popular variant of poker, learned the laws of physics from experimental data, and become experts in video games - tasks that would have been deemed impossible not too long ago. In parallel, the number of companies centered on applying complex data analysis to varying industries has exploded, and it is thus unsurprising that some analytic companies are turning attention to problems in health care. The purpose of this review is to explore what problems in medicine might benefit from such learning approaches and use examples from the literature to introduce basic concepts in machine learning. It is important to note that seemingly large enough medical data sets and adequate learning algorithms have been available for many decades, and yet, although there are thousands of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Thus, part of my effort will be to identify what obstacles there may be to changing the practice of medicine through statistical learning approaches, and discuss how these might be overcome.\n",
            "\n",
            "Paper ID : 3d1653512d2e85e0c46ce1c36657e7d4ae9a4d3e \tArticle : ML-Plan: Automated machine learning via hierarchical planning\n",
            "Author(s) : ['Felix Mohr', 'Marcel Wever', 'E. Hüllermeier']\n",
            "Year : 2018 \n",
            "Abstract : Automated machine learning (AutoML) seeks to automatically select, compose, and parametrize machine learning algorithms, so as to achieve optimal performance on a given task (dataset). Although current approaches to AutoML have already produced impressive results, the field is still far from mature, and new techniques are still being developed. In this paper, we present ML-Plan, a new approach to AutoML based on hierarchical planning. To highlight the potential of this approach, we compare ML-Plan to the state-of-the-art frameworks Auto-WEKA, auto-sklearn, and TPOT. In an extensive series of experiments, we show that ML-Plan is highly competitive and often outperforms existing approaches.\n",
            "\n",
            "Paper ID : f6d9106b0e169204a506eb1deec2b85e0f296e4a \tArticle : Feature selection in machine learning: A new perspective\n",
            "Author(s) : ['Jie Cai', 'Jiawei Luo', 'Shulin Wang', 'Sheng Yang']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #19------------------------------------\n",
            "\n",
            "Paper ID : f6d9106b0e169204a506eb1deec2b85e0f296e4a \tArticle : Feature selection in machine learning: A new perspective\n",
            "Author(s) : ['Jie Cai', 'Jiawei Luo', 'Shulin Wang', 'Sheng Yang']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 7e355a8f42becb6648efc4cf0c129a7f560789be \tArticle : Machine Learning Methods for Histopathological Image Analysis\n",
            "Author(s) : ['D. Komura', 'S. Ishikawa']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : c7ce37d51ba01e17c9291f1dbf2ca8223b948b7f \tArticle : Machine learning for Internet of Things data analysis: A survey\n",
            "Author(s) : ['M. Mahdavinejad', 'M. Rezvan', 'M. Barekatain', 'Peyman Adibi', 'P. Barnaghi', 'A. Sheth']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : a821071ce32bdaa6130e618bb1ad1112a6a94915 \tArticle : Machine learning in chemoinformatics and drug discovery.\n",
            "Author(s) : ['Y. Lo', 'Stefano E. Rensi', 'Wen Torng', 'R. Altman']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : f4c3315684cfd474c3d13ae4954de0dead5e81a3 \tArticle : Machine learning in cardiovascular medicine: are we there yet?\n",
            "Author(s) : ['K. Shameer', 'Kipp W. Johnson', 'Benjamin S. Glicksberg', 'J. Dudley', 'P. Sengupta']\n",
            "Year : 2018 \n",
            "Abstract : Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.\n",
            "\n",
            "Paper ID : 3c8a456509e6c0805354bd40a35e3f2dbf8069b1 \tArticle : PyTorch: An Imperative Style, High-Performance Deep Learning Library\n",
            "Author(s) : ['Adam Paszke', 'S. Gross', 'Francisco Massa', 'Adam Lerer', 'James Bradbury', 'Gregory Chanan', 'Trevor Killeen', 'Zeming Lin', 'N. Gimelshein', 'L. Antiga', 'Alban Desmaison', 'Andreas Köpf', 'E. Yang', 'Zach DeVito', 'Martin Raison', 'Alykhan Tejani', 'Sasank Chilamkurthy', 'Benoit Steiner', 'Lu Fang', 'Junjie Bai', 'Soumith Chintala']\n",
            "Year : 2019 \n",
            "Abstract : Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.\n",
            "\n",
            "Paper ID : d2c12b2e57d1262a3c8beb119bbadf596d7a116a \tArticle : Next-Generation Machine Learning for Biological Networks\n",
            "Author(s) : ['Diogo M. Camacho', 'K. M. Collins', 'R. Powers', 'J. Costello', 'J. Collins']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : dd41d656e21c30dd761bee2eba303d1aa014d120 \tArticle : Machine Learning Paradigms for Next-Generation Wireless Networks\n",
            "Author(s) : ['Chunxiao Jiang', 'Haijun Zhang', 'Yong Ren', 'Zhu Han', 'Kwang-Cheng Chen', 'L. Hanzo']\n",
            "Year : 2017 \n",
            "Abstract : Next-generation wireless networks are expected to support extremely high data rates and radically new applications, which require a new wireless radio technology paradigm. The challenge is that of assisting the radio in intelligent adaptive learning and decision making, so that the diverse requirements of next-generation wireless networks can be satisfied. Machine learning is one of the most promising artificial intelligence tools, conceived to support smart radio terminals. Future smart 5G mobile terminals are expected to autonomously access the most meritorious spectral bands with the aid of sophisticated spectral efficiency learning and inference, in order to control the transmission power, while relying on energy efficiency learning/inference and simultaneously adjusting the transmission protocols with the aid of quality of service learning/inference. Hence we briefly review the rudimentary concepts of machine learning and propose their employment in the compelling applications of 5G networks, including cognitive radios, massive MIMOs, femto/small cells, heterogeneous networks, smart grid, energy harvesting, device-todevice communications, and so on. Our goal is to assist the readers in refining the motivation, problem formulation, and methodology of powerful machine learning algorithms in the context of future networks in order to tap into hitherto unexplored applications and services.\n",
            "\n",
            "Paper ID : 5b77625b30ab2fa8abf5c152831a6985a61516ee \tArticle : Can machine-learning improve cardiovascular risk prediction using routine clinical data?\n",
            "Author(s) : ['S. Weng', 'J. Reps', 'J. Kai', 'J. Garibaldi', 'N. Qureshi']\n",
            "Year : 2017 \n",
            "Abstract : Background Current approaches to predict cardiovascular risk fail to identify many people who would benefit from preventive treatment, while others receive unnecessary intervention. Machine-learning offers opportunity to improve accuracy by exploiting complex interactions between risk factors. We assessed whether machine-learning can improve cardiovascular risk prediction. Methods Prospective cohort study using routine clinical data of 378,256 patients from UK family practices, free from cardiovascular disease at outset. Four machine-learning algorithms (random forest, logistic regression, gradient boosting machines, neural networks) were compared to an established algorithm (American College of Cardiology guidelines) to predict first cardiovascular event over 10-years. Predictive accuracy was assessed by area under the ‘receiver operating curve’ (AUC); and sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) to predict 7.5% cardiovascular risk (threshold for initiating statins). Findings 24,970 incident cardiovascular events (6.6%) occurred. Compared to the established risk prediction algorithm (AUC 0.728, 95% CI 0.723–0.735), machine-learning algorithms improved prediction: random forest +1.7% (AUC 0.745, 95% CI 0.739–0.750), logistic regression +3.2% (AUC 0.760, 95% CI 0.755–0.766), gradient boosting +3.3% (AUC 0.761, 95% CI 0.755–0.766), neural networks +3.6% (AUC 0.764, 95% CI 0.759–0.769). The highest achieving (neural networks) algorithm predicted 4,998/7,404 cases (sensitivity 67.5%, PPV 18.4%) and 53,458/75,585 non-cases (specificity 70.7%, NPV 95.7%), correctly predicting 355 (+7.6%) more patients who developed cardiovascular disease compared to the established algorithm. Conclusions Machine-learning significantly improves accuracy of cardiovascular risk prediction, increasing the number of patients identified who could benefit from preventive treatment, while avoiding unnecessary treatment of others.\n",
            "\n",
            "Paper ID : 79320387bfe5e5b9d95d32ad3436671c1665f290 \tArticle : Ten quick tips for machine learning in computational biology\n",
            "Author(s) : ['D. Chicco']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.\n",
            "------------------------------------Extracting Page #20------------------------------------\n",
            "\n",
            "Paper ID : 1ce15f4a83706b877e86f29549920651a888b144 \tArticle : Data Mining and Analytics in the Process Industry: The Role of Machine Learning\n",
            "Author(s) : ['Zhiqiang Ge', 'Zhi-huan Song', 'S. Ding', 'Biao Huang']\n",
            "Year : 2017 \n",
            "Abstract : Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computational engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.\n",
            "\n",
            "Paper ID : 33d9d4593d44792e17a045e5f3407f0fe7a40dd1 \tArticle : Machine learning of accurate energy-conserving molecular force fields\n",
            "Author(s) : ['Stefan Chmiela', 'A. Tkatchenko', 'H. E. Sauceda', 'I. Poltavsky', 'Kristof T. Schütt', 'K. Müller']\n",
            "Year : 2017 \n",
            "Abstract : The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields. Using conservation of energy—a fundamental property of closed classical and quantum mechanical systems—we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol−1 for energies and 1 kcal mol−1 Å̊−1 for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.\n",
            "\n",
            "Paper ID : 79320387bfe5e5b9d95d32ad3436671c1665f290 \tArticle : Ten quick tips for machine learning in computational biology\n",
            "Author(s) : ['D. Chicco']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning has become a pivotal tool for many projects in computational biology, bioinformatics, and health informatics. Nevertheless, beginners and biomedical researchers often do not have enough experience to run a data mining project effectively, and therefore can follow incorrect practices, that may lead to common mistakes or over-optimistic results. With this review, we present ten quick tips to take advantage of machine learning in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects. We believe our ten suggestions can strongly help any machine learning practitioner to carry on a successful project in computational biology and related sciences.\n",
            "\n",
            "Paper ID : a7f8b8e6124901c1e22e940092e87b5b93776ab3 \tArticle : Machine Learning With Big Data: Challenges and Approaches\n",
            "Author(s) : ['Alexandra L’Heureux', 'Katarina Grolinger', 'H. F. ElYamany', 'Miriam A. M. Capretz']\n",
            "Year : 2017 \n",
            "Abstract : The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.\n",
            "\n",
            "Paper ID : e3f8c8253767b6fe0891025cdf772cd286337921 \tArticle : A Proposal on Machine Learning via Dynamical Systems\n",
            "Author(s) : ['E. Weinan']\n",
            "Year : 2017 \n",
            "Abstract : We discuss the idea of using continuous dynamical systems to model general high-dimensional nonlinear functions used in machine learning. We also discuss the connection with deep learning.\n",
            "\n",
            "Paper ID : 02e2e79a77d8aabc1af1900ac80ceebac20abde4 \tArticle : Explanation and Justification in Machine Learning : A Survey Or\n",
            "Author(s) : ['Or Biran', 'Courtenay V. Cotton']\n",
            "Year : 2017 \n",
            "Abstract : We present a survey of the research concerning explanation and justification in the Machine Learning literature and several adjacent fields. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justification.\n",
            "\n",
            "Paper ID : c0eb2d5d65ecc27cb00501bffdcc55167c61cfe0 \tArticle : What can machine learning do? Workforce implications\n",
            "Author(s) : ['E. Brynjolfsson', 'Tom. Mitchell']\n",
            "Year : 2017 \n",
            "Abstract : Profound change is coming, but roles for humans remain Digital computers have transformed work in almost every sector of the economy over the past several decades (1). We are now at the beginning of an even larger and more rapid transformation due to recent advances in machine learning (ML), which is capable of accelerating the pace of automation itself. However, although it is clear that ML is a “general purpose technology,” like the steam engine and electricity, which spawns a plethora of additional innovations and capabilities (2), there is no widely shared agreement on the tasks where ML systems excel, and thus little agreement on the specific expected impacts on the workforce and on the economy more broadly. We discuss what we see to be key implications for the workforce, drawing on our rubric of what the current generation of ML systems can and cannot do [see the supplementary materials (SM)]. Although parts of many jobs may be “suitable for ML” (SML), other tasks within these same jobs do not fit the criteria for ML well; hence, effects on employment are more complex than the simple replacement and substitution story emphasized by some. Although economic effects of ML are relatively limited today, and we are not facing the imminent “end of work” as is sometimes proclaimed, the implications for the economy and the workforce going forward are profound.\n",
            "\n",
            "Paper ID : b191fc4294f6f067067e4e152bd4efc8bbb87afd \tArticle : Unintended Consequences of Machine Learning in Medicine\n",
            "Author(s) : ['F. Cabitza', 'Raffaele Rasoini', 'G. Gensini']\n",
            "Year : 2017 \n",
            "Abstract : Over the past decade, machine learning techniques have made substantial advances in many domains. In health care, global interest in the potential of machine learning has increased; for example, a deep learning algorithm has shown high accuracy in detecting diabetic retinopathy.1 There have been suggestions that machine learning will drive changes in health care within a few years, specifically in medical disciplines that require more accurate prognostic models (eg, oncology) and those based on pattern recognition (eg, radiology and pathology). However, comparative studies on the effectiveness of machine learning–based decision support systems (ML-DSS) in medicine are lacking, especially regarding the effects on health outcomes. Moreover, the introduction of new technologies in health care has not always been straightforward or without unintended and adverse effects.2 In this Viewpoint we consider the potential unintended consequences that may result from the application of ML-DSS in clinical practice.\n",
            "\n",
            "Paper ID : 2e5ee400be272be9c64c40f19ea91efb11046202 \tArticle : Bypassing the Kohn-Sham equations with machine learning\n",
            "Author(s) : ['F. Brockherde', 'Li Li', 'K. Burke', 'K. Müller']\n",
            "Year : 2017 \n",
            "Abstract : Last year, at least 30,000 scientific papers used the Kohn–Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields. Machine learning holds the promise of learning the energy functional via examples, bypassing the need to solve the Kohn–Sham equations. This should yield substantial savings in computer time, allowing larger systems and/or longer time-scales to be tackled, but attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty by directly learning the density-potential and energy-density maps for test systems and various molecules. We perform the first molecular dynamics simulation with a machine-learned density functional on malonaldehyde and are able to capture the intramolecular proton transfer process. Learning density models now allows the construction of accurate density functionals for realistic molecular systems.Machine learning allows electronic structure calculations to access larger system sizes and, in dynamical simulations, longer time scales. Here, the authors perform such a simulation using a machine-learned density functional that avoids direct solution of the Kohn-Sham equations.\n",
            "\n",
            "Paper ID : ca62b1904a4f3982b172c85204a588f494cb6a22 \tArticle : Taking Human out of Learning Applications: A Survey on Automated Machine Learning\n",
            "Author(s) : ['Quanming Yao', 'Mengshuo Wang', 'H. Escalante', 'I. Guyon', 'Yi-Qi Hu', 'Yu-Feng Li', 'Wei-Wei Tu', 'Qiang Yang', 'Yang Yu']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning techniques have deeply rooted in our everyday life. However, since it is knowledge- and labor-intensive to pursue good learning performance, human experts are heavily involved in every aspect of machine learning. In order to make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest. In this paper, we provide an up to date survey on AutoML. First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning. Then, we propose a general AutoML framework that not only covers most existing approaches to date but also can guide the design for new methods. Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques. Finally, we provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications. We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.\n",
            "------------------------------------Extracting Page #21------------------------------------\n",
            "\n",
            "Paper ID : ebd52485454d0d25e45c6404bb1c687325053a3b \tArticle : Machine Learning\n",
            "Author(s) : ['Ravindra Das']\n",
            "Year : 2020 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 998001b5102459f97c3d21f898b626a5f11cd414 \tArticle : Machine Learning\n",
            "Author(s) : ['E. Ruspini', 'P. Bonissone', 'W. Pedrycz']\n",
            "Year : 2020 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 1d41d6ec4805f80b84a1ccd17f6753ba71e107f7 \tArticle : Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)\n",
            "Author(s) : ['C. Rasmussen', 'Christopher K. I. Williams']\n",
            "Year : 2005 \n",
            "Abstract : Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.\n",
            "\n",
            "Paper ID : 27fb93067e2828951db050e4f42186caa043475c \tArticle : Machine learning in materials informatics: recent applications and prospects\n",
            "Author(s) : ['R. Ramprasad', 'R. Batra', 'G. Pilania', 'A. Mannodi-Kanakkithodi', 'Chiho Kim']\n",
            "Year : 2017 \n",
            "Abstract : Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods—due to the cost, time or effort involved—but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints, also referred to as “descriptors”, may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative—extending into new materials spaces—provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven “materials informatics” strategies undertaken in the last decade, with particular emphasis on the fingerprint or descriptor choices. The review also identifies some challenges the community is facing and those that should be overcome in the near future.\n",
            "\n",
            "Paper ID : e96fe4e820c2ca9002ad56c6a58c1e3840130fa7 \tArticle : Making machine learning robust against adversarial inputs\n",
            "Author(s) : ['Ian J. Goodfellow', 'P. Mcdaniel', 'Nicolas Papernot']\n",
            "Year : 2018 \n",
            "Abstract : Such inputs distort how machine-learning-based systems are able to function in the world as it is.\n",
            "\n",
            "Paper ID : 4a8c332b09bb99333a8bce6a4640a20c1352aa63 \tArticle : A Survey on Security Threats and Defensive Techniques of Machine Learning: A Data Driven View\n",
            "Author(s) : ['Qiang Liu', 'Pan Li', 'Wentao Zhao', 'Wei Cai', 'Shui Yu', 'Victor C. M. Leung']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.\n",
            "\n",
            "Paper ID : 1696cbf7da0ee845c50591843993e6605adec177 \tArticle : A few useful things to know about machine learning\n",
            "Author(s) : ['Pedro M. Domingos']\n",
            "Year : 2012 \n",
            "Abstract : Tapping into the \"folk knowledge\" needed to advance machine learning applications.\n",
            "\n",
            "Paper ID : 474d8f5a80ca3dc56ed7b149a1fb09edc71931b2 \tArticle : Machine Learning for Medical Imaging.\n",
            "Author(s) : ['B. Erickson', 'P. Korfiatis', 'Z. Akkus', 'T. Kline']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works. ©RSNA, 2017.\n",
            "\n",
            "Paper ID : 8da350feb5e18e92bcc91f02899187381382c6e4 \tArticle : Machine learning unifies the modeling of materials and molecules\n",
            "Author(s) : ['A. Bartók', 'Sandip De', 'C. Poelking', 'N. Bernstein', 'J. Kermode', 'Gábor Csányi', 'M. Ceriotti']\n",
            "Year : 2017 \n",
            "Abstract : Statistical learning based on a local representation of atomic structures provides a universal model of chemical stability. Determining the stability of molecules and condensed phases is the cornerstone of atomistic modeling, underpinning our understanding of chemical and materials properties and transformations. We show that a machine-learning model, based on a local description of chemical environments and Bayesian statistical learning, provides a unified framework to predict atomic-scale properties. It captures the quantum mechanical effects governing the complex surface reconstructions of silicon, predicts the stability of different classes of molecules with chemical accuracy, and distinguishes active and inactive protein ligands with more than 99% reliability. The universality and the systematic nature of our framework provide new insight into the potential energy surface of materials and molecules.\n",
            "\n",
            "Paper ID : 4ab891e6044695abb31c72048f654b3b205c60bb \tArticle : Survey of Machine Learning Algorithms for Disease Diagnostic\n",
            "Author(s) : ['Meherwar Fatima', 'M. Pasha']\n",
            "Year : 2017 \n",
            "Abstract : In medical imaging, Computer Aided Diagnosis (CAD) is a rapidly growing dynamic area of research. In recent years, significant attempts are made for the enhancement of computer aided diagnosis applications because errors in medical diagnostic systems can result in seriously misleading medical treatments. Machine learning is important in Computer Aided Diagnosis. After using an easy equation, objects such as organs may not be indicated accurately. So, pattern recognition fundamentally involves learning from examples. In the field of bio-medical, pattern recognition and machine learning promise the improved accuracy of perception and diagnosis of disease. They also promote the objectivity of decision-making process. For the analysis of high-dimensional and multimodal bio-medical data, machine learning offers a worthy approach for making classy and automatic algorithms. This survey paper provides the comparative analysis of different machine learning algorithms for diagnosis of different diseases such as heart disease, diabetes disease, liver disease, dengue disease and hepatitis disease. It brings attention towards the suite of machine learning algorithms and tools that are used for the analysis of diseases and decision-making process accordingly.\n",
            "------------------------------------Extracting Page #22------------------------------------\n",
            "\n",
            "Paper ID : f615bd164110160e160c98f59d7bfcc931a3cdc1 \tArticle : Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution\n",
            "Author(s) : ['J. Pearl']\n",
            "Year : 2018 \n",
            "Abstract : Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.\n",
            "\n",
            "Paper ID : 959c9dbfceda3825787f75f63fd7c87f332dc271 \tArticle : Machine Learning-Based Sentiment Analysis for Twitter Accounts\n",
            "Author(s) : ['A. Hasan', 'Sana Moin', 'Ahmad Karim', 'Shahaboddin Shamshirband']\n",
            "Year : 2018 \n",
            "Abstract : Growth in the area of opinion mining and sentiment analysis has been rapid and aims to explore the opinions or text present on different platforms of social media through machine-learning techniques with sentiment, subjectivity analysis or polarity calculations. Despite the use of various machine-learning techniques and tools for sentiment analysis during elections, there is a dire need for a state-of-the-art approach. To deal with these challenges, the contribution of this paper includes the adoption of a hybrid approach that involves a sentiment analyzer that includes machine learning. Moreover, this paper also provides a comparison of techniques of sentiment analysis in the analysis of political views by applying supervised machine-learning algorithms such as Naive Bayes and support vector machines (SVM).\n",
            "\n",
            "Paper ID : 1d7ec1c2c7e60b8097cadaafc5d9380c0de6f287 \tArticle : Decoupled Classifiers for Group-Fair and Efficient Machine Learning\n",
            "Author(s) : ['C. Dwork', 'Nicole Immorlica', 'A. Kalai', 'Mark D. M. Leiserson']\n",
            "Year : 2018 \n",
            "Abstract : When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the näıve application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, which can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.\n",
            "\n",
            "Paper ID : ea7887fadc666d6faf92e569d4a10d994ee91297 \tArticle : iml: An R package for Interpretable Machine Learning\n",
            "Author(s) : ['Christoph Molnar', 'Giuseppe Casalicchio', 'B. Bischl']\n",
            "Year : 2018 \n",
            "Abstract : Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.\n",
            "\n",
            "Paper ID : e1f9ef01ab55d53349096a58d76fd0cfa7bb051d \tArticle : Quantum machine learning: a classical perspective\n",
            "Author(s) : ['C. Ciliberto', 'M. Herbster', 'Alessandro Davide Ialongo', 'M. Pontil', 'Andrea Rocchetto', 'S. Severini', 'L. Wossnig']\n",
            "Year : 2018 \n",
            "Abstract : Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning (ML) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical ML algorithms. Here we review the literature in quantum ML and discuss perspectives for a mixed readership of classical ML and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in ML are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed.\n",
            "\n",
            "Paper ID : bb0bec2f6836b97af51f280eb77895e7170f47c9 \tArticle : Machine learning at the energy and intensity frontiers of particle physics\n",
            "Author(s) : ['A. Radovic', 'Mike Williams', 'D. Rousseau', 'M. Kagan', 'D. Bonacorsi', 'A. Himmel', 'A. Aurisano', 'K. Terao', 'T. Wongjirad']\n",
            "Year : 2018 \n",
            "Abstract : Our knowledge of the fundamental particles of nature and their interactions is summarized by the standard model of particle physics. Advancing our understanding in this field has required experiments that operate at ever higher energies and intensities, which produce extremely large and information-rich data samples. The use of machine-learning techniques is revolutionizing how we interpret these data samples, greatly increasing the discovery potential of present and future experiments. Here we summarize the challenges and opportunities that come with the use of machine learning at the frontiers of particle physics.The application and development of machine-learning methods used in experiments at the frontiers of particle physics (such as the Large Hadron Collider) are reviewed, including recent advances based on deep learning.\n",
            "\n",
            "Paper ID : 95615c6bce2123f12e39c3d9eb293ebb759501aa \tArticle : Machine learning, social learning and the governance of self-driving cars\n",
            "Author(s) : ['J. Stilgoe']\n",
            "Year : 2018 \n",
            "Abstract : Self-driving cars, a quintessentially ‘smart’ technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking ‘Who is learning, what are they learning and how are they learning?’ Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. ‘Self-driving’ or ‘autonomous’ cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning.\n",
            "\n",
            "Paper ID : 1c3752586e7d746b13eb5b2784ac9fe53756b7fd \tArticle : Attractor reconstruction by machine learning.\n",
            "Author(s) : ['Zhixin Lu', 'B. Hunt', 'E. Ott']\n",
            "Year : 2018 \n",
            "Abstract : A machine-learning approach called \"reservoir computing\" has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.\n",
            "\n",
            "Paper ID : 22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd \tArticle : DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning\n",
            "Author(s) : ['Tianshi Chen', 'Zidong Du', 'Ninghui Sun', 'Jia Wang', 'Chengyong Wu', 'Yunji Chen', 'O. Temam']\n",
            "Year : 2014 \n",
            "Abstract : Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.\n",
            "\n",
            "Paper ID : b7621b358485eb3e34a874b97d1ecd6b5d5ac70b \tArticle : Machine learning methods for solar radiation forecasting: A review\n",
            "Author(s) : ['C. Voyant', 'G. Notton', 'S. Kalogirou', 'M. Nivet', 'C. Paoli', 'F. Motte', 'A. Fouilloy']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #23------------------------------------\n",
            "\n",
            "Paper ID : b7621b358485eb3e34a874b97d1ecd6b5d5ac70b \tArticle : Machine learning methods for solar radiation forecasting: A review\n",
            "Author(s) : ['C. Voyant', 'G. Notton', 'S. Kalogirou', 'M. Nivet', 'C. Paoli', 'F. Motte', 'A. Fouilloy']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : f4bfd0add23a9a8e06f5b1ac6780e713dc9d4eff \tArticle : Machine learning in catalysis\n",
            "Author(s) : ['J. Kitchin']\n",
            "Year : 2018 \n",
            "Abstract : Catalysis is a complex, multidimensional and multiscale field of research. Machine learning is helping to build better models, understand catalysis research and generate new knowledge about catalysis.\n",
            "\n",
            "Paper ID : 43d1fe40167c5f2ed010c8e06c8e008c774fd22b \tArticle : Non-convex Optimization for Machine Learning\n",
            "Author(s) : ['Prateek Jain', 'Purushottam Kar']\n",
            "Year : 2017 \n",
            "Abstract : A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately, structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks.  The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve.  A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization.  On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties.  This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. We hope that an insight into the inner workings of these methods will allow the reader to appreciate the unique marriage of task structure and generative models that allow these heuristic techniques to (provably) succeed. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.\n",
            "\n",
            "Paper ID : 6d67ddd0855c60ada2fb4151f0f944feffdaf357 \tArticle : Machine Learning from Theory to Algorithms: An Overview\n",
            "Author(s) : ['J. Alzubi', 'A. Nayyar', 'Akshi Kumar']\n",
            "Year : 2018 \n",
            "Abstract : The current SMAC (Social, Mobile, Analytic, Cloud) technology trend paves the way to a future in which intelligent machines, networked processes and big data are brought together. This virtual world has generated vast amount of data which is accelerating the adoption of machine learning solutions & practices. Machine Learning enables computers to imitate and adapt human-like behaviour. Using machine learning, each interaction, each action performed, becomes something the system can learn and use as experience for the next time. This work is an overview of this data analytics method which enables computers to learn and do what comes naturally to humans, i.e. learn from experience. It includes the preliminaries of machine learning, the definition, nomenclature and applications’ describing it’s what, how and why. The technology roadmap of machine learning is discussed to understand and verify its potential as a market & industry practice. The primary intent of this work is to give insight into why machine learning is the future.\n",
            "\n",
            "Paper ID : 95274ca3be569765960464d24f898c6fe025bac9 \tArticle : Machine learning applications in cancer prognosis and prediction\n",
            "Author(s) : ['Konstantina D. Kourou', 'T. Exarchos', 'K. Exarchos', 'M. Karamouzis', 'D. Fotiadis']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 60e801e3dfc9812e294ed9de6d579e0293d61643 \tArticle : Probabilistic machine learning and artificial intelligence\n",
            "Author(s) : ['Zoubin Ghahramani']\n",
            "Year : 2015 \n",
            "Abstract : How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.\n",
            "\n",
            "Paper ID : 46f74231b9afeb0c290d6d550043c55045284e5f \tArticle : The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]\n",
            "Author(s) : ['L. Deng']\n",
            "Year : 2012 \n",
            "Abstract : In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.\n",
            "\n",
            "Paper ID : 534362486b5ae4b839f124af3413032c95c0483e \tArticle : Instance spaces for machine learning classification\n",
            "Author(s) : ['Mario Andrés Muñoz', 'L. Villanova', 'Davaatseren Baatar', 'K. Smith‐Miles']\n",
            "Year : 2017 \n",
            "Abstract : This paper tackles the issue of objective performance evaluation of machine learning classifiers, and the impact of the choice of test instances. Given that statistical properties or features of a dataset affect the difficulty of an instance for particular classification algorithms, we examine the diversity and quality of the UCI repository of test instances used by most machine learning researchers. We show how an instance space can be visualized, with each classification dataset represented as a point in the space. The instance space is constructed to reveal pockets of hard and easy instances, and enables the strengths and weaknesses of individual classifiers to be identified. Finally, we propose a methodology to generate new test instances with the aim of enriching the diversity of the instance space, enabling potentially greater insights than can be afforded by the current UCI repository.\n",
            "\n",
            "Paper ID : 192f45d07d16c47e8194e1e3d00ec8c8b05f128c \tArticle : Materials discovery and design using machine learning\n",
            "Author(s) : ['Yue Liu', 'Tianlu Zhao', 'Wangwei Ju', 'S. Shi']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 079b2cfe950a96d5a43a3febc983d151fb533b53 \tArticle : Machine learning on big data: Opportunities and challenges\n",
            "Author(s) : ['Lina Zhou', 'Shimei Pan', 'Jianwu Wang', 'A. Vasilakos']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #24------------------------------------\n",
            "\n",
            "Paper ID : 079b2cfe950a96d5a43a3febc983d151fb533b53 \tArticle : Machine learning on big data: Opportunities and challenges\n",
            "Author(s) : ['Lina Zhou', 'Shimei Pan', 'Jianwu Wang', 'A. Vasilakos']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 3af7252f7e3ca8e9bc8f45e6cbf567b10ecb5d95 \tArticle : Machine learning models and bankruptcy prediction\n",
            "Author(s) : ['Flavio Barboza', 'H. Kimura', 'E. Altman']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 561269a24f2f2a06409109723a8ab93a01696efc \tArticle : Federated Optimization: Distributed Machine Learning for On-Device Intelligence\n",
            "Author(s) : ['Jakub Konecný', 'H. B. McMahan', 'D. Ramage', 'Peter Richtárik']\n",
            "Year : 2016 \n",
            "Abstract : We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. \n",
            "A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. \n",
            "We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \\federated optimization.\n",
            "\n",
            "Paper ID : 4f975da00a5b2a2f7236e34edcb7274e5fdab937 \tArticle : Combining satellite imagery and machine learning to predict poverty\n",
            "Author(s) : ['Neal Jean', 'M. Burke', 'Sang Michael Xie', 'W. Davis', 'D. Lobell', 'S. Ermon']\n",
            "Year : 2016 \n",
            "Abstract : Measuring consumption and wealth remotely Nighttime lighting is a rough proxy for economic wealth, and nighttime maps of the world show that many developing countries are sparsely illuminated. Jean et al. combined nighttime maps with high-resolution daytime satellite images (see the Perspective by Blumenstock). With a bit of machine-learning wizardry, the combined images can be converted into accurate estimates of household consumption and assets, both of which are hard to measure in poorer countries. Furthermore, the night- and day-time data are publicly available and nonproprietary. Science, this issue p. 790; see also p. 753 Satellites collect data that can be used to measure income and wealth. Reliable data on economic livelihoods remain scarce in the developing world, hampering efforts to study these outcomes and to design policies that improve them. Here we demonstrate an accurate, inexpensive, and scalable method for estimating consumption expenditure and asset wealth from high-resolution satellite imagery. Using survey and satellite data from five African countries—Nigeria, Tanzania, Uganda, Malawi, and Rwanda—we show how a convolutional neural network can be trained to identify image features that can explain up to 75% of the variation in local-level economic outcomes. Our method, which requires only publicly available data, could transform efforts to track and target poverty in developing countries. It also demonstrates how powerful machine learning techniques can be applied in a setting with limited training data, suggesting broad potential application across many scientific domains.\n",
            "\n",
            "Paper ID : 54648b90b4bc3c94fdb9340598161993a13ecbc8 \tArticle : Introduction to Machine Learning and R\n",
            "Author(s) : ['K. Ramasubramanian', 'Abhishek Singh']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning played a pivotal role in transforming statistics into a more accessible subject by showing the applications to the real-world problems. However, many statisticians probably won't agree with machine learning giving life to statistics, giving rise to the never-ending chicken and egg conundrum kind of discussions.\n",
            "\n",
            "Paper ID : 56703e0ccba03378962f5006f299cd98d48198f9 \tArticle : Smart Machining Process Using Machine Learning: A Review and Perspective on Machining Industry\n",
            "Author(s) : ['Dong-Hyeon Kim', 'T. J. Y. Kim', 'Xinlin Wang', 'Min-Cheol Kim', 'Ying-Jun Quan', 'Jin Woo Oh', 'S. Min', 'Hyungjung Kim', 'B. Bhandari', 'Insoon Yang', 'Sung-hoon Ahn']\n",
            "Year : 2018 \n",
            "Abstract : The Fourth Industrial Revolution incorporates the digital revolution into the physical world, creating a new direction in a number of fields, including artificial intelligence, quantum computing, nanotechnology, biotechnology, robotics, 3D printing, autonomous vehicles, and the Internet of Things. The artificial intelligence field has encountered a turning point mainly due to advancements in machine learning, which allows machines to learn, improve, and perform a specific task through data without being explicitly programmed. Machine learning can be utilized with machining processes to improve product quality levels and productivity rates, to monitor the health of systems, and to optimize design and process parameters. This is known as smart machining, referring to a new machining paradigm in which machine tools are fully connected through a cyber-physical system. This paper reviews and summarizes machining processes using machine learning algorithms and suggests a perspective on the machining industry.\n",
            "\n",
            "Paper ID : f2df0c1026ffa474f603a535e48e5c115d3d8629 \tArticle : Extreme learning machine: Theory and applications\n",
            "Author(s) : ['G. Huang', 'Q. Zhu', 'C. Siew']\n",
            "Year : 2006 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 91e36e1c6e3d4a5bdc97fa5ab5b89bdf9113413d \tArticle : How the machine ‘thinks’: Understanding opacity in machine learning algorithms\n",
            "Author(s) : ['J. Burrell']\n",
            "Year : 2016 \n",
            "Abstract : This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.\n",
            "\n",
            "Paper ID : 1ed33896e82cc3810b349cdffc2039f0b8edd82e \tArticle : Deep learning and its applications to machine health monitoring\n",
            "Author(s) : ['Rui Zhao', 'Ruqiang Yan', 'Zhenghua Chen', 'K. Mao', 'Peng Wang', 'R. Gao']\n",
            "Year : 2019 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 3ed23970d9e4dd278741318a8c253117e030426d \tArticle : Encyclopedia of Machine Learning and Data Mining\n",
            "Author(s) : ['C. Sammut', 'Geoffrey I. Webb']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #25------------------------------------\n",
            "\n",
            "Paper ID : 971766088dfaf63fb55e6f0190b14f28f2c98ad0 \tArticle : A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection\n",
            "Author(s) : ['A. Buczak', 'Erhan Guven']\n",
            "Year : 2016 \n",
            "Abstract : This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.\n",
            "\n",
            "Paper ID : a749fccbcfc0382f26789ce0b9a03fa98b9e608c \tArticle : Programs for Machine Learning. Part I\n",
            "Author(s) : ['A. Hormann']\n",
            "Year : 1962 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : f5c7d0998b4cf8de9b5b171e3593427724ec4600 \tArticle : Lifelong Machine Learning, Second Edition\n",
            "Author(s) : ['Zhiyuan Chen', 'B. Liu']\n",
            "Year : 2018 \n",
            "Abstract : Lifelong Machine Learning, Second Edition is an introduction to an advanced machine learning paradigm that continuously learns by accumulating past knowledge that it then uses in future learning and problem solving. In contrast, the current dominant machine learning paradigm learns in isolation: given a training dataset, it runs a machine learning algorithm on the dataset to produce a model that is then used in its intended application. It makes no attempt to retain the learned knowledge and use it in subsequent learning. Unlike this isolated system, humans learn effectively with only a few examples precisely because our learning is very knowledge-driven: the knowledge learned in the past helps us learn new things with little data or effort. Lifelong learning aims to emulate this capability, because without it, an AI system cannot be considered truly intelligent. Research in lifelong learning has developed significantly in the relatively short time since the first edition of this book was published. The purpose of this second edition is to expand the definition of lifelong learning, update the content of several chapters, and add a new chapter about continual learning in deep neural networks—which has been actively researched over the past two or three years. A few chapters have also been reorganized to make each of them more coherent for the reader. Moreover, the authors want to propose a unified framework for the research area. Currently, there are several research topics in machine learning that are closely related to lifelong learning—most notably, multi-task learning, transfer learning, and meta-learning—because they also employ the idea of knowledge sharing and transfer. This book brings all these topics under one roof and discusses their similarities and differences. Its goal is to introduce this emerging machine learning paradigm and present a comprehensive survey and review of the important research results and latest ideas in the area. This book is thus suitable for students, researchers, and practitioners who are interested in machine learning, data mining, natural language processing, or pattern recognition. Lecturers can readily use the book for courses in any of these related fields.\n",
            "\n",
            "Paper ID : be08c04637a5f0ba13bfb52c670dbeb227dcb4cc \tArticle : PMLB: a large benchmark suite for machine learning evaluation and comparison\n",
            "Author(s) : ['Randal S. Olson', 'W. L. Cava', 'P. Orzechowski', 'R. Urbanowicz', 'J. Moore']\n",
            "Year : 2017 \n",
            "Abstract : BackgroundThe selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists.ResultsThe present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. From this study, we find that existing benchmarks lack the diversity to properly benchmark machine learning algorithms, and there are several gaps in benchmarking problems that still need to be considered.ConclusionsThis work represents another important step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.\n",
            "\n",
            "Paper ID : 2f6000c3be00335633cd490f03d642fcf101cd0d \tArticle : Machine Learning Predicts Laboratory Earthquakes\n",
            "Author(s) : ['B. Rouet-Leduc', 'C. Hulbert', 'N. Lubbers', 'K. Barros', 'C. Humphreys', 'P. Johnson']\n",
            "Year : 2017 \n",
            "Abstract : We apply machine learning to data sets from shear laboratory experiments, with the goal of identifying hidden signals that precede earthquakes. Here we show that by listening to the acoustic signal emitted by a laboratory fault, machine learning can predict the time remaining before it fails with great accuracy. These predictions are based solely on the instantaneous physical characteristics of the acoustical signal and do not make use of its history. Surprisingly, machine learning identifies a signal emitted from the fault zone previously thought to be low‐amplitude noise that enables failure forecasting throughout the laboratory quake cycle. We infer that this signal originates from continuous grain motions of the fault gouge as the fault blocks displace. We posit that applying this approach to continuous seismic data may lead to significant advances in identifying currently unknown signals, in providing new insights into fault physics, and in placing bounds on fault failure times.\n",
            "\n",
            "Paper ID : 31fbba3638f0d930a678d85247045ea5a1a4ec88 \tArticle : Unsupervised Machine Learning on a Hybrid Quantum Computer\n",
            "Author(s) : ['J. Otterbach', 'R. Manenti', 'N. Alidoust', 'A. Bestwick', 'M. Block', 'B. Bloom', 'S. Caldwell', 'N. Didier', 'E. Fried', 'S. Hong', 'Peter J. Karalekas', 'C. Osborn', 'A. Papageorge', 'E. C. Peterson', 'G. Prawiroatmodjo', 'N. Rubin', 'C. Ryan', 'D. Scarabelli', 'M. Scheer', 'E. A. Sete', 'P. Sivarajah', 'Robert S. Smith', 'A. Staley', 'N. Tezak', 'W. Zeng', 'A. Hudson', 'Blake R. Johnson', 'M. Reagor', 'M. Silva', 'C. Rigetti']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning techniques have led to broad adoption of a statistical model of computing. The statistical distributions natively available on quantum processors are a superset of those available classically. Harnessing this attribute has the potential to accelerate or otherwise improve machine learning relative to purely classical performance. A key challenge toward that goal is learning to hybridize classical computing resources and traditional learning techniques with the emerging capabilities of general purpose quantum processors. Here, we demonstrate such hybridization by training a 19-qubit gate model processor to solve a clustering problem, a foundational challenge in unsupervised learning. We use the quantum approximate optimization algorithm in conjunction with a gradient-free Bayesian optimization to train the quantum machine. This quantum/classical hybrid algorithm shows robustness to realistic noise, and we find evidence that classical optimization can be used to train around both coherent and incoherent imperfections.\n",
            "\n",
            "Paper ID : a5c0309b9895066ebd08acfe326b01ce2fdefdd4 \tArticle : Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges\n",
            "Author(s) : ['B. Goldstein', 'A. Navar', 'R. Carter']\n",
            "Year : 2017 \n",
            "Abstract : Abstract Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.\n",
            "\n",
            "Paper ID : 364fb0677a5d7083e56c0e38629a78cb94836f53 \tArticle : API design for machine learning software: experiences from the scikit-learn project\n",
            "Author(s) : ['L. Buitinck', 'Gilles Louppe', 'Mathieu Blondel', 'Fabian Pedregosa', 'Andreas Mueller', 'O. Grisel', 'Vlad Niculae', 'P. Prettenhofer', 'Alexandre Gramfort', 'Jaques Grobler', 'R. Layton', 'J. Vanderplas', 'Arnaud Joly', 'Brian Holt', 'G. Varoquaux']\n",
            "Year : 2013 \n",
            "Abstract : Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.\n",
            "\n",
            "Paper ID : fdd025e077a36166b10120b448d0c4e4009824a9 \tArticle : Model-Agnostic Interpretability of Machine Learning\n",
            "Author(s) : ['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']\n",
            "Year : 2016 \n",
            "Abstract : Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.\n",
            "\n",
            "Paper ID : dd308eb0d7be24e593fe355476057fc37ab5bf0e \tArticle : Machine learning phases of matter\n",
            "Author(s) : ['J. Carrasquilla', 'R. Melko']\n",
            "Year : 2016 \n",
            "Abstract : The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order. Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits1. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the ‘curse of dimensionality’ commonly encountered in machine learning2. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks3, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries4,5, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo6,7.\n",
            "------------------------------------Extracting Page #26------------------------------------\n",
            "\n",
            "Paper ID : fdd025e077a36166b10120b448d0c4e4009824a9 \tArticle : Model-Agnostic Interpretability of Machine Learning\n",
            "Author(s) : ['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']\n",
            "Year : 2016 \n",
            "Abstract : Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.\n",
            "\n",
            "Paper ID : dd308eb0d7be24e593fe355476057fc37ab5bf0e \tArticle : Machine learning phases of matter\n",
            "Author(s) : ['J. Carrasquilla', 'R. Melko']\n",
            "Year : 2016 \n",
            "Abstract : The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order. Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits1. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the ‘curse of dimensionality’ commonly encountered in machine learning2. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks3, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries4,5, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo6,7.\n",
            "\n",
            "Paper ID : db8c3cfaae04a14c1209d62953029b6fa53e23c7 \tArticle : Challenges in representation learning: A report on three machine learning contests\n",
            "Author(s) : ['Ian J. Goodfellow', 'D. Erhan', 'P. Carrier', 'Aaron C. Courville', 'Mehdi Mirza', 'Benjamin Hamner', 'William J. Cukierski', 'Yichuan Tang', 'David Thaler', 'Dong-Hyun Lee', 'Yingbo Zhou', 'Chetan Ramaiah', 'Fangxiang Feng', 'Ruifan Li', 'Xiaojie Wang', 'Dimitris Athanasakis', 'J. Shawe-Taylor', 'Maxim Milakov', 'John Park', 'Radu Tudor Ionescu', 'M. Popescu', 'C. Grozea', 'J. Bergstra', 'Jingjing Xie', 'Lukasz Romaszko', 'Bing Xu', 'Chuang Zhang', 'Yoshua Bengio']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 005efab1573f982a2c53dafc6305342f71cd228a \tArticle : Machine Learning in Medical Imaging.\n",
            "Author(s) : ['M. Giger']\n",
            "Year : 2018 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 6d0fb3a5ad66c83e9f2ef066d83f0ae23180da41 \tArticle : Distributed GraphLab : A Framework for Machine Learning and Data Mining in the Cloud\n",
            "Author(s) : ['Y. Low', 'Joseph Gonzalez', 'Aapo Kyrola', 'D. Bickson', 'Carlos Guestrin', 'J. Hellerstein']\n",
            "Year : 2012 \n",
            "Abstract : While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.\n",
            "\n",
            "Paper ID : 139b2afafdaccba02c983d2f40f257db64860320 \tArticle : Machine Learning Topological States\n",
            "Author(s) : ['D. Deng', 'Xiaopeng Li', 'S. Sarma']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning, the core of artificial intelligence and data science, is a very active field, with vast applications throughout science and technology. Recently, machine learning techniques have been adopted to tackle intricate quantum many-body problems and phase transitions. In this work, the authors construct exact mappings from exotic quantum states to machine learning network models. This work shows for the first time that the restricted Boltzmann machine can be used to study both symmetry-protected topological phases and intrinsic topological order. The exact results are expected to provide a substantial boost to the field of machine learning of phases of matter.\n",
            "\n",
            "Paper ID : 259bd09bc382763f864986498e46ab0178714f58 \tArticle : Lifelong Machine Learning\n",
            "Author(s) : ['Zhiyuan Chen', 'B. Liu']\n",
            "Year : 2016 \n",
            "Abstract : Lifelong Machine Learning (or Lifelong Learning) is an advanced machine learning paradigm that learns continuously, accumulates the knowledge learned in previous tasks, and uses it to help future learning. In the process, the learner becomes more and more knowledgeable and effective at learning. This learning ability is one of the hallmarks of human intelligence. However, the current dominant machine learning paradigm learns in isolation: given a training dataset, it runs a machine learning algorithm on the dataset to produce a model. It makes no attempt to retain the learned knowledge and use it in future learning. Although this isolated learning paradigm has been very successful, it requires a large number of training examples, and is only suitable for well-defined and narrow tasks. In comparison, we humans can learn effectively with a few examples because we have accumulated so much knowledge in the past which enables us to learn with little data or effort. Lifelong learning aims to achieve this capability. As statistical machine learning matures, it is time to make a major effort to break the isolated learning tradition and to study lifelong learning to bring machine learning to new heights. Applications such as intelligent assistants, chatbots, and physical robots that interact with humans and systems in real-life environments are also calling for such lifelong learning capabilities. Without the ability to accumulate the learned knowledge and use it to learn more knowledge incrementally, a system will probably never be truly intelligent. This book serves as an introductory text and survey to lifelong learning.\n",
            "\n",
            "Paper ID : 4ec306ab408839fa060cfe4f6ce969c43df65ed9 \tArticle : Machine Learning Phases of Strongly Correlated Fermions\n",
            "Author(s) : [\"K. Ch'ng\", 'J. Carrasquilla', 'R. Melko', 'E. Khatami']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning offers an unprecedented perspective for the problem of classifying phases in condensed matter physics. We employ neural-network machine learning techniques to distinguish finite-temperature phases of the strongly correlated fermions on cubic lattices. We show that a three dimensional convolutional network trained on auxiliary field configurations produced by quantum Monte Carlo simulations of the Hubbard model can correctly predict the magnetic phase diagram of the model at the average density of one (half filling). We then use the network, trained at half filling, to explore the trend in the transition temperature as the system is doped away from half filling. This transfer learning approach predicts that the instability to the magnetic phase extends to at least 5% doping in this region. Our results pave the way for other machine learning applications in correlated quantum many-body systems.\n",
            "\n",
            "Paper ID : 528ecb0f88a9ea6110ba309b98cc2f0678f257c9 \tArticle : Data Mining Practical Machine Learning Tools And Techniques With Java Implementations\n",
            "Author(s) : ['Marcel Abendroth']\n",
            "Year : 2016 \n",
            "Abstract : Thank you for reading data mining practical machine learning tools and techniques with java implementations. As you may know, people have look hundreds times for their favorite novels like this data mining practical machine learning tools and techniques with java implementations, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their laptop.\n",
            "\n",
            "Paper ID : f488e4a252c018f9391b0dc90036687a0b361844 \tArticle : Implementing Machine Learning in Radiology Practice and Research.\n",
            "Author(s) : ['M. Kohli', 'L. Prevedello', 'Ross W. Filice', 'J. R. Geis']\n",
            "Year : 2017 \n",
            "Abstract : OBJECTIVE\n",
            "The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects, including common algorithms, supervised as opposed to unsupervised techniques, statistical pitfalls, and data considerations for training and evaluation, and to briefly describe ethical dilemmas and legal risk.\n",
            "\n",
            "\n",
            "CONCLUSION\n",
            "Machine learning includes a broad class of computer programs that improve with experience. The complexity of creating, training, and monitoring machine learning indicates that the success of the algorithms will require radiologist involvement for years to come, leading to engagement rather than replacement.\n",
            "------------------------------------Extracting Page #27------------------------------------\n",
            "\n",
            "Paper ID : 7aaede70f5efcb1542a80707c1f0f8b01955a7d2 \tArticle : Oblivious Multi-Party Machine Learning on Trusted Processors\n",
            "Author(s) : ['O. Ohrimenko', 'Felix Schuster', 'C. Fournet', 'Aastha Mehta', 'S. Nowozin', 'Kapil Vaswani', 'Manuel Costa']\n",
            "Year : 2016 \n",
            "Abstract : Privacy-preserving multi-party machine learning allows multiple organizations to perform collaborative data analytics while guaranteeing the privacy of their individual datasets. Using trusted SGX-processors for this task yields high performance, but requires a careful selection, adaptation, and implementation of machine-learning algorithms to provably prevent the exploitation of any side channels induced by data-dependent access patterns. \n",
            " \n",
            "We propose data-oblivious machine learning algorithms for support vector machines, matrix factorization, neural networks, decision trees, and k-means clustering. We show that our efficient implementation based on Intel Skylake processors scales up to large, realistic datasets, with overheads several orders of magnitude lower than with previous approaches based on advanced cryptographic multi-party computation schemes.\n",
            "\n",
            "Paper ID : a10bc90b3c97a4abe86c73cfb2a8490a9b44373f \tArticle : A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials\n",
            "Author(s) : ['Logan T. Ward', 'Ankit Agrawal', 'A. Choudhary', 'C. Wolverton']\n",
            "Year : 2016 \n",
            "Abstract : A very active area of materials research is to devise methods that use machine learning to automatically extract predictive models from existing materials data. While prior examples have demonstrated successful models for some applications, many more applications exist where machine learning can make a strong impact. To enable faster development of machine-learning-based models for such applications, we have created a framework capable of being applied to a broad range of materials data. Our method works by using a chemically diverse list of attributes, which we demonstrate are suitable for describing a wide variety of properties, and a novel method for partitioning the data set into groups of similar materials in order to boost the predictive accuracy. In this manuscript, we demonstrate how this new method can be used to predict diverse properties of crystalline and amorphous materials, such as band gap energy and glass-forming ability.\n",
            "\n",
            "Paper ID : 441c31274f4535a4a50892c1ad6e19eacfd17f8c \tArticle : Perspective: Machine learning potentials for atomistic simulations.\n",
            "Author(s) : ['J. Behler']\n",
            "Year : 2016 \n",
            "Abstract : Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.\n",
            "\n",
            "Paper ID : 09755f549468209199565f8037061281080c968f \tArticle : Interactive machine learning for health informatics: when do we need the human-in-the-loop?\n",
            "Author(s) : ['Andreas Holzinger']\n",
            "Year : 2016 \n",
            "Abstract : Machine learning (ML) is the fastest growing field in computer science, and health informatics is among the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic machine learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive machine learning (iML) may be of help, having its roots in reinforcement learning, preference learning, and active learning. The term iML is not yet well used, so we define it as “algorithms that can interact with agents and can optimize their learning behavior through these interactions, where the agents can also be human.” This “human-in-the-loop” can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.\n",
            "\n",
            "Paper ID : 9d46dc975aeed3f96bddb144079b50238f746ecd \tArticle : Machine learning in manufacturing: advantages, challenges, and applications\n",
            "Author(s) : ['T. Wuest', 'Daniel Weimer', 'C. Irgens', 'K. Thoben']\n",
            "Year : 2016 \n",
            "Abstract : The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.\n",
            "\n",
            "Paper ID : 27245e65a27bde90b5b0bb25d157bb75a0ad8b5a \tArticle : A survey of machine learning for big data processing\n",
            "Author(s) : ['Junfei Qiu', 'Qi-hui Wu', 'Guoru Ding', 'Yuhua Xu', 'S. Feng']\n",
            "Year : 2016 \n",
            "Abstract : There is no doubt that big data are now rapidly expanding in all science and engineering domains. While the potential of these massive data is undoubtedly significant, fully making sense of them requires new ways of thinking and novel learning techniques to address the various challenges. In this paper, we present a literature survey of the latest advances in researches on machine learning for big data processing. First, we review the machine learning techniques and highlight some promising learning methods in recent studies, such as representation learning, deep learning, distributed and parallel learning, transfer learning, active learning, and kernel-based learning. Next, we focus on the analysis and discussions about the challenges and possible solutions of machine learning for big data. Following that, we investigate the close connections of machine learning with signal processing techniques for big data processing. Finally, we outline several open issues and research trends.\n",
            "\n",
            "Paper ID : ebab687cd1be7d25392c11f89fce6a63bef7219d \tArticle : Towards the Science of Security and Privacy in Machine Learning\n",
            "Author(s) : ['Nicolas Papernot', 'P. Mcdaniel', 'Arunesh Sinha', 'Michael P. Wellman']\n",
            "Year : 2016 \n",
            "Abstract : Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community’s understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.\n",
            "\n",
            "Paper ID : c87a4433c57ddabd50f32ca2c2d2197244692106 \tArticle : mlr: Machine Learning in R\n",
            "Author(s) : ['B. Bischl', 'Michel Lang', 'Lars Kotthoff', 'J. Schiffner', 'Jakob Richter', 'Erich Studerus', 'Giuseppe Casalicchio', 'Zachary M. Jones']\n",
            "Year : 2016 \n",
            "Abstract : The MLR package provides a generic, object-oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.\n",
            "\n",
            "Paper ID : c396ba3ce3c58591b8ee282e65ab5a6d610e16ed \tArticle : Machine Teaching: A New Paradigm for Building Machine Learning Systems\n",
            "Author(s) : ['P. Simard', 'Saleema Amershi', 'D. M. Chickering', 'Alicia Edelman Pelton', 'S. Ghorashi', 'Christopher Meek', 'Gonzalo A. Ramos', 'Jina Suh', 'J. Verwey', 'Mo Wang', 'J. Wernsing']\n",
            "Year : 2017 \n",
            "Abstract : The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. \n",
            "While machine learning focuses on creating new algorithms and improving the accuracy of \"learners\", the machine teaching discipline focuses on the efficacy of the \"teachers\". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. \n",
            "In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.\n",
            "\n",
            "Paper ID : 77233d2f6fd10465a574ca33b869707822bf0c0b \tArticle : A brief survey of machine learning methods and their sensor and IoT applications\n",
            "Author(s) : ['U. Shanthamallu', 'A. Spanias', 'C. Tepedelenlioğlu', 'M. Stanley']\n",
            "Year : 2017 \n",
            "Abstract : This paper provides a brief survey of the basic concepts and algorithms used for Machine Learning and its applications. We begin with a broader definition of machine learning and then introduce various learning modalities including supervised and unsupervised methods and deep learning paradigms. In the rest of the paper, we discuss applications of machine learning algorithms in various fields including pattern recognition, sensor networks, anomaly detection, Internet of Things (IoT) and health monitoring. In the final sections, we present some of the software tools and an extensive bibliography.\n",
            "------------------------------------Extracting Page #28------------------------------------\n",
            "\n",
            "Paper ID : 44479cc5266788c3bafcc0b12ef0758827741fe3 \tArticle : Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View\n",
            "Author(s) : ['Wei Luo', 'Dinh Phung', 'T. Tran', 'Sunil Gupta', 'S. Rana', 'C. Karmakar', 'A. Shilton', 'J. Yearwood', 'N. Dimitrova', 'T. Ho', 'S. Venkatesh', 'M. Berk']\n",
            "Year : 2016 \n",
            "Abstract : Background As more and more researchers are turning to big data for new opportunities of biomedical discoveries, machine learning models, as the backbone of big data analysis, are mentioned more often in biomedical journals. However, owing to the inherent complexity of machine learning methods, they are prone to misuse. Because of the flexibility in specifying machine learning models, the results are often insufficiently reported in research articles, hindering reliable assessment of model validity and consistent interpretation of model outputs. Objective To attain a set of guidelines on the use of machine learning predictive models within clinical settings to make sure the models are correctly applied and sufficiently reported so that true discoveries can be distinguished from random coincidence. Methods A multidisciplinary panel of machine learning experts, clinicians, and traditional statisticians were interviewed, using an iterative process in accordance with the Delphi method. Results The process produced a set of guidelines that consists of (1) a list of reporting items to be included in a research article and (2) a set of practical sequential steps for developing predictive models. Conclusions A set of guidelines was generated to enable correct application of machine learning models and consistent reporting of model specifications and results in biomedical research. We believe that such guidelines will accelerate the adoption of big data analysis, particularly with machine learning methods, in the biomedical research community.\n",
            "\n",
            "Paper ID : 033c08ca48aaed2d5ab0a17d668d410538678ed8 \tArticle : Evasion Attacks against Machine Learning at Test Time\n",
            "Author(s) : ['B. Biggio', 'I. Corona', 'Davide Maiorca', 'Blaine Nelson', 'Nedim Srndic', 'P. Laskov', 'G. Giacinto', 'F. Roli']\n",
            "Year : 2013 \n",
            "Abstract : In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.\n",
            "\n",
            "Paper ID : 1eb131a34fbb508a9dd8b646950c65901d6f1a5b \tArticle : Hidden Technical Debt in Machine Learning Systems\n",
            "Author(s) : ['D. Sculley', 'Gary Holt', 'D. Golovin', 'Eugene Davydov', 'Todd Phillips', 'D. Ebner', 'Vinay Chaudhary', 'Michael Young', 'J. Crespo', 'Dan Dennison']\n",
            "Year : 2015 \n",
            "Abstract : Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.\n",
            "\n",
            "Paper ID : 9e6060316394393c226b5c86ce51b06c4c75bee1 \tArticle : Machine Learning Classification over Encrypted Data\n",
            "Author(s) : ['Raphael Bost', 'R. A. Popa', 'Stephen Tu', 'S. Goldwasser']\n",
            "Year : 2015 \n",
            "Abstract : Machine learning classification is used for numerous tasks nowadays, such as medical or genomics predictions, spam detection, face recognition, and financial predictions. Due to privacy concerns, in some of these applications, it is important that the data and the classifier remain confidential. In this work, we construct three major classification protocols that satisfy this privacy constraint: hyperplane decision, Naïve Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis of these constructions is a new library of building blocks, which enables constructing a wide range of privacy-preserving classifiers; we demonstrate how this library can be used to construct other classifiers than the three mentioned above, such as a multiplexer and a face detection classifier. We implemented and evaluated our library and our classifiers. Our protocols are efficient, taking milliseconds to a few seconds to perform a classification when running on real medical datasets.\n",
            "\n",
            "Paper ID : 9873d43696165b50fab955d27b9dde838c0a0152 \tArticle : Machine learning applications in genetics and genomics\n",
            "Author(s) : ['Maxwell W. Libbrecht', 'William Stafford Noble']\n",
            "Year : 2015 \n",
            "Abstract : The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.\n",
            "\n",
            "Paper ID : e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772 \tArticle : Some Studies in Machine Learning Using the Game of Checkers\n",
            "Author(s) : ['A. Samuel']\n",
            "Year : 1959 \n",
            "Abstract : Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called “alpha-beta” pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the\n",
            "\n",
            "Paper ID : 4c80f5f68cdb02213ca018c999e882135d01c180 \tArticle : Machine Learning with R\n",
            "Author(s) : ['A. Ghatak']\n",
            "Year : 2017 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 233e1651094717e3df60d231c65000eb2105e283 \tArticle : Python Machine Learning\n",
            "Author(s) : ['S. Raschka']\n",
            "Year : 2015 \n",
            "Abstract : Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.\n",
            "\n",
            "Paper ID : af56f16bbfcecb9cefb13f228d1d867388296e64 \tArticle : Machine Learning\n",
            "Author(s) : ['Srinivasaraghavan']\n",
            "Year : 2019 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 8f04029d1d83f41eaebf5a216ebecf2a61ff6dc0 \tArticle : Selection of Relevant Features and Examples in Machine Learning\n",
            "Author(s) : ['A. Blum', 'P. Langley']\n",
            "Year : 1997 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #29------------------------------------\n",
            "\n",
            "Paper ID : 64cd22264d9beb3f7243acfd49e34e3b7b55292f \tArticle : Machine Learning\n",
            "Author(s) : ['U. Müller']\n",
            "Year : 2019 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 8f04029d1d83f41eaebf5a216ebecf2a61ff6dc0 \tArticle : Selection of Relevant Features and Examples in Machine Learning\n",
            "Author(s) : ['A. Blum', 'P. Langley']\n",
            "Year : 1997 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 92ace17730c2173e642934d64f96d359697b7a93 \tArticle : Bayesian reasoning and machine learning\n",
            "Author(s) : ['D. Barber']\n",
            "Year : 2012 \n",
            "Abstract : Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.\n",
            "\n",
            "Paper ID : f15367ed93c3505b1d62d802f3f4b769ae0f4ba5 \tArticle : Machine learning for neuroimaging with scikit-learn\n",
            "Author(s) : ['A. Abraham', 'Fabian Pedregosa', 'Michael Eickenberg', 'Philippe Gervais', 'Andreas Mueller', 'Jean Kossaifi', 'Alexandre Gramfort', 'B. Thirion', 'G. Varoquaux']\n",
            "Year : 2014 \n",
            "Abstract : Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.\n",
            "\n",
            "Paper ID : 184d770ef86cf302836bc8873ea9c03acc454bf1 \tArticle : TPOT: A Tree-based Pipeline Optimization Tool for Automating Machine Learning\n",
            "Author(s) : ['Randal S. Olson', 'J. Moore']\n",
            "Year : 2016 \n",
            "Abstract : As data science becomes increasingly mainstream, there will be an ever-growing demand for data science tools that are more accessible, flexible, and scalable. In response to this demand, automated machine learning (AutoML) researchers have begun building systems that automate the process of designing and optimizing machine learning pipelines. In this chapter we present TPOT v0.3, an open source genetic programming-based AutoML system that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification task. We benchmark TPOT on a series of 150 supervised classification tasks and find that it significantly outperforms a basic machine learning analysis in 21 of them, while experiencing minimal degradation in accuracy on 4 of the benchmarks—all without any domain knowledge nor human input. As such, genetic programming-based AutoML systems show considerable promise in the AutoML domain.\n",
            "\n",
            "Paper ID : d68725804eadecf83d707d89e12c5132bf376187 \tArticle : Sparse Bayesian Learning and the Relevance Vector Machine\n",
            "Author(s) : ['George Eastman House', 'Guildhall StreetCambridge']\n",
            "Year : 2001 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : ebe14ab38c7bc8187737c8aed7fef3d7cd2becf7 \tArticle : Machine Learning methods for Quantitative Radiomic Biomarkers\n",
            "Author(s) : ['C. Parmar', 'P. Grossmann', 'J. Bussink', 'P. Lambin', 'H. Aerts']\n",
            "Year : 2015 \n",
            "Abstract : Radiomics extracts and mines large number of medical imaging features quantifying tumor phenotypic characteristics. Highly accurate and reliable machine-learning approaches can drive the success of radiomic applications in clinical care. In this radiomic study, fourteen feature selection methods and twelve classification methods were examined in terms of their performance and stability for predicting overall survival. A total of 440 radiomic features were extracted from pre-treatment computed tomography (CT) images of 464 lung cancer patients. To ensure the unbiased evaluation of different machine-learning methods, publicly available implementations along with reported parameter configurations were used. Furthermore, we used two independent radiomic cohorts for training (n = 310 patients) and validation (n = 154 patients). We identified that Wilcoxon test based feature selection method WLCX (stability = 0.84 ± 0.05, AUC = 0.65 ± 0.02) and a classification method random forest RF (RSD = 3.52%, AUC = 0.66 ± 0.03) had highest prognostic performance with high stability against data perturbation. Our variability analysis indicated that the choice of classification method is the most dominant source of performance variation (34.21% of total variance). Identification of optimal machine-learning methods for radiomic applications is a crucial step towards stable and clinically relevant radiomic biomarkers, providing a non-invasive way of quantifying and monitoring tumor-phenotypic characteristics in clinical practice.\n",
            "\n",
            "Paper ID : 765d93759b7888fa1f7b2f3576809ad558c60caf \tArticle : Machine-learning-assisted materials discovery using failed experiments\n",
            "Author(s) : ['Paul Raccuglia', 'Katherine C. Elbert', 'Philip Adler', 'Casey Falk', 'Malia B. Wenny', 'Aurelio Mollo', 'M. Zeller', 'Sorelle A. Friedler', 'Joshua Schrier', 'A. Norquist']\n",
            "Year : 2016 \n",
            "Abstract : Inorganic–organic hybrid materials such as organically templated metal oxides, metal–organic frameworks (MOFs) and organohalide perovskites have been studied for decades, and hydrothermal and (non-aqueous) solvothermal syntheses have produced thousands of new materials that collectively contain nearly all the metals in the periodic table. Nevertheless, the formation of these compounds is not fully understood, and development of new compounds relies primarily on exploratory syntheses. Simulation- and data-driven approaches (promoted by efforts such as the Materials Genome Initiative) provide an alternative to experimental trial-and-error. Three major strategies are: simulation-based predictions of physical properties (for example, charge mobility, photovoltaic properties, gas adsorption capacity or lithium-ion intercalation) to identify promising target candidates for synthetic efforts; determination of the structure–property relationship from large bodies of experimental data, enabled by integration with high-throughput synthesis and measurement tools; and clustering on the basis of similar crystallographic structure (for example, zeolite structure classification or gas adsorption properties). Here we demonstrate an alternative approach that uses machine-learning algorithms trained on reaction data to predict reaction outcomes for the crystallization of templated vanadium selenites. We used information on ‘dark’ reactions—failed or unsuccessful hydrothermal syntheses—collected from archived laboratory notebooks from our laboratory, and added physicochemical property descriptions to the raw notebook information using cheminformatics techniques. We used the resulting data to train a machine-learning model to predict reaction success. When carrying out hydrothermal synthesis experiments using previously untested, commercially available organic building blocks, our machine-learning model outperformed traditional human strategies, and successfully predicted conditions for new organically templated inorganic product formation with a success rate of 89 per cent. Inverting the machine-learning model reveals new hypotheses regarding the conditions for successful product formation.\n",
            "\n",
            "Paper ID : 32e29041fa352a9df0889f42807ed6141bc0b5ff \tArticle : Machine learning in geosciences and remote sensing\n",
            "Author(s) : ['David John Lary', 'A. Alavi', 'A. Gandomi', 'A. Walker']\n",
            "Year : 2016 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : a09a9f69ff145508a12e6dbb81ccd7f5be4bd2fc \tArticle : Machine Learning for High-Throughput Stress Phenotyping in Plants.\n",
            "Author(s) : ['Arti Singh', 'B. Ganapathysubramanian', 'Asheesh K Singh', 'S. Sarkar']\n",
            "Year : 2016 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #30------------------------------------\n",
            "\n",
            "Paper ID : 3804ca5590a0829c5d56e84d860a2b2a456e3757 \tArticle : Principles of Explanatory Debugging to Personalize Interactive Machine Learning\n",
            "Author(s) : ['T. Kulesza', 'M. Burnett', 'Weng-Keen Wong', 'S. Stumpf']\n",
            "Year : 2015 \n",
            "Abstract : How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.\n",
            "\n",
            "Paper ID : dbde7dfa6cae81df8ac19ef500c42db96c3d1edd \tArticle : Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n",
            "Author(s) : ['Yonghui Wu', 'M. Schuster', 'Z. Chen', 'Quoc V. Le', 'Mohammad Norouzi', 'Wolfgang Macherey', 'M. Krikun', 'Yuan Cao', 'Qin Gao', 'Klaus Macherey', 'J. Klingner', 'Apurva Shah', 'Melvin Johnson', 'Xiaobing Liu', 'Lukasz Kaiser', 'Stephan Gouws', 'Y. Kato', 'Taku Kudo', 'H. Kazawa', 'K. Stevens', 'George Kurian', 'Nishant Patil', 'Wei Wang', 'C. Young', 'Jason R. Smith', 'Jason Riesa', 'Alex Rudnick', 'Oriol Vinyals', 'G. Corrado', 'Macduff Hughes', 'J. Dean']\n",
            "Year : 2016 \n",
            "Abstract : Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.\n",
            "\n",
            "Paper ID : bf5cf36407ece2569f0717f2b5593c4bd2140ebb \tArticle : Machine Learning for Predictive Maintenance: A Multiple Classifier Approach\n",
            "Author(s) : ['Gian Antonio Susto', 'A. Schirru', 'S. Pampuri', 'S. McLoone', 'A. Beghi']\n",
            "Year : 2015 \n",
            "Abstract : In this paper, a multiple classifier machine learning (ML) methodology for predictive maintenance (PdM) is presented. PdM is a prominent strategy for dealing with maintenance issues given the increasing need to minimize downtime and associated costs. One of the challenges with PdM is generating the so-called “health factors,” or quantitative indicators, of the status of a system associated with a given maintenance issue, and determining their relationship to operating costs and failure risk. The proposed PdM methodology allows dynamical decision rules to be adopted for maintenance management, and can be used with high-dimensional and censored data problems. This is achieved by training multiple classification modules with different prediction horizons to provide different performance tradeoffs in terms of frequency of unexpected breaks and unexploited lifetime, and then employing this information in an operating cost-based maintenance decision system to minimize expected costs. The effectiveness of the methodology is demonstrated using a simulated example and a benchmark semiconductor manufacturing maintenance problem.\n",
            "\n",
            "Paper ID : 07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b \tArticle : Advances and Open Problems in Federated Learning\n",
            "Author(s) : ['P. Kairouz', 'H. B. McMahan', 'Brendan Avent', 'A. Bellet', 'M. Bennis', 'A. Bhagoji', 'Keith Bonawitz', 'Zachary B. Charles', 'Graham Cormode', 'Rachel Cummings', \"Rafael G. L. D'Oliveira\", 'S. Rouayheb', 'David Evans', 'Josh Gardner', 'Zachary Garrett', 'Adrià Gascón', 'Badih Ghazi', 'Phillip B. Gibbons', 'M. Gruteser', 'Z. Harchaoui', 'Chaoyang He', 'Lie He', 'Zhouyuan Huo', 'Ben Hutchinson', 'Justin Hsu', 'Martin Jaggi', 'T. Javidi', 'Gauri Joshi', 'M. Khodak', 'Jakub Konecný', 'A. Korolova', 'F. Koushanfar', 'O. Koyejo', 'Tancrède Lepoint', 'Yang Liu', 'Prateek Mittal', 'M. Mohri', 'R. Nock', 'A. Özgür', 'R. Pagh', 'Mariana Raykova', 'Hang Qi', 'D. Ramage', 'R. Raskar', 'D. Song', 'Weikang Song', 'Sebastian U. Stich', 'Ziteng Sun', 'A. Suresh', 'Florian Tramèr', 'Praneeth Vepakomma', 'Jianyu Wang', 'Li Xiong', 'Zheng Xu', 'Qiang Yang', 'Felix X. Yu', 'Han Yu', 'Sen Zhao']\n",
            "Year : 2021 \n",
            "Abstract : Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.\n",
            "\n",
            "Paper ID : 451299066dd39de9d48010f4b4fa45e4ecfc5cee \tArticle : Machine Learning\n",
            "Author(s) : ['Ashrf Althbiti', 'Xiaogang Ma']\n",
            "Year : 2022 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6 \tArticle : Matching Networks for One Shot Learning\n",
            "Author(s) : ['Oriol Vinyals', 'C. Blundell', 'T. Lillicrap', 'K. Kavukcuoglu', 'Daan Wierstra']\n",
            "Year : 2016 \n",
            "Abstract : Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.\n",
            "\n",
            "Paper ID : f743833c22961537791171ef1d3fb42db8f357a3 \tArticle : Machine Learning Methods for Attack Detection in the Smart Grid\n",
            "Author(s) : ['M. Ozay', 'I. Esnaola', 'F. Yarman-Vural', 'S. Kulkarni', 'H. Poor']\n",
            "Year : 2016 \n",
            "Abstract : Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semisupervised) are employed with decision- and feature-level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than attack detection algorithms that employ state vector estimation methods in the proposed attack detection framework.\n",
            "\n",
            "Paper ID : a8797f1d253c75669d96e6fcceda2be3f8534e1d \tArticle : Support Vector Machine Active Learning with Applications to Text Classification\n",
            "Author(s) : ['Simon Tong', 'D. Koller']\n",
            "Year : 2001 \n",
            "Abstract : Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.\n",
            "\n",
            "Paper ID : f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6 \tArticle : Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\n",
            "Author(s) : ['Y. Gal', 'Zoubin Ghahramani']\n",
            "Year : 2016 \n",
            "Abstract : Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.\n",
            "\n",
            "Paper ID : 56e8863838b4dcc4790108cd1e7e680a104a7c30 \tArticle : Machine Learning Algorithms : A Review\n",
            "Author(s) : ['Ayon Dey']\n",
            "Year : 2016 \n",
            "Abstract : In this paper, various machine learning algorithms have been discussed. These algorithms are used for various purposes like data mining, image processing, predictive analytics, etc. to name a few. The main advantage of using machine learning is that, once an algorithm learns what to do with data, it can do its work automatically.\n",
            "------------------------------------Extracting Page #31------------------------------------\n",
            "\n",
            "Paper ID : 6aae0dc122102693e8136856ffc8b72df7f78386 \tArticle : A study of the behavior of several methods for balancing machine learning training data\n",
            "Author(s) : ['Gustavo E. A. P. A. Batista', 'R. Prati', 'M. C. Monard']\n",
            "Year : 2004 \n",
            "Abstract : There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.\n",
            "\n",
            "Paper ID : 73811a7f8b89de1b8bdad6bb938e58059a9076d3 \tArticle : Introduction to machine learning: k-nearest neighbors.\n",
            "Author(s) : ['Zhongheng Zhang']\n",
            "Year : 2016 \n",
            "Abstract : Machine learning techniques have been widely used in many scientific fields, but its use in medical literature is limited partly because of technical difficulties. k-nearest neighbors (kNN) is a simple method of machine learning. The article introduces some basic ideas underlying the kNN algorithm, and then focuses on how to perform kNN modeling with R. The dataset should be prepared before running the knn() function in R. After prediction of outcome with kNN algorithm, the diagnostic performance of the model should be checked. Average accuracy is the mostly widely used statistic to reflect the kNN algorithm. Factors such as k value, distance calculation and choice of appropriate predictors all have significant impact on the model performance.\n",
            "\n",
            "Paper ID : 53b55682222692323a3a0d546d9e1a3de29454f0 \tArticle : A review of supervised machine learning algorithms\n",
            "Author(s) : ['Amanpreet Singh', 'Narina Thakur', 'Aakanksha Sharma']\n",
            "Year : 2016 \n",
            "Abstract : Supervised machine learning is the construction of algorithms that are able to produce general patterns and hypotheses by using externally supplied instances to predict the fate of future instances. Supervised machine learning classification algorithms aim at categorizing data from prior information. Classification is carried out very frequently in data science problems. Various successful techniques have been proposed to solve such problems viz. Rule-based techniques, Logic-based techniques, Instance-based techniques, stochastic techniques. This paper discusses the efficacy of supervised machine learning algorithms in terms of the accuracy, speed of learning, complexity and risk of over fitting measures. The main objective of this paper is to provide a general comparison with state of art machine learning algorithms.\n",
            "\n",
            "Paper ID : 2ea6a93199c9227fa0c1c7de13725f918c9be3a4 \tArticle : Dlib-ml: A Machine Learning Toolkit\n",
            "Author(s) : ['Davis E. King']\n",
            "Year : 2009 \n",
            "Abstract : There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.\n",
            "\n",
            "Paper ID : e5278af5fd3ac8796992cb291e23fa8d47b5460f \tArticle : Kernel Methods for Machine Learning\n",
            "Author(s) : ['Michael Rabadi']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 227e0591634cef50d0bcfc73fe6c5b34a2256e5f \tArticle : Radio Machine Learning Dataset Generation with GNU Radio\n",
            "Author(s) : [\"Tim O'Shea\", 'Nathan E. West']\n",
            "Year : 2016 \n",
            "Abstract : This paper surveys emerging applications of Machine Learning (ML) to the Radio Signal Processing domain.  Provides some brief background on enabling methods and discusses some of the potential advancements for the field.  It discusses the critical importance of good datasets for model learning, testing, and evaluation and introduces several public open source synthetic datasets for various radio machine learning tasks.  These are intended to provide a robust common baselines for those working in the field and to provide a benchmark measure against which many techniques can be rapidly evaluated and compared.\n",
            "\n",
            "Paper ID : 9f86366feecbcfdf6c5be165fcf38c679164cc89 \tArticle : Machine Learning and the Profession of Medicine.\n",
            "Author(s) : ['Alison M Darcy', 'A. Louie', 'L. Roberts']\n",
            "Year : 2016 \n",
            "Abstract : This Viewpoint discusses the opportunities and ethical implications of using machine learning technologies, which can rapidly collect and learn from large amounts of personal data, to provide individalized patient care.\n",
            "\n",
            "Paper ID : a4cec122a08216fe8a3bc19b22e78fbaea096256 \tArticle : Deep Learning\n",
            "Author(s) : ['Ian J. Goodfellow', 'Yoshua Bengio', 'Aaron C. Courville']\n",
            "Year : 2015 \n",
            "Abstract : Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.\n",
            "\n",
            "Paper ID : ad0d1149875291d9b1177bc47e53e09237beeca0 \tArticle : Quantum-enhanced machine learning\n",
            "Author(s) : ['V. Dunjko', 'Jacob M. Taylor', 'H. Briegel']\n",
            "Year : 2016 \n",
            "Abstract : The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.\n",
            "\n",
            "Paper ID : 9bbf8ca712bca3e93c898aea56d021a0317a30c1 \tArticle : Machine-learning approaches in drug discovery: methods and applications.\n",
            "Author(s) : ['A. Lavecchia']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #32------------------------------------\n",
            "\n",
            "Paper ID : 0bf08d3dcc1f4a3b7b4f9a68f9c980c0a3f4ed2a \tArticle : A review of automatic selection methods for machine learning algorithms and hyper-parameter values\n",
            "Author(s) : ['Gang Luo']\n",
            "Year : 2016 \n",
            "Abstract : Machine learning studies automatic algorithms that improve themselves through experience. It is widely used for analyzing and extracting value from large biomedical data sets, or “big biomedical data,” advancing biomedical research, and improving healthcare. Before a machine learning model is trained, the user of a machine learning software tool typically must manually select a machine learning algorithm and set one or more model parameters termed hyper-parameters. The algorithm and hyper-parameter values used can greatly impact the resulting model’s performance, but their selection requires special expertise as well as many labor-intensive manual iterations. To make machine learning accessible to layman users with limited computing expertise, computer science researchers have proposed various automatic selection methods for algorithms and/or hyper-parameter values for a given supervised machine learning problem. This paper reviews these methods, identifies several of their limitations in the big biomedical data environment, and provides preliminary thoughts on how to address these limitations. These findings establish a foundation for future research on automatically selecting algorithms and hyper-parameter values for analyzing big biomedical data.\n",
            "\n",
            "Paper ID : f38513dc4350cfb987a8f0b774fc361c4d910a17 \tArticle : Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies\n",
            "Author(s) : ['John D. Kelleher', 'Brian Mac Namee', \"Aoife D'Arcy\"]\n",
            "Year : 2015 \n",
            "Abstract : Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context. After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution. The book, informed by the authors' many years of teaching machine learning, and working on predictive data analytics projects, is suitable for use by undergraduates in computer science, engineering, mathematics, or statistics; by graduate students in disciplines with applications for predictive data analytics; and as a reference for professionals.\n",
            "\n",
            "Paper ID : 825ca26af5a2a510dbc1a7b97587212bc98ae968 \tArticle : Power to the People: The Role of Humans in Interactive Machine Learning\n",
            "Author(s) : ['Saleema Amershi', 'M. Cakmak', 'W. B. Knox', 'T. Kulesza']\n",
            "Year : 2014 \n",
            "Abstract : Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.\n",
            "\n",
            "Paper ID : 24e6c5bfe9bb0751e5708b501d04e860011b2953 \tArticle : Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.\n",
            "Author(s) : ['Shujun Huang', 'Nianguang Cai', 'Pedro Penzuti Pacheco', 'Shavira Narrandes', 'Yang Wang', 'Wayne Xu']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.\n",
            "\n",
            "Paper ID : 8accfb94a0729e0670999641d5f3e691a48056bb \tArticle : Neuro-Fuzzy and Soft Computing-A Computational Approach to Learning and Machine Intelligence [Book Review]\n",
            "Author(s) : ['J. Jang', 'Chuen-Tsai Sun', 'Eiji Mizutani']\n",
            "Year : 1997 \n",
            "Abstract : Interestingly, neuro fuzzy and soft computing a computational approach to learning and machine intelligence that you really wait for now is coming. It's significant to wait for the representative and beneficial books to read. Every book that is provided in better way and utterance will be expected by many peoples. Even you are a good reader or not, feeling to read this book will always appear when you find it. But, when you feel hard to find it as yours, what to do? Borrow to your friends and don't know when to give back it to her or him.\n",
            "\n",
            "Paper ID : 19469939ee1c6cb51db757b71338c61fcfe8ee76 \tArticle : A survey of open source tools for machine learning with big data in the Hadoop ecosystem\n",
            "Author(s) : ['Sara Landset', 'T. Khoshgoftaar', 'Aaron N. Richter', 'Tawfiq Hasanin']\n",
            "Year : 2015 \n",
            "Abstract : With an ever-increasing amount of options, the task of selecting machine learning tools for big data can be difficult. The available tools have advantages and drawbacks, and many have overlapping uses. The world’s data is growing rapidly, and traditional tools for machine learning are becoming insufficient as we move towards distributed and real-time processing. This paper is intended to aid the researcher or professional who understands machine learning but is inexperienced with big data. In order to evaluate tools, one should have a thorough understanding of what to look for. To that end, this paper provides a list of criteria for making selections along with an analysis of the advantages and drawbacks of each. We do this by starting from the beginning, and looking at what exactly the term “big data” means. From there, we go on to the Hadoop ecosystem for a look at many of the projects that are part of a typical machine learning architecture and an understanding of how everything might fit together. We discuss the advantages and disadvantages of three different processing paradigms along with a comparison of engines that implement them, including MapReduce, Spark, Flink, Storm, and H2O. We then look at machine learning libraries and frameworks including Mahout, MLlib, SAMOA, and evaluate them based on criteria such as scalability, ease of use, and extensibility. There is no single toolkit that truly embodies a one-size-fits-all solution, so this paper aims to help make decisions smoother by providing as much information as possible and quantifying what the tradeoffs will be. Additionally, throughout this paper, we review recent research in the field using these tools and talk about possible future directions for toolkit-based learning.\n",
            "\n",
            "Paper ID : aaa76e15235d937d093a7063c0be86ed84494dee \tArticle : Machine Learning: A Bayesian and Optimization Perspective\n",
            "Author(s) : ['S. Theodoridis']\n",
            "Year : 2015 \n",
            "Abstract : This tutorial text gives a unifying perspective on machine learning by covering bothprobabilistic and deterministic approaches -which are based on optimization techniques together with the Bayesian inference approach, whose essence liesin the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts. The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling. Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied. MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.\n",
            "\n",
            "Paper ID : 596dac923657992a817d3d68ae83f2fba9cf1ab8 \tArticle : An Introduction to MCMC for Machine Learning\n",
            "Author(s) : ['C. Andrieu', 'N. D. Freitas', 'A. Doucet', 'Michael I. Jordan']\n",
            "Year : 2004 \n",
            "Abstract : This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.\n",
            "\n",
            "Paper ID : 7f8c7783a92d4c2f388902fffb3b378921f9e8ad \tArticle : Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications\n",
            "Author(s) : ['Mohammad Abu Alsheikh', 'Shaowei Lin', 'D. Niyato', 'H. Tan']\n",
            "Year : 2014 \n",
            "Abstract : Wireless sensor networks (WSNs) monitor dynamic environments that change rapidly over time. This dynamic behavior is either caused by external factors or initiated by the system designers themselves. To adapt to such conditions, sensor networks often adopt machine learning techniques to eliminate the need for unnecessary redesign. Machine learning also inspires many practical solutions that maximize resource utilization and prolong the lifespan of the network. In this paper, we present an extensive literature review over the period 2002-2013 of machine learning methods that were used to address common issues in WSNs. The advantages and disadvantages of each proposed algorithm are evaluated against the corresponding problem. We also provide a comparative guide to aid WSN designers in developing suitable machine learning solutions for their specific application challenges.\n",
            "\n",
            "Paper ID : 74d8094f967be96ac1d1212e1957b97ac0674d5d \tArticle : Machine Learning: The New AI\n",
            "Author(s) : ['Ethem Alpaydin']\n",
            "Year : 2016 \n",
            "Abstract : Today, machine learning underlies a range of applications we use every day, from product recommendations to voice recognition -- as well as some we don't yet use everyday, including driverless cars. It is the basis of the new approach in computing where we do not write programs but collect data; the idea is to learn the algorithms for the tasks automatically from data. As computing devices grow more ubiquitous, a larger part of our lives and work is recorded digitally, and as \"Big Data\" has gotten bigger, the theory of machine learning -- the foundation of efforts to process that data into knowledge -- has also advanced. In this book, machine learning expert Ethem Alpaydin offers a concise overview of the subject for the general reader, describing its evolution, explaining important learning algorithms, and presenting example applications. Alpaydin offers an account of how digital technology advanced from number-crunching mainframes to mobile devices, putting today's machine learning boom in context. He describes the basics of machine learning and some applications; the use of machine learning algorithms for pattern recognition; artificial neural networks inspired by the human brain; algorithms that learn associations between instances, with such applications as customer segmentation and learning recommendations; and reinforcement learning, when an autonomous agent learns act so as to maximize reward and minimize penalty. Alpaydin then considers some future directions for machine learning and the new field of \"data science,\" and discusses the ethical and legal implications for data privacy and security.\n",
            "------------------------------------Extracting Page #33------------------------------------\n",
            "\n",
            "Paper ID : a73f4e905834f72ecd7e65c88aebebaea1153fd0 \tArticle : Data mining - practical machine learning tools and techniques, Second Edition\n",
            "Author(s) : ['I. Witten', 'Eibe Frank']\n",
            "Year : 2005 \n",
            "Abstract : As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work.\n",
            "\n",
            "Paper ID : 68837728232463651283edbb7ef0c93b2f502b2b \tArticle : PuDianNao: A Polyvalent Machine Learning Accelerator\n",
            "Author(s) : ['Dao-Fu Liu', 'Tianshi Chen', 'Shaoli Liu', 'Jinhong Zhou', 'Shengyuan Zhou', 'O. Temam', 'Xiaobing Feng', 'Xuehai Zhou', 'Yunji Chen']\n",
            "Year : 2015 \n",
            "Abstract : Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique. In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.\n",
            "\n",
            "Paper ID : c162fb5f54dae3689908fe1b2615fa680172f9b5 \tArticle : Scikit-learn: Machine Learning Without Learning the Machinery\n",
            "Author(s) : ['G. Varoquaux', 'L. Buitinck', 'Gilles Louppe', 'O. Grisel', 'Fabian Pedregosa', 'Andreas Mueller']\n",
            "Year : 2015 \n",
            "Abstract : Machine learning is a pervasive development at the intersection of statistics and computer science. While it can benefit many data-related applications, the technical nature of the research literature and the corresponding algorithms slows down its adoption. Scikit-learn is an open-source software project that aims at making machine learning accessible to all, whether it be in academia or in industry. It benefits from the general-purpose Python language, which is both broadly adopted in the scientific world, and supported by a thriving ecosystem of contributors. Here we give a quick introduction to scikit-learn as well as to machine-learning basics.\n",
            "\n",
            "Paper ID : 85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175 \tArticle : An introduction to quantum machine learning\n",
            "Author(s) : ['M. Schuld', 'I. Sinayskiy', 'Francesco Petruccione']\n",
            "Year : 2014 \n",
            "Abstract : Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.\n",
            "\n",
            "Paper ID : fad1bd501aa769f7701c1016f8a4d1473ca77601 \tArticle : Machine Learning, Neural and Statistical Classification\n",
            "Author(s) : ['D. Michie', 'D. Spiegelhalter', 'C. C. Taylor']\n",
            "Year : 1994 \n",
            "Abstract : Survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning.\n",
            "\n",
            "Paper ID : 3449b65008b27f6e60a73d80c1fd990f0481126b \tArticle : Torch7: A Matlab-like Environment for Machine Learning\n",
            "Author(s) : ['Ronan Collobert', 'K. Kavukcuoglu', 'C. Farabet']\n",
            "Year : 2011 \n",
            "Abstract : Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua’s light interface.\n",
            "\n",
            "Paper ID : 44c04f1f6af260cd3d36cd88bb27cec550339b9e \tArticle : Machine learning bandgaps of double perovskites\n",
            "Author(s) : ['G. Pilania', 'A. Mannodi-Kanakkithodi', 'B. Uberuaga', 'R. Ramprasad', 'J. Gubernatis', 'T. Lookman']\n",
            "Year : 2016 \n",
            "Abstract : The ability to make rapid and accurate predictions on bandgaps of double perovskites is of much practical interest for a range of applications. While quantum mechanical computations for high-fidelity bandgaps are enormously computation-time intensive and thus impractical in high throughput studies, informatics-based statistical learning approaches can be a promising alternative. Here we demonstrate a systematic feature-engineering approach and a robust learning framework for efficient and accurate predictions of electronic bandgaps of double perovskites. After evaluating a set of more than 1.2 million features, we identify lowest occupied Kohn-Sham levels and elemental electronegativities of the constituent atomic species as the most crucial and relevant predictors. The developed models are validated and tested using the best practices of data science and further analyzed to rationalize their prediction performance.\n",
            "\n",
            "Paper ID : bdb5f7fc1608dd80e2d3fdb1575cec2d3c074ad6 \tArticle : A survey of feature selection and feature extraction techniques in machine learning\n",
            "Author(s) : ['S. Khalid', 'Tehmina Khalil', 'Shamila Nasreen']\n",
            "Year : 2014 \n",
            "Abstract : Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.\n",
            "\n",
            "Paper ID : 29b9ff8f4a26acc90e6182e1e749f15f688bc7cf \tArticle : Machine learning for quantum mechanics in a nutshell\n",
            "Author(s) : ['M. Rupp']\n",
            "Year : 2015 \n",
            "Abstract : Models that combine quantum mechanics (QM) with machine learning (ML) promise to deliver the accuracy of QM at the speed of ML. This hands-on tutorial introduces the reader to QM/ML models based on kernel learning, an elegant, systematically nonlinear form of ML. Pseudocode and a reference implementation are provided, enabling the reader to reproduce results from recent publications where atomization energies of small organic molecules are predicted using kernel ridge regression. © 2015 Wiley Periodicals, Inc.\n",
            "\n",
            "Paper ID : 04ca5de59edbdd49a9c0502c58331524d220bc8c \tArticle : Communication Efficient Distributed Machine Learning with the Parameter Server\n",
            "Author(s) : ['Mu Li', 'D. Andersen', 'Alex Smola', 'Kai Yu']\n",
            "Year : 2014 \n",
            "Abstract : This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from l1 -regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.\n",
            "------------------------------------Extracting Page #34------------------------------------\n",
            "\n",
            "Paper ID : bdb5f7fc1608dd80e2d3fdb1575cec2d3c074ad6 \tArticle : A survey of feature selection and feature extraction techniques in machine learning\n",
            "Author(s) : ['S. Khalid', 'Tehmina Khalil', 'Shamila Nasreen']\n",
            "Year : 2014 \n",
            "Abstract : Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.\n",
            "\n",
            "Paper ID : 29b9ff8f4a26acc90e6182e1e749f15f688bc7cf \tArticle : Machine learning for quantum mechanics in a nutshell\n",
            "Author(s) : ['M. Rupp']\n",
            "Year : 2015 \n",
            "Abstract : Models that combine quantum mechanics (QM) with machine learning (ML) promise to deliver the accuracy of QM at the speed of ML. This hands-on tutorial introduces the reader to QM/ML models based on kernel learning, an elegant, systematically nonlinear form of ML. Pseudocode and a reference implementation are provided, enabling the reader to reproduce results from recent publications where atomization energies of small organic molecules are predicted using kernel ridge regression. © 2015 Wiley Periodicals, Inc.\n",
            "\n",
            "Paper ID : 04ca5de59edbdd49a9c0502c58331524d220bc8c \tArticle : Communication Efficient Distributed Machine Learning with the Parameter Server\n",
            "Author(s) : ['Mu Li', 'D. Andersen', 'Alex Smola', 'Kai Yu']\n",
            "Year : 2014 \n",
            "Abstract : This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from l1 -regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.\n",
            "\n",
            "Paper ID : 0173ca962e4ab3d084c89568345e06f67d3d7efc \tArticle : Hyperparameter Search in Machine Learning\n",
            "Author(s) : ['Marc Claesen', 'B. Moor']\n",
            "Year : 2015 \n",
            "Abstract : We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.\n",
            "\n",
            "Paper ID : 48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016 \tArticle : Determinantal Point Processes for Machine Learning\n",
            "Author(s) : ['Alex Kulesza', 'B. Taskar']\n",
            "Year : 2012 \n",
            "Abstract : Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning.\n",
            "\n",
            "Paper ID : f83ca18f3834d45a70e9b54578e2c33870dde67d \tArticle : Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education\n",
            "Author(s) : ['Xiaojin Zhu']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            " \n",
            " I draw the reader's attention to machine teaching, the problem of finding an optimal training set given a machine learning algorithm and a target model. In addition to generating fascinating mathematical questions for computer scientists to ponder, machine teaching holds the promise of enhancing education and personnel training. The Socratic dialogue style aims to stimulate critical thinking.\n",
            " \n",
            "\n",
            "\n",
            "Paper ID : c61134ada9f0e3f3373d635c31a8b3caa37f9977 \tArticle : Genetic Algorithms and Machine Learning\n",
            "Author(s) : ['D. Goldberg', 'J. Holland']\n",
            "Year : 2005 \n",
            "Abstract : There is no a priori reason why machine learning must borrow from nature. A field could exist, complete with well-defined algorithms, data structures, and theories of learning, without once referring to organisms, cognitive or genetic structures, and psychological or evolutionary theories. Yet at the end of the day, with the position papers written, the computers plugged in, and the programs debugged, a learning edifice devoid of natural metaphor would lack something. It would ignore the fact that all these creations have become possible only after three billion years of evolution on this planet. It would miss the point that the very ideas of adaptation and learning are concepts invented by the most recent representatives of the species Homo sapiens from the careful observation of themselves and life around them. It would miss the point that natural examples of learning and adaptation are treasure troves of robust procedures and structures. Fortunately, the field of machine learning does rely upon nature's bounty for both inspiration and mechanism. Many machine learning systems now borrow heavily from current thinking in cognitive science, and rekindled interest in neural networks and connectionism is evidence of serious mechanistic and philosophical currents running through the field. Another area where natural example has been tapped is in work on genetic algorithms (GAs) and genetics-based machine learning. Rooted in the early cybernetics movement (Holland, 1962), progress has been made in both theory (Holland, 1975; Holland, Holyoak, Nisbett, & Thagard, 1986) and application (Goldberg, 1989; Grefenstette, 1985, 1987) to the point where genetics-based systems are finding their way into everyday commercial use (Davis & Coombs, 1987; Fourman, 1985).\n",
            "\n",
            "Paper ID : 3fdd37c3a30da1dd0e376889bf2bdddc637b0b34 \tArticle : A survey of multi-view machine learning\n",
            "Author(s) : ['Shiliang Sun']\n",
            "Year : 2013 \n",
            "Abstract : Multi-view learning or learning with multiple distinct feature sets is a rapidly growing direction in machine learning with well theoretical underpinnings and great practical success. This paper reviews theories developed to understand the properties and behaviors of multi-view learning and gives a taxonomy of approaches according to the machine learning mechanisms involved and the fashions in which multiple views are exploited. This survey aims to provide an insightful organization of current developments in the field of multi-view learning, identify their limitations, and give suggestions for further research. One feature of this survey is that we attempt to point out specific open problems which can hopefully be useful to promote the research of multi-view machine learning.\n",
            "\n",
            "Paper ID : bcb46edd9262251da16f7442a170471f2a81a6bd \tArticle : Machine Learning in Materials Science\n",
            "Author(s) : ['Tim Mueller', 'A. Kusne', 'R. Ramprasad']\n",
            "Year : 2016 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : ed5ab1cff7dd3a902eea4a811b15aa5ea3a36b30 \tArticle : MLaaS: Machine Learning as a Service\n",
            "Author(s) : ['Mauro Ribeiro', 'Katarina Grolinger', 'Miriam A. M. Capretz']\n",
            "Year : 2015 \n",
            "Abstract : The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.\n",
            "------------------------------------Extracting Page #35------------------------------------\n",
            "\n",
            "Paper ID : ed5ab1cff7dd3a902eea4a811b15aa5ea3a36b30 \tArticle : MLaaS: Machine Learning as a Service\n",
            "Author(s) : ['Mauro Ribeiro', 'Katarina Grolinger', 'Miriam A. M. Capretz']\n",
            "Year : 2015 \n",
            "Abstract : The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.\n",
            "\n",
            "Paper ID : 61ce67533d2dd6605c907146658ccdbc4778a5d8 \tArticle : Learning a Multi-View Stereo Machine\n",
            "Author(s) : ['Abhishek Kar', 'Christian Häne', 'Jitendra Malik']\n",
            "Year : 2017 \n",
            "Abstract : We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.\n",
            "\n",
            "Paper ID : df2a7756382540e92895f10703cec32d50c4f316 \tArticle : Fast and accurate modeling of molecular atomization energies with machine learning.\n",
            "Author(s) : ['M. Rupp', 'A. Tkatchenko', 'K. Müller', 'O. A. von Lilienfeld']\n",
            "Year : 2012 \n",
            "Abstract : We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10  kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.\n",
            "\n",
            "Paper ID : b430afae080f51c925da240aef5c2ec65c9ab2ae \tArticle : Entanglement-based machine learning on a quantum computer.\n",
            "Author(s) : ['X. Cai', 'D. Wu', 'Z. Su', 'M.-C. Chen', 'X.-L. Wang', 'Li Li', 'N-L Liu', 'C.-Y. Lu', 'J.-W. Pan']\n",
            "Year : 2015 \n",
            "Abstract : Machine learning, a branch of artificial intelligence, learns from previous experience to optimize performance, which is ubiquitous in various fields such as computer sciences, financial analysis, robotics, and bioinformatics. A challenge is that machine learning with the rapidly growing \"big data\" could become intractable for classical computers. Recently, quantum machine learning algorithms [Lloyd, Mohseni, and Rebentrost, arXiv.1307.0411] were proposed which could offer an exponential speedup over classical algorithms. Here, we report the first experimental entanglement-based classification of two-, four-, and eight-dimensional vectors to different clusters using a small-scale photonic quantum computer, which are then used to implement supervised and unsupervised machine learning. The results demonstrate the working principle of using quantum computers to manipulate and classify high-dimensional vectors, the core mathematical routine in machine learning. The method can, in principle, be scaled to larger numbers of qubits, and may provide a new route to accelerate machine learning.\n",
            "\n",
            "Paper ID : 5888c776e38f39efb9b96d0ba2713981008a86b1 \tArticle : Structural Health Monitoring: A Machine Learning Perspective\n",
            "Author(s) : ['C. Farrar', 'K. Worden']\n",
            "Year : 2012 \n",
            "Abstract : This book focuses on structural health monitoring in the context of machine learning. The authors review the technical literature and include case studies. Chapters include: operational evaluation, sensing and data acquisition, introduction to probability and statistics, machine learning and statistical pattern recognition, and data prognosis.\n",
            "\n",
            "Paper ID : 0664d5fa1b811b38066ab2ad91670c7e5d7c2673 \tArticle : Machine Learning\n",
            "Author(s) : ['Pedro M. Domingos']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning's focus on ill-defined problems and highly flexible methods makes it ideally suited for knowledge discovery in databases (KDD) applications. Among the ideas machine learning contributes to KDD are the importance of empirical validation, the impossibility of learning without a priori assumptions, and the utility of limited-search or limited-representation methods. Machine learning provides methods for incorporating knowledge into the learning process, changing and combining representations, combatting the curse of dimensionality, and learning comprehensible models. KDD challenges for machine learning include scaling up its algorithms to large databases, using cost information in learning, automating data preprocessing, and enabling rapid development of applications. KDD opens up new directions for machine-learning research and brings new urgency to others. These directions include interfacing with the human user and the database system, learning from nonattribute-vector data, learning partial models, and learning continuously from an open-ended stream of data.\n",
            "\n",
            "Paper ID : 9b0aa51901f05278928bdfcb4e9826a429a81293 \tArticle : Quantum algorithms for supervised and unsupervised machine learning\n",
            "Author(s) : ['S. Lloyd', 'M. Mohseni', 'P. Rebentrost']\n",
            "Year : 2013 \n",
            "Abstract : Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.\n",
            "\n",
            "Paper ID : 5df0a0e9ceec70a9321b0555288222bf53216342 \tArticle : Neural Networks in Machine Learning\n",
            "Author(s) : ['Parul Parashar']\n",
            "Year : 2014 \n",
            "Abstract : Machine Learning is associated with the study and construction of systems that can learn on their own rather than following instructions. It is used in search engines, optical character recognition, computer vision etc. Neural networks are one of the several techniques used in machine learning. Here we are trying to discuss neural network approaches used in machine learning.\n",
            "\n",
            "Paper ID : 6eabf6e67c29778265bc9fef3b58b2756c739c83 \tArticle : Machine Learning for Aerial Image Labeling\n",
            "Author(s) : ['Geoffrey E. Hinton', 'Volodymyr Mnih']\n",
            "Year : 2013 \n",
            "Abstract : Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. \n",
            "We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with semantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incomplete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. \n",
            "We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.\n",
            "\n",
            "Paper ID : 2878d9936f494ed7d0c8aec47e9bcc5e51609f9a \tArticle : Extreme Learning Machine for Multilayer Perceptron\n",
            "Author(s) : ['Jiexiong Tang', 'Chenwei Deng', 'G. Huang']\n",
            "Year : 2016 \n",
            "Abstract : Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.\n",
            "------------------------------------Extracting Page #36------------------------------------\n",
            "\n",
            "Paper ID : 4efe53a9653d1481e50382a2c95bce6eb4f6de9d \tArticle : Kernel Methods and Machine Learning\n",
            "Author(s) : ['S. Kung']\n",
            "Year : 2014 \n",
            "Abstract : Part I. Machine Learning and Kernel Vector Spaces: 1. Fundamentals of machine learning 2. Kernel-induced vector spaces Part II. Dimension-Reduction: Feature Selection and PCA/KPCA: 3. Feature selection 4. PCA and Kernel-PCA Part III. Unsupervised Learning Models for Cluster Analysis: 5. Unsupervised learning for cluster discovery 6. Kernel methods for cluster discovery Part IV. Kernel Ridge Regressors and Variants: 7. Kernel-based regression and regularization analysis 8. Linear regression and discriminant analysis for supervised classification 9. Kernel ridge regression for supervised classification Part V. Support Vector Machines and Variants: 10. Support vector machines 11. Support vector learning models for outlier detection 12. Ridge-SVM learning models Part VI. Kernel Methods for Green Machine Learning Technologies: 13. Efficient kernel methods for learning and classifcation Part VII. Kernel Methods and Statistical Estimation Theory: 14. Statistical regression analysis and errors-in-variables models 15: Kernel methods for estimation, prediction, and system identification Part VIII. Appendices: Appendix A. Validation and test of learning models Appendix B. kNN, PNN, and Bayes classifiers References Index.\n",
            "\n",
            "Paper ID : 51891710e30da33c4ced4ae7daee1593e0cb5cc4 \tArticle : Machine Learning: The High Interest Credit Card of Technical Debt\n",
            "Author(s) : ['D. Sculley', 'Gary Holt', 'D. Golovin', 'Eugene Davydov', 'Todd Phillips', 'D. Ebner', 'Vinay Chaudhary', 'Michael Young']\n",
            "Year : 2014 \n",
            "Abstract : Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening APIs, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of “glue code” or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.\n",
            "\n",
            "Paper ID : 1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6 \tArticle : Transfer Learning for Low-Resource Neural Machine Translation\n",
            "Author(s) : ['Barret Zoph', 'Deniz Yuret', 'Jonathan May', 'Kevin Knight']\n",
            "Year : 2016 \n",
            "Abstract : The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.\n",
            "\n",
            "Paper ID : b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e \tArticle : Machine learning - an artificial intelligence approach\n",
            "Author(s) : ['R. Michalski', 'John R. Anderson']\n",
            "Year : 1984 \n",
            "Abstract : This book contains tutorial overviews and research papers on contemporary trends in the area of machine learning viewed from an AI perspective. Research directions covered include: learning from examples, modeling human learning strategies, knowledge acquisition for expert systems, learning heuristics, discovery systems, and conceptual data analysis.\n",
            "\n",
            "Paper ID : 3ced109e0c16014cf4cdd677aeb1b95acf1c35d6 \tArticle : How to represent crystal structures for machine learning: Towards fast prediction of electronic properties\n",
            "Author(s) : ['K. T. Schutt', 'H. Glawe', 'F. Brockherde', 'A. Sanna', 'K. Muller', 'E. Gross']\n",
            "Year : 2014 \n",
            "Abstract : High-throughput density functional calculations of solids are highly time-consuming. As an alternative, we propose a machine learning approach for the fast prediction of solid-state properties. To achieve this, local spin-density approximation calculations are used as a training set. We focus on predicting the value of the density of electronic states at the Fermi energy. We find that conventional representations of the input data, such as the Coulomb matrix, are not suitable for the training of learning machines in the case of periodic solids. We propose a novel crystal structure representation for which learning and competitive prediction accuracies become possible within an unrestricted class of spd systems of arbitrary unit-cell size.\n",
            "\n",
            "Paper ID : e8dccfb88a6524a67b6239f6b38a8fdaf15f6b39 \tArticle : Quantum Machine Learning: What Quantum Computing Means to Data Mining\n",
            "Author(s) : ['P. Wittek']\n",
            "Year : 2014 \n",
            "Abstract : Quantum Machine Learning bridges the gap between abstract developments in quantum computing and the applied research on machine learning. Paring down the complexity of the disciplines involved, it ...\n",
            "\n",
            "Paper ID : 06a81f63fc4ccfcf02934647a7c17454b91853b0 \tArticle : Machine Learning - The Art and Science of Algorithms that Make Sense of Data\n",
            "Author(s) : ['P. Flach']\n",
            "Year : 2012 \n",
            "Abstract : As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.\n",
            "\n",
            "Paper ID : 23b559b5ab27f2fca6f56c0a7b6478bcf69db509 \tArticle : Dual Learning for Machine Translation\n",
            "Author(s) : ['Di He', 'Yingce Xia', 'Tao Qin', 'Liwei Wang', 'Nenghai Yu', 'Tie-Yan Liu', 'Wei-Ying Ma']\n",
            "Year : 2016 \n",
            "Abstract : While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English ↔ French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.\n",
            "\n",
            "Paper ID : 8f566001453bc6be0a935bf69ffd90d9db3af32b \tArticle : Toward Causal Representation Learning\n",
            "Author(s) : ['B. Scholkopf', 'Francesco Locatello', 'Stefan Bauer', 'Nan Rosemary Ke', 'Nal Kalchbrenner', 'Anirudh Goyal', 'Yoshua Bengio']\n",
            "Year : 2021 \n",
            "Abstract : The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.\n",
            "\n",
            "Paper ID : 3bb6d5834bfb355553588e382ac5f9fa8a8d831d \tArticle : Distributed GraphLab: A Framework for Machine Learning in the Cloud\n",
            "Author(s) : ['Y. Low', 'Joseph E. Gonzalez', 'Aapo Kyrola', 'D. Bickson', 'Carlos Guestrin', 'J. Hellerstein']\n",
            "Year : 2012 \n",
            "Abstract : While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. \n",
            " \n",
            "We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.\n",
            "------------------------------------Extracting Page #37------------------------------------\n",
            "\n",
            "Paper ID : 92a314bf9ae817836389cc97af5a9f42c561a772 \tArticle : Accelerating materials property predictions using machine learning\n",
            "Author(s) : ['G. Pilania', 'Chenchen Wang', 'Xun Jiang', 'S. Rajasekaran', 'R. Ramprasad']\n",
            "Year : 2013 \n",
            "Abstract : The materials discovery process can be significantly expedited and simplified if we can learn effectively from available knowledge and data. In the present contribution, we show that efficient and accurate prediction of a diverse set of properties of material systems is possible by employing machine (or statistical) learning methods trained on quantum mechanical computations in combination with the notions of chemical similarity. Using a family of one-dimensional chain systems, we present a general formalism that allows us to discover decision rules that establish a mapping between easily accessible attributes of a system and its properties. It is shown that fingerprints based on either chemo-structural (compositional and configurational information) or the electronic charge density distribution can be used to make ultra-fast, yet accurate, property predictions. Harnessing such learning paradigms extends recent efforts to systematically explore and mine vast chemical spaces, and can significantly accelerate the discovery of new application-specific materials.\n",
            "\n",
            "Paper ID : 6aa8f8ae07a57a2025d6adc27a0bc53f6a7ee385 \tArticle : Machine learning for targeted display advertising: transfer learning in action\n",
            "Author(s) : ['Claudia Perlich', 'Brian Dalessandro', 'Troy Raeder', 'Ori Stitelman', 'F. Provost']\n",
            "Year : 2013 \n",
            "Abstract : This paper presents the design of a fully deployed multistage transfer learning system for targeted display advertising, highlighting the important role of problem formulation and the sampling of data from distributions different from that of the target environment. Notably, the machine learning system itself is deployed and has been in continual use for years for thousands of advertising campaigns—in contrast to the more common case where predictive models are built outside the system, curated, and then deployed. In this domain, acquiring sufficient data for training from the ideal sampling distribution is prohibitively expensive. Instead, data are drawn from surrogate distributions and learning tasks, and then transferred to the target task. We present the design of the transfer learning system We then present a detailed experimental evaluation, showing that the different transfer stages indeed each add value. We also present production results across a variety of advertising clients from a variety of industries, illustrating the performance of the system in use. We close the paper with a collection of lessons learned from over half a decade of research and development on this complex, deployed, and intensely used machine learning system.\n",
            "\n",
            "Paper ID : 184ac0766262312ba76bbdece4e7ffad0aa8180b \tArticle : Representation Learning: A Review and New Perspectives\n",
            "Author(s) : ['Yoshua Bengio', 'Aaron C. Courville', 'Pascal Vincent']\n",
            "Year : 2013 \n",
            "Abstract : The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.\n",
            "\n",
            "Paper ID : 01e1fa7924b3eb76b73f1828c93805f3ba028bae \tArticle : MLbase: A Distributed Machine-learning System\n",
            "Author(s) : ['Tim Kraska', 'Ameet S. Talwalkar', 'John C. Duchi', 'R. Griffith', 'M. Franklin', 'Michael I. Jordan']\n",
            "Year : 2013 \n",
            "Abstract : Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelming|many users do not understand the trade-os and challenges of parameterizing and choosing between dierent learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.\n",
            "\n",
            "Paper ID : 0d39cc42f2186dac99898121bbc8bb2b965ba93d \tArticle : Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies.\n",
            "Author(s) : ['K. Hansen', 'G. Montavon', 'Franziska Biegler', 'S. Fazli', 'M. Rupp', 'M. Scheffler', 'O. A. von Lilienfeld', 'A. Tkatchenko', 'K. Müller']\n",
            "Year : 2013 \n",
            "Abstract : The accurate and reliable prediction of properties of molecules typically requires computationally intensive quantum-chemical calculations. Recently, machine learning techniques applied to ab initio calculations have been proposed as an efficient approach for describing the energies of molecules in their given ground-state structure throughout chemical compound space (Rupp et al. Phys. Rev. Lett. 2012, 108, 058301). In this paper we outline a number of established machine learning techniques and investigate the influence of the molecular representation on the methods performance. The best methods achieve prediction errors of 3 kcal/mol for the atomization energies of a wide variety of molecules. Rationales for this performance improvement are given together with pitfalls and challenges when applying machine learning approaches to the prediction of quantum-mechanical observables.\n",
            "\n",
            "Paper ID : 836acf6fc99ebf81d219e2b67f7ab25efc29a6a4 \tArticle : Pylearn2: a machine learning research library\n",
            "Author(s) : ['Ian J. Goodfellow', 'David Warde-Farley', 'Pascal Lamblin', 'Vincent Dumoulin', 'Mehdi Mirza', 'Razvan Pascanu', 'J. Bergstra', 'Frédéric Bastien', 'Yoshua Bengio']\n",
            "Year : 2013 \n",
            "Abstract : Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.\n",
            "\n",
            "Paper ID : 5776d0fea69d826519ee3649f620e8755a490efe \tArticle : Lifelong Machine Learning Systems: Beyond Learning Algorithms\n",
            "Author(s) : ['D. Silver', 'Qiang Yang', 'Lianghao Li']\n",
            "Year : 2013 \n",
            "Abstract : Lifelong Machine Learning, or LML, considers systems that can learn many tasks from one or more domains over its lifetime. The goal is to sequentially retain learned knowledge and to selectively transfer that knowledge when learning a new task so as to develop more accurate hypotheses or policies. Following a review of prior work on LML, we propose that it is now appropriate for the AI community to move beyond learning algorithms to more seriously consider the nature of systems that are capable of learning over a lifetime. Reasons for our position are presented and potential counter-arguments are discussed. The remainder of the paper contributes by defining LML, presenting a reference framework that considers all forms of machine learning, and listing several key challenges for and benefits from LML research. We conclude with ideas for next steps to advance the field.\n",
            "\n",
            "Paper ID : d8ed6678758f9200bd23fcf11dd733c8f4d9d71c \tArticle : IEEE Transactions on Pattern Analysis and Machine Intelligence Information for Authors\n",
            "Author(s) : ['Ieee Xplore']\n",
            "Year : 2022 \n",
            "Abstract : In the real world, a realistic setting for computer vision or multimedia recognition problems is that we have some classes containing lots of training data and many classes contain a small amount of training data. Therefore, how to use frequent classes to help learning rare classes for which it is harder to collect the training data is an open question. Learning with Shared Information is an emerging topic in machine learning, computer vision and multimedia analysis. There are different level of components that can be shared during concept modeling and machine learning stages, such as sharing generic object parts, sharing attributes, sharing transformations, sharing regularization parameters and sharing training examples, etc. Regarding the specific methods, multi-task learning, transfer learning and deep learning can be seen as using different strategies to share information. These learning with shared information methods are very effective in solving real-world large-scale problems. This special issue aims at gathering the recent advances in learning with shared information methods and their applications in computer vision and multimedia analysis. Both state-of-the-art works, as well as literature reviews, are welcome for submission. Papers addressing interesting real-world computer vision and multimedia applications are especially encouraged. Topics of interest include, but are not limited to:  • Multi-task learning or transfer learning for large-scale computer vision and multimedia analysis • Deep learning for large-scale computer vision and multimedia analysis • Multi-modal approach for large-scale computer vision and multimedia analysis • Different sharing strategies, e.g., sharing generic object parts, sharing attributes, sharing transformations, sharing regularization parameters and sharing training examples, • Real-world computer vision and multimedia applications based on learning with shared information, e.g., event detection, object recognition, object detection, action recognition, human head pose estimation, object tracking, location-based services, semantic indexing. • New datasets and metrics to evaluate the benefit of the proposed sharing ability for the specific computer vision or multimedia problem. • Survey papers regarding the topic of learning with shared information.  Authors who are unsure whether their planned submission is in scope may contact the guest editors prior to the submission deadline with an abstract, in order to receive feedback.\n",
            "\n",
            "Paper ID : bc745811e231d1b4e37d2c56cbd2d67e37ba9032 \tArticle : Machine Learning Paradigms for Speech Recognition: An Overview\n",
            "Author(s) : ['L. Deng', 'Xiao Li']\n",
            "Year : 2013 \n",
            "Abstract : Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem - for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.\n",
            "\n",
            "Paper ID : d82f27f4a8dcee6cbab41ff954cc6c2b7709a693 \tArticle : Mastering Machine Learning With scikit-learn\n",
            "Author(s) : ['Gavin Hackeling']\n",
            "Year : 2014 \n",
            "Abstract : Apply effective learning algorithms to real-world problems using scikit-learn About This BookDesign and troubleshoot machine learning systems for common tasks including regression, classification, and clusteringAcquaint yourself with popular machine learning algorithms, including decision trees, logistic regression, and support vector machinesA practical example-based guide to help you gain expertise in implementing and evaluating machine learning systems using scikit-learnWho This Book Is ForIf you are a software developer who wants to learn how machine learning models work and how to apply them effectively, this book is for you. Familiarity with machine learning fundamentals and Python will be helpful, but is not essential. In Detail This book examines machine learning models including logistic regression, decision trees, and support vector machines, and applies them to common problems such as categorizing documents and classifying images. It begins with the fundamentals of machine learning, introducing you to the supervised-unsupervised spectrum, the uses of training and test data, and evaluating models. You will learn how to use generalized linear models in regression problems, as well as solve problems with text and categorical features.You will be acquainted with the use of logistic regression, regularization, and the various loss functions that are used by generalized linear models. The book will also walk you through an example project that prompts you to label the most uncertain training examples. You will also use an unsupervised Hidden Markov Model to predict stock prices.By the end of the book, you will be an expert in scikit-learn and will be well versed in machine learning\n",
            "------------------------------------Extracting Page #38------------------------------------\n",
            "\n",
            "Paper ID : 64ad89855f829d0587e0f77d1be52030616c67cb \tArticle : Sentiment analysis in twitter using machine learning techniques\n",
            "Author(s) : ['M. Neethu', 'R. Rajasree']\n",
            "Year : 2013 \n",
            "Abstract : Sentiment analysis deals with identifying and classifying opinions or sentiments expressed in source text. Social media is generating a vast amount of sentiment rich data in the form of tweets, status updates, blog posts etc. Sentiment analysis of this user generated data is very useful in knowing the opinion of the crowd. Twitter sentiment analysis is difficult compared to general sentiment analysis due to the presence of slang words and misspellings. The maximum limit of characters that are allowed in Twitter is 140. Knowledge base approach and Machine learning approach are the two strategies used for analyzing sentiments from the text. In this paper, we try to analyze the twitter posts about electronic products like mobiles, laptops etc using Machine Learning approach. By doing sentiment analysis in a specific domain, it is possible to identify the effect of domain information in sentiment classification. We present a new feature vector for classifying the tweets as positive, negative and extract peoples' opinion about products.\n",
            "\n",
            "Paper ID : cb7853c5d609081ea12cd4db3863a87da2d51808 \tArticle : From machine learning to machine reasoning\n",
            "Author(s) : ['L. Bottou']\n",
            "Year : 2013 \n",
            "Abstract : A plausible definition of “reasoning” could be “algebraically manipulating previously acquired knowledge in order to answer a new question”. This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated “all-purpose” inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.\n",
            "\n",
            "Paper ID : b90d44f59fcb74c71d3e31f67a3f09efab187a4e \tArticle : Machine learning in cell biology – teaching computers to recognize phenotypes\n",
            "Author(s) : ['Christoph Sommer', 'D. Gerlich']\n",
            "Year : 2013 \n",
            "Abstract : Summary Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.\n",
            "\n",
            "Paper ID : a15067563a18378dac71a206c6cc2e2d8c871301 \tArticle : Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning\n",
            "Author(s) : ['M. Hall']\n",
            "Year : 2000 \n",
            "Abstract : Algorithms for feature selection fall into two broad categories: wrappers that use the learning algorithm itself to evaluate the usefulness of features and filters that evaluate features according to heuristics based on general characteristics of the data. For application to large databases, filters have proven to be more practical than wrappers because they are much faster. However, most existing filter algorithms only work with discrete classification problems. This paper describes a fast, correlation-based filter algorithm that can be applied to continuous and discrete problems. The algorithm often outperforms the well-known ReliefF attribute estimator when used as a preprocessing step for naive Bayes, instance-based learning, decision trees, locally weighted regression, and model trees. It performs more feature selection than ReliefF does—reducing the data dimensionality by fifty percent in most cases. Also, decision and model trees built from the preprocessed data are often significantly smaller.\n",
            "\n",
            "Paper ID : 49bdeb07b045dd77f0bfe2b44436608770235a23 \tArticle : Federated Learning: Challenges, Methods, and Future Directions\n",
            "Author(s) : ['Tian Li', 'Anit Kumar Sahu', 'Ameet S. Talwalkar', 'Virginia Smith']\n",
            "Year : 2020 \n",
            "Abstract : Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.\n",
            "\n",
            "Paper ID : 941000ea62b7a7048b912c73a0adaeedc683b78b \tArticle : Introduction to machine learning.\n",
            "Author(s) : ['Y. Bastanlar', 'Mustafa Ozuysal']\n",
            "Year : 2014 \n",
            "Abstract : The machine learning field, which can be briefly defined as enabling computers make successful predictions using past experiences, has exhibited an impressive development recently with the help of the rapid increase in the storage capacity and processing power of computers. Together with many other disciplines, machine learning methods have been widely employed in bioinformatics. The difficulties and cost of biological analyses have led to the development of sophisticated machine learning approaches for this application area. In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning and types of classification. Then, we point out the main issues of designing machine learning experiments and their performance evaluation. Finally, we introduce some supervised learning methods.\n",
            "\n",
            "Paper ID : 73b51b02a061e2eae2eebe2ceae45872ea7d509d \tArticle : A Survey on Machine-Learning Techniques in Cognitive Radios\n",
            "Author(s) : ['Mario Bkassiny', 'Yang Li', 'S. Jayaweera']\n",
            "Year : 2013 \n",
            "Abstract : In this survey paper, we characterize the learning problem in cognitive radios (CRs) and state the importance of artificial intelligence in achieving real cognitive communications systems. We review various learning problems that have been studied in the context of CRs classifying them under two main categories: Decision-making and feature classification. Decision-making is responsible for determining policies and decision rules for CRs while feature classification permits identifying and classifying different observation models. The learning algorithms encountered are categorized as either supervised or unsupervised algorithms. We describe in detail several challenging learning issues that arise in cognitive radio networks (CRNs), in particular in non-Markovian environments and decentralized networks, and present possible solution methods to address them. We discuss similarities and differences among the presented algorithms and identify the conditions under which each of the techniques may be applied.\n",
            "\n",
            "Paper ID : d480f90ec1d48309850c1b92b2053990a60522a8 \tArticle : MLI: An API for Distributed Machine Learning\n",
            "Author(s) : ['Evan R. Sparks', 'Ameet S. Talwalkar', 'Virginia Smith', 'Jey Kottalam', 'Xinghao Pan', 'Joseph E. Gonzalez', 'M. Franklin', 'Michael I. Jordan', 'Tim Kraska']\n",
            "Year : 2013 \n",
            "Abstract : MLI is an Application Programming Interface designed to address the challenges of building Machine Learning algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.\n",
            "\n",
            "Paper ID : a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3 \tArticle : Learning scikit-learn: Machine Learning in Python\n",
            "Author(s) : ['Ral Garreta', 'Guillermo Moncecchi']\n",
            "Year : 2013 \n",
            "Abstract : Experience the benefits of machine learning techniques by applying them to real-world problems using Python and the open source scikit-learn library Overview Use Python and scikit-learn to create intelligent applications Apply regression techniques to predict future behaviour and learn to cluster items in groups by their similarities Make use of classification techniques to perform image recognition and document classification In Detail Machine learning, the art of creating applications that learn from experience and data, has been around for many years. However, in the era of big data, huge amounts of information is being generated. This makes machine learning an unavoidable source of new data-based approximations for problem solving. With Learning scikit-learn: Machine Learning in Python, you will learn to incorporate machine learning in your applications. The book combines an introduction to some of the main concepts and methods in machine learning with practical, hands-on examples of real-world problems. Ranging from handwritten digit recognition to document classification, examples are solved step by step using Scikit-learn and Python. The book starts with a brief introduction to the core concepts of machine learning with a simple example. Then, using real-world applications and advanced features, it takes a deep dive into the various machine learning techniques. You will learn to evaluate your results and apply advanced techniques for preprocessing data. You will also be able to select the best set of features and the best methods for each problem. With Learning scikit-learn: Machine Learning in Python you will learn how to use the Python programming language and the scikit-learn library to build applications that learn from experience, applying the main concepts and techniques of machine learning. What you will learn from this book Set up scikit-learn inside your Python environment Classify objects (from documents to human faces and flower species) based on some of their features, using a variety of methods from Support Vector Machines to Nave Bayes Use Decision Trees to explain the main causes of certain phenomenon such as the Titanic passengers survival Predict house prices using regression techniques Display and analyse groups in your data using dimensionality reduction Make use of different tools to preprocess, extract, and select the learning features Select the best parameters for your models using model selection Improve the way you build your models using parallelization techniques Approach The book adopts a tutorial-based approach to introduce the user to Scikit-learn. Who this book is written for If you are a programmer who wants to explore machine learning and data-based methods to build intelligent applications and enhance your programming skills, this the book for you. No previous experience with machine-learning algorithms is required.\n",
            "\n",
            "Paper ID : 5fb8ba2e3967e079c57aa703bf216c168ea8104f \tArticle : Model-based machine learning\n",
            "Author(s) : ['Charles M. Bishop']\n",
            "Year : 2013 \n",
            "Abstract : Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.\n",
            "------------------------------------Extracting Page #39------------------------------------\n",
            "\n",
            "Paper ID : 73b51b02a061e2eae2eebe2ceae45872ea7d509d \tArticle : A Survey on Machine-Learning Techniques in Cognitive Radios\n",
            "Author(s) : ['Mario Bkassiny', 'Yang Li', 'S. Jayaweera']\n",
            "Year : 2013 \n",
            "Abstract : In this survey paper, we characterize the learning problem in cognitive radios (CRs) and state the importance of artificial intelligence in achieving real cognitive communications systems. We review various learning problems that have been studied in the context of CRs classifying them under two main categories: Decision-making and feature classification. Decision-making is responsible for determining policies and decision rules for CRs while feature classification permits identifying and classifying different observation models. The learning algorithms encountered are categorized as either supervised or unsupervised algorithms. We describe in detail several challenging learning issues that arise in cognitive radio networks (CRNs), in particular in non-Markovian environments and decentralized networks, and present possible solution methods to address them. We discuss similarities and differences among the presented algorithms and identify the conditions under which each of the techniques may be applied.\n",
            "\n",
            "Paper ID : d480f90ec1d48309850c1b92b2053990a60522a8 \tArticle : MLI: An API for Distributed Machine Learning\n",
            "Author(s) : ['Evan R. Sparks', 'Ameet S. Talwalkar', 'Virginia Smith', 'Jey Kottalam', 'Xinghao Pan', 'Joseph E. Gonzalez', 'M. Franklin', 'Michael I. Jordan', 'Tim Kraska']\n",
            "Year : 2013 \n",
            "Abstract : MLI is an Application Programming Interface designed to address the challenges of building Machine Learning algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.\n",
            "\n",
            "Paper ID : a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3 \tArticle : Learning scikit-learn: Machine Learning in Python\n",
            "Author(s) : ['Ral Garreta', 'Guillermo Moncecchi']\n",
            "Year : 2013 \n",
            "Abstract : Experience the benefits of machine learning techniques by applying them to real-world problems using Python and the open source scikit-learn library Overview Use Python and scikit-learn to create intelligent applications Apply regression techniques to predict future behaviour and learn to cluster items in groups by their similarities Make use of classification techniques to perform image recognition and document classification In Detail Machine learning, the art of creating applications that learn from experience and data, has been around for many years. However, in the era of big data, huge amounts of information is being generated. This makes machine learning an unavoidable source of new data-based approximations for problem solving. With Learning scikit-learn: Machine Learning in Python, you will learn to incorporate machine learning in your applications. The book combines an introduction to some of the main concepts and methods in machine learning with practical, hands-on examples of real-world problems. Ranging from handwritten digit recognition to document classification, examples are solved step by step using Scikit-learn and Python. The book starts with a brief introduction to the core concepts of machine learning with a simple example. Then, using real-world applications and advanced features, it takes a deep dive into the various machine learning techniques. You will learn to evaluate your results and apply advanced techniques for preprocessing data. You will also be able to select the best set of features and the best methods for each problem. With Learning scikit-learn: Machine Learning in Python you will learn how to use the Python programming language and the scikit-learn library to build applications that learn from experience, applying the main concepts and techniques of machine learning. What you will learn from this book Set up scikit-learn inside your Python environment Classify objects (from documents to human faces and flower species) based on some of their features, using a variety of methods from Support Vector Machines to Nave Bayes Use Decision Trees to explain the main causes of certain phenomenon such as the Titanic passengers survival Predict house prices using regression techniques Display and analyse groups in your data using dimensionality reduction Make use of different tools to preprocess, extract, and select the learning features Select the best parameters for your models using model selection Improve the way you build your models using parallelization techniques Approach The book adopts a tutorial-based approach to introduce the user to Scikit-learn. Who this book is written for If you are a programmer who wants to explore machine learning and data-based methods to build intelligent applications and enhance your programming skills, this the book for you. No previous experience with machine-learning algorithms is required.\n",
            "\n",
            "Paper ID : 5fb8ba2e3967e079c57aa703bf216c168ea8104f \tArticle : Model-based machine learning\n",
            "Author(s) : ['Charles M. Bishop']\n",
            "Year : 2013 \n",
            "Abstract : Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.\n",
            "\n",
            "Paper ID : a43e61f7900c64e7f3bf18ef7ff5770295149b37 \tArticle : Machine Learning, a Probabilistic Perspective\n",
            "Author(s) : ['C. Robert']\n",
            "Year : 2014 \n",
            "Abstract : ISBN-13: 978-0262018029 The book also introduces the notion of a Bayesian likelihood function (p.228), which “differs slightly from that in classical statistics.” The only difference I can spot is in the interpretation: Both functions of (θ, x) are numerically the same. Overall, the chapter on Bayesian inference does not spend much time on prior specification. There is a section on conjugate priors that does not mention picking the hyperparameters. While improper priors are introduced as limits of proper priors and as conveying “the least amount of information about [the parameters]” (p.236), the difficulty in using improper priors for hypothesis testing is not mentioned. Both Chib’s method and the Savage-Dickey density ratio are suggested for the approximation of marginal likelihoods.\n",
            "\n",
            "Paper ID : 93aa298b40bb3ec23c25239089284fdf61ded917 \tArticle : Support vector machine learning for interdependent and structured output spaces\n",
            "Author(s) : ['Ioannis Tsochantaridis', 'Thomas Hofmann', 'T. Joachims', 'Y. Altun']\n",
            "Year : 2004 \n",
            "Abstract : Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.\n",
            "\n",
            "Paper ID : ea11efe27e029e391ea52609468353f98d9f946b \tArticle : Machine learning on Big Data\n",
            "Author(s) : ['Tyson Condie', 'Paul Mineiro', 'Neoklis Polyzotis', 'Markus Weimer']\n",
            "Year : 2013 \n",
            "Abstract : Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.\n",
            "\n",
            "Paper ID : 84bb60b83f82ad847e19d96403ad0011abfc888f \tArticle : The Boosting Approach to Machine Learning An Overview\n",
            "Author(s) : ['R. Schapire']\n",
            "Year : 2003 \n",
            "Abstract : Boosting is a general method for improving the accuracy of any given learning algorithm. Focusing primarily on the AdaBoost algorithm, this chapter overviews some of the recent work on boosting including analyses of AdaBoost’s training error and generalization error; boosting’s connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of AdaBoost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting.\n",
            "\n",
            "Paper ID : d2b7da2775b8a65034faaa5b1486e5062f827505 \tArticle : Optimization for Machine Learning\n",
            "Author(s) : ['S. Sra', 'S. Nowozin', 'Stephen J. Wright']\n",
            "Year : 2013 \n",
            "Abstract : The interplay between optimization and machine learning is one of the most important developments in modern computational science. Optimization formulations and methods are proving to be vital in designing algorithms to extract essential knowledge from huge volumes of data. Machine learning, however, is not simply a consumer of optimization technology but a rapidly evolving field that is itself generating new optimization ideas. This book captures the state of the art of the interaction between optimization and machine learning in a way that is accessible to researchers in both fields.Optimization approaches have enjoyed prominence in machine learning because of their wide applicability and attractive theoretical properties. The increasing complexity, size, and variety of today's machine learning models call for the reassessment of existing assumptions. This book starts the process of reassessment. It describes the resurgence in novel contexts of established frameworks such as first-order methods, stochastic approximations, convex relaxations, interior-point methods, and proximal methods. It also devotes attention to newer themes such as regularized optimization, robust optimization, gradient and subgradient methods, splitting techniques, and second-order methods. Many of these techniques draw inspiration from other fields, including operations research, theoretical computer science, and subfields of optimization. The book will enrich the ongoing cross-fertilization between the machine learning community and these other fields, and within the broader optimization community.\n",
            "\n",
            "Paper ID : 61017ac0c108f68c987a1fe3e4c2b6223ddd6f31 \tArticle : Machine learning classifiers and fMRI: A tutorial overview\n",
            "Author(s) : ['Francisco Pereira', 'Tom Michael Mitchell', 'M. Botvinick']\n",
            "Year : 2009 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #40------------------------------------\n",
            "\n",
            "Paper ID : c0f7fcc2e03be4395625b6757b17b8834632b952 \tArticle : Ensemble Machine Learning: Methods and Applications\n",
            "Author(s) : ['Cha Zhang', 'Yunqian Ma']\n",
            "Year : 2012 \n",
            "Abstract : It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics. Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.\n",
            "\n",
            "Paper ID : 7da323e7103245eeaed32367c46abe3f4913df86 \tArticle : A survey of techniques for internet traffic classification using machine learning\n",
            "Author(s) : ['Thuy T. T. Nguyen', 'G. Armitage']\n",
            "Year : 2008 \n",
            "Abstract : The research community has begun looking for IP traffic classification techniques that do not rely on `well known¿ TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.\n",
            "\n",
            "Paper ID : f9ae5196908d21336ab02f5c20258dc760d125d6 \tArticle : Adversarial machine learning\n",
            "Author(s) : ['J. Tygar']\n",
            "Year : 2011 \n",
            "Abstract : In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.\n",
            "\n",
            "Paper ID : 0fa45cfa88ee9ac93cb01ec159ad8713d0e32d93 \tArticle : Data Mining: Practical Machine Learning Tools and Techniques, 3/E\n",
            "Author(s) : ['I. Witten', 'Eibe Frank', 'M. Hall']\n",
            "Year : 2011 \n",
            "Abstract : Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. \n",
            " \n",
            " Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. \n",
            " \n",
            " *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks—in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization\n",
            "\n",
            "Paper ID : be9811f7e6019d5cd59ff97829a44bb5577bab00 \tArticle : Machine Learning in Action\n",
            "Author(s) : ['P. Harrington']\n",
            "Year : 2012 \n",
            "Abstract : SummaryMachine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification. About the BookA machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns. Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.Machine Learning in Action is a clearly written tutorial for developers. It avoids academic language and takes you straight to the techniques you'll use in your day-to-day work. Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse. You'll understand the concepts and how they fit in with tactical tasks like classification, forecasting, recommendations, and higher-level features like summarization and simplification.Readers need no prior experience with machine learning or statistical processing. Familiarity with Python is helpful.Purchase includes free PDF, ePub, and Kindle eBooks downloadable at manning.com. What's InsideA no-nonsense introduction Examples showing common ML tasks Everyday data analysis Implementing classic algorithms like Apriori and Adaboos=================================== Table of ContentsPART 1 CLASSIFICATION Machine learning basics Classifying with k-Nearest Neighbors Splitting datasets one feature at a time: decision trees Classifying with probability theory: nave Bayes Logistic regression Support vector machines Improving classification with the AdaBoost meta algorithm PART 2 FORECASTING NUMERIC VALUES WITH REGRESSION Predicting numeric values: regression Tree-based regression PART 3 UNSUPERVISED LEARNING Grouping unlabeled items using k-means clustering Association analysis with the Apriori algorithm Efficiently finding frequent itemsets with FP-growth PART 4 ADDITIONAL TOOLS Using principal component analysis to simplify data Simplifying data with the singular value decomposition Big data and MapReduce\n",
            "\n",
            "Paper ID : 9105a8c02778847bfe0f1c1f95e5e0877c8c2730 \tArticle : Machine Learning in Medical Imaging\n",
            "Author(s) : ['Kenji Suzuki', 'Pingkun Yan', 'Fei Wang', 'D. Shen']\n",
            "Year : 2012 \n",
            "Abstract : Medical imaging is becoming indispensable for patients' healthcare. Machine learning plays an essential role in the medical imaging field, including computer-aided diagnosis, image segmentation, image registration, image fusion, image-guided therapy, image annotation, and image database retrieval. With advances in medical imaging, new imaging modalities and methodologies such as cone-beam/multislice CT, 3D ultrasound imaging, tomosynthesis, diffusion-weighted magnetic resonance imaging (MRI), positron-emission tomography (PET)/CT, electrical impedance tomography, and diffuse optical tomography, new machine-learning algorithms/applications are demanded in the medical imaging field. Because of large variations and complexityit is generally difficult to derive analytic solutions or simple equations to represent objects such as lesions and anatomy in medical images. Therefore, tasks in medical imaging require “learning from examples” for accurate representation of data and prior knowledge. Because of its essential needs, machine learning in medical imaging is one of the most promising, growing fields. \n",
            " \n",
            "The main aim of this special issue is to help advance the scientific research within the broad field of machine learning in medical imaging. The special issue was planned in conjunction with the International Workshop on Machine Learning in Medical Imaging (MLMI 2010) [1], which was the first workshop on this topic, held at the 13th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2010) in September, 2010, in Beijing, China. This special issue is one in a series of special issues of journals on this topic [2]; it focuses on major trends and challenges in this area, and it presents work aimed at identifying new cutting-edge techniques and their use in medical imaging. \n",
            " \n",
            "The quality level of the submissions for this special issue was very high. A total of 17 papers were submitted to this issue in response to the call for papers. Based on a rigorous review process, 10 papers (59%) were accepted for publication in the special issue. The special issue starts by a review of studies on a class of machine-learning techniques, called pixel/voxel-based machine learning, in medical imaging by K. Suzuki. A series of medical imaging applications of machine-learning techniques are presented. A large variety of applications are well represented here, including organ modeling by D. Wang et al. and X. Qiao and Y.-W. Chen, brain function estimation by V. Michel et al., image reconstruction by H. Shouno et al., lesion classification by P. Wighton et al., modality classification by X.-H. Han and Y.-W. Chen, lesion segmentation by M. Zortea et al., organ segmentation by S. Alzubi et al., and visualization of molecular signals by F. Mattoli et al. Also, the issue covers various biomedical imaging modalities, including MRI by D. Wang et al., CT by X. Qiao and Y.-W. Chen and H. Shouno et al., functional MRI by V. Michel et al., dermoscopy by P. Wighton et al. and M. Zortea et al., scintigraphy by X.-H. Han and Y.-W. Chen, ultrasound imaging by X.-H. Han and Y.-W. Chen, radiography by X.-H. Han and Y.-W. Chen, MR angiography by S. Alzubi et al., and microscopy by F. Mattoli et al. as well as a variety of organs, including the kidneys by D. Wang et al., liver by X. Qiao and Y.-W. Chen, brain by V. Michel et al. and F. Mattoli et al., chest by S. Alzubi et al., skin by P. Wighton et al. and M. Zortea et al., and heart by F. Mattoli et al. Various machine-learning techniques were developed/used to solve the respective problems, including structured dictionary learning by D. Wang et al., generalized N-dimensional principal component analysis by X. Qiao et al., multiclass sparse Bayesian regression by V. Michel et al., Bayesian hyperparameter inference by H. Shouno et al., supervised learning of probabilistic models based on maximum aposteriori estimation and conditional random fields by P. Wighton et al., joint kernel equal contribution in support vector classification by X.-H. Han and Y.-W. Chen, and iterative hybrid classification by M. Zortea et al. \n",
            " \n",
            "We are grateful to all authors for their excellent contributions to this special issue and to all reviewers for their reviews and constructive suggestions. We hope that this special issue will inspire further ideas for creative research, advance the field of machine learning in medical imaging, and facilitate the translation of the research from bench to bedside. \n",
            " \n",
            " \n",
            "Kenji Suzuki \n",
            " \n",
            "Pingkun Yan \n",
            " \n",
            "Fei Wang \n",
            " \n",
            "Dinggang Shen\n",
            "\n",
            "Paper ID : 90848562905d26873b57bbf3f2f98319e38e5bde \tArticle : Density Ratio Estimation in Machine Learning\n",
            "Author(s) : ['Masashi Sugiyama', 'Taiji Suzuki', 'T. Kanamori']\n",
            "Year : 2012 \n",
            "Abstract : Machine learning is an interdisciplinary field of science and engineering that studies mathematical theories and practical applications of systems that learn. This book introduces theories, methods, and applications of density ratio estimation, which is a newly emerging paradigm in the machine learning community. Various machine learning problems such as non-stationarity adaptation, outlier detection, dimensionality reduction, independent component analysis, clustering, classification, and conditional density estimation can be systematically solved via the estimation of probability density ratios. The authors offer a comprehensive introduction of various density ratio estimators including methods via density estimation, moment matching, probabilistic classification, density fitting, and density ratio fitting as well as describing how these can be applied to machine learning. The book also provides mathematical theories for density ratio estimation including parametric and non-parametric convergence analysis and numerical stability analysis to complete the first and definitive treatment of the entire framework of density ratio estimation in machine learning.\n",
            "\n",
            "Paper ID : 498ca0a1f8c980586408addf7ab2919ecdb7dd3d \tArticle : Factorizing YAGO: scalable machine learning for linked data\n",
            "Author(s) : ['Maximilian Nickel', 'Volker Tresp', 'H. Kriegel']\n",
            "Year : 2012 \n",
            "Abstract : Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 ⋅ 1014 possible triples in the YAGO~2 core ontology.\n",
            "\n",
            "Paper ID : bb99668d4df98a3f6ff0b9fa3402e09008f22e2c \tArticle : Making large-scale support vector machine learning practical\n",
            "Author(s) : ['T. Joachims']\n",
            "Year : 1998 \n",
            "Abstract : Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.\n",
            "\n",
            "Paper ID : fc2057499fcc5dfe61316f1722db7837971a9c94 \tArticle : ML Confidential: Machine Learning on Encrypted Data\n",
            "Author(s) : ['T. Graepel', 'K. Lauter', 'M. Naehrig']\n",
            "Year : 2012 \n",
            "Abstract : We demonstrate that, by using a recently proposed leveled homomorphic encryption scheme, it is possible to delegate the execution of a machine learning algorithm to a computing service while retaining confidentiality of the training and test data. Since the computational complexity of the homomorphic encryption scheme depends primarily on the number of levels of multiplications to be carried out on the encrypted data, we define a new class of machine learning algorithms in which the algorithm's predictions, viewed as functions of the input data, can be expressed as polynomials of bounded degree. We propose confidential algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential machine learning pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security.\n",
            "------------------------------------Extracting Page #41------------------------------------\n",
            "\n",
            "Paper ID : fc2057499fcc5dfe61316f1722db7837971a9c94 \tArticle : ML Confidential: Machine Learning on Encrypted Data\n",
            "Author(s) : ['T. Graepel', 'K. Lauter', 'M. Naehrig']\n",
            "Year : 2012 \n",
            "Abstract : We demonstrate that, by using a recently proposed leveled homomorphic encryption scheme, it is possible to delegate the execution of a machine learning algorithm to a computing service while retaining confidentiality of the training and test data. Since the computational complexity of the homomorphic encryption scheme depends primarily on the number of levels of multiplications to be carried out on the encrypted data, we define a new class of machine learning algorithms in which the algorithm's predictions, viewed as functions of the input data, can be expressed as polynomials of bounded degree. We propose confidential algorithms for binary classification based on polynomial approximations to least-squares solutions obtained by a small number of gradient descent steps. We present experimental validation of the confidential machine learning pipeline and discuss the trade-offs regarding computational complexity, prediction accuracy and cryptographic security.\n",
            "\n",
            "Paper ID : d2972fa779c91162f447d1e15540fba0df4cb547 \tArticle : Deploying an interactive machine learning system in an evidence-based practice center: abstrackr\n",
            "Author(s) : ['Byron C. Wallace', 'Kevin Small', 'C. Brodley', 'J. Lau', 'T. Trikalinos']\n",
            "Year : 2012 \n",
            "Abstract : Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.\n",
            "\n",
            "Paper ID : a35d565e8f70cb6ddc09a65904a72622e16b9485 \tArticle : Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers\n",
            "Author(s) : ['Stephen P. Boyd', 'Neal Parikh', 'E. Chu', 'B. Peleato', 'Jonathan Eckstein']\n",
            "Year : 2011 \n",
            "Abstract : Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.\n",
            "\n",
            "Paper ID : 99a39ea1b9bf4e3afc422329c1d4d77446f060b8 \tArticle : Machine Learning Strategies for Time Series Forecasting\n",
            "Author(s) : ['Gianluca Bontempi', 'S. B. Taieb', 'Y. Borgne']\n",
            "Year : 2012 \n",
            "Abstract : The increasing availability of large amounts of historical data and the need of performing accurate forecasting of future behavior in several scientific and applied domains demands the definition of robust and efficient techniques able to infer from observations the stochastic dependency between past and future. The forecasting domain has been influenced, from the 1960s on, by linear statistical methods such as ARIMA models. More recently, machine learning models have drawn attention and have established themselves as serious contenders to classical statistical models in the forecasting community. This chapter presents an overview of machine learning techniques in time series forecasting by focusing on three aspects: the formalization of one-step forecasting problems as supervised learning tasks, the discussion of local learning techniques as an effective tool for dealing with temporal data and the role of the forecasting strategy when we move from one-step to multiple-step forecasting.\n",
            "\n",
            "Paper ID : c62043a7d2537bbf40a84b9913957452a47fdb83 \tArticle : Dataset Shift in Machine Learning\n",
            "Author(s) : ['Joaquin Quionero-Candela', 'Masashi Sugiyama', 'Anton Schwaighofer', 'Neil D. Lawrence']\n",
            "Year : 2009 \n",
            "Abstract : Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series\n",
            "\n",
            "Paper ID : 6c10262a5d4230c7c85fbe528b8bbd9444116bca \tArticle : Finding Density Functionals with Machine Learning\n",
            "Author(s) : ['John C. Snyder', 'M. Rupp', 'K. Hansen', 'K. Müller', 'K. Burke']\n",
            "Year : 2012 \n",
            "Abstract : Machine learning is used to approximate density functionals. For the model problem of the kinetic energy of noninteracting fermions in 1D, mean absolute errors below 1 kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities. A predictor identifies if a test density is within the interpolation region. Via principal component analysis, a projected functional derivative finds highly accurate self-consistent densities. The challenges for application of our method to real electronic structure problems are discussed.\n",
            "\n",
            "Paper ID : 28d82be56e54df4b28a17908de17480d699e1409 \tArticle : A Machine Learning Approach to Android Malware Detection\n",
            "Author(s) : ['Justin Sahs', 'L. Khan']\n",
            "Year : 2012 \n",
            "Abstract : With the recent emergence of mobile platforms capable of executing increasingly complex software and the rising ubiquity of using mobile platforms in sensitive applications such as banking, there is a rising danger associated with malware targeted at mobile devices. The problem of detecting such malware presents unique challenges due to the limited resources avalible and limited privileges granted to the user, but also presents unique opportunity in the required metadata attached to each application. In this article, we present a machine learning-based system for the detection of malware on Android devices. Our system extracts a number of features and trains a One-Class Support Vector Machine in an offline (off-device) manner, in order to leverage the higher computing power of a server or cluster of servers.\n",
            "\n",
            "Paper ID : d837267b364b4dc97bb35facda235a19be5ed374 \tArticle : Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation\n",
            "Author(s) : ['Masashi Sugiyama', 'M. Kawanabe']\n",
            "Year : 2012 \n",
            "Abstract : As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.\n",
            "\n",
            "Paper ID : 7ea08581dff3d8ac301b80e693ebe0e6d247f2f5 \tArticle : The master algorithm: how the quest for the ultimate learning machine will remake our world\n",
            "Author(s) : ['W. Hasperué']\n",
            "Year : 2015 \n",
            "Abstract : Nowadays, “machine learning” is present in several aspects of the current world, internet advisors, advertisements and “smart” devices that seem to know what we need in a given moment. These are some examples of the problems solved by machine learning. This book presents the past, the present and the future of the different types of machine learning algorithms. At the beginning of the book, the author takes us to the first years of the computing science, where a programmer had to do absolutely everything by himself to make an algorithm do a certain task. As time passes, there appeared the first algorithms that were capable of programming themselves learning from the available data. The author presents what he himself calls the five “tribes” of machine learning, the essence that defends each one and the kind of problems that are able to solve without problems. With a great amount of simple examples, the author depicts which advantages and disadvantages of the “master” algorithms of each “tribes” are, saying that the problem that a tribe solves perfectly well, another one cannot do it, and the other way about. The author suggests to get the best out of each “tribe” and make a unique learning algorithm able to learn without caring about the problem: the master algorithm.\n",
            "\n",
            "Paper ID : 222c68fc80b05064680ddc467243b620604fd11e \tArticle : Machine learning and radiology\n",
            "Author(s) : ['Shijun Wang', 'R. Summers']\n",
            "Year : 2012 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #42------------------------------------\n",
            "\n",
            "Paper ID : 7fe7e80bf59a112386211b38ef2ea0b71ae76345 \tArticle : Machine Learning that Matters\n",
            "Author(s) : ['K. Wagstaff']\n",
            "Year : 2012 \n",
            "Abstract : Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.\n",
            "\n",
            "Paper ID : ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da \tArticle : Making machine learning models interpretable\n",
            "Author(s) : ['A. Vellido', 'J. Martín-Guerrero', 'P. Lisboa']\n",
            "Year : 2012 \n",
            "Abstract : Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.\n",
            "\n",
            "Paper ID : 02227c94dd41fe0b439e050d377b0beb5d427cda \tArticle : Reading Digits in Natural Images with Unsupervised Feature Learning\n",
            "Author(s) : ['Yuval Netzer', 'Tao Wang', 'Adam Coates', 'A. Bissacco', 'Bo Wu', 'A. Ng']\n",
            "Year : 2011 \n",
            "Abstract : Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.\n",
            "\n",
            "Paper ID : 3c8bf504ddc7db1829466b6e9da5251025dd48f1 \tArticle : Automatic analysis of malware behavior using machine learning\n",
            "Author(s) : ['K. Rieck', 'Philipp Trinius', 'Carsten Willems', 'Thorsten Holz']\n",
            "Year : 2011 \n",
            "Abstract : Malicious software - so called malware - poses a major threat to the security of computer systems. The amount and diversity of its variants render classic security defenses ineffective, such that millions of hosts in the Internet are infected with malware in the form of computer viruses, Internet worms and Trojan horses. While obfuscation and polymorphism employed by malware largely impede detection at file level, the dynamic analysis of malware binaries during run-time provides an instrument for characterizing and defending against the threat of malicious software. \n",
            " \n",
            "In this article, we propose a framework for the automatic analysis of malware behavior using machine learning. The framework allows for automatically identifying novel classes of malware with similar behavior (clustering) and assigning unknown malware to these discovered classes (classification). Based on both, clustering and classification, we propose an incremental approach for behavior-based analysis, capable of processing the behavior of thousands of malware binaries on a daily basis. The incremental analysis significantly reduces the run-time overhead of current analysis methods, while providing accurate discovery and discrimination of novel malware variants.\n",
            "\n",
            "Paper ID : 1ee7be29ce297de6d79a438d26037595967f5b17 \tArticle : Ensemble Machine Learning\n",
            "Author(s) : ['Cha Zhang', 'Yunqian Ma']\n",
            "Year : 2012 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 58209c6db7b321ea7c75395b23ddb5100cd9bf81 \tArticle : Machine Learning for the New York City Power Grid\n",
            "Author(s) : ['C. Rudin', 'D. Waltz', 'Roger N. Anderson', 'A. Boulanger', 'Ansaf Salleb-Aouissi', 'M. Chow', 'Haimonti Dutta', 'Philip Gross', 'Bert Huang', 'Steve Ierome']\n",
            "Year : 2012 \n",
            "Abstract : Power companies can benefit from the use of knowledge discovery methods and statistical machine learning for preventive maintenance. We introduce a general process for transforming historical electrical grid data into models that aim to predict the risk of failures for components and systems. These models can be used directly by power companies to assist with prioritization of maintenance and repair work. Specialized versions of this process are used to produce (1) feeder failure rankings, (2) cable, joint, terminator, and transformer rankings, (3) feeder Mean Time Between Failure (MTBF) estimates, and (4) manhole events vulnerability rankings. The process in its most general form can handle diverse, noisy, sources that are historical (static), semi-real-time, or real-time, incorporates state-of-the-art machine learning algorithms for prioritization (supervised ranking or MTBF), and includes an evaluation of results via cross-validation and blind test. Above and beyond the ranked lists and MTBF estimates are business management interfaces that allow the prediction capability to be integrated directly into corporate planning and decision support; such interfaces rely on several important properties of our general modeling approach: that machine learning features are meaningful to domain experts, that the processing of data is transparent, and that prediction results are accurate enough to support sound decision making. We discuss the challenges in working with historical electrical grid data that were not designed for predictive purposes. The “rawness” of these data contrasts with the accuracy of the statistical models that can be obtained from the process; these models are sufficiently accurate to assist in maintaining New York City's electrical grid.\n",
            "\n",
            "Paper ID : b6cf9167aeb2782651156de5e22cad82ee69a225 \tArticle : UCI Repository of Machine Learning Databases\n",
            "Author(s) : ['C. Merz']\n",
            "Year : 1996 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 9ead7583542e55c84bb9b90260ffde7a70c88e8d \tArticle : Large-scale machine learning at twitter\n",
            "Author(s) : ['Jimmy J. Lin', 'A. Kolcz']\n",
            "Year : 2012 \n",
            "Abstract : The success of data-driven solutions to difficult problems, along with the dropping costs of storing and processing massive amounts of data, has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. We begin with an overview of this platform, which handles \"traditional\" data warehousing and business intelligence tasks for the organization. The core of this work lies in recent Pig extensions to provide predictive analytics capabilities that incorporate machine learning, focused specifically on supervised classification. In particular, we have identified stochastic gradient descent techniques for online learning and ensemble methods as being highly amenable to scaling out to large amounts of data. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig, via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment, as well as access to rich libraries of user-defined functions and the materialized output of other scripts.\n",
            "\n",
            "Paper ID : 1b04936c2599e59b120f743fbb30df2eed3fd782 \tArticle : Shortcut Learning in Deep Neural Networks\n",
            "Author(s) : ['Robert Geirhos', 'J. Jacobsen', 'Claudio Michaelis', 'R. Zemel', 'Wieland Brendel', 'M. Bethge', 'Felix Wichmann']\n",
            "Year : 2020 \n",
            "Abstract : Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.\n",
            "\n",
            "Paper ID : 9e35e0fa69a96dea383890401d44366117eb6631 \tArticle : Machine Learning\n",
            "Author(s) : ['F. Alexander']\n",
            "Year : 2013 \n",
            "Abstract : The guest editor discusses some recent advances in machine learning and their applications to exciting new problem areas.\n",
            "------------------------------------Extracting Page #43------------------------------------\n",
            "\n",
            "Paper ID : d37fc9e9c4fedc32865b08661e7fb950df1f8fbe \tArticle : Kernel methods in machine learning\n",
            "Author(s) : ['Thomas Hofmann', 'B. Schölkopf', 'Alex Smola']\n",
            "Year : 2008 \n",
            "Abstract : We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.\n",
            "\n",
            "Paper ID : f04df4e20a18358ea2f689b4c129781628ef7fc1 \tArticle : A large annotated corpus for learning natural language inference\n",
            "Author(s) : ['Samuel R. Bowman', 'Gabor Angeli', 'Christopher Potts', 'Christopher D. Manning']\n",
            "Year : 2015 \n",
            "Abstract : Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.\n",
            "\n",
            "Paper ID : 0148bbc80ea2f2526ab019a317639b4fb357f399 \tArticle : A Machine Learning Approach to Twitter User Classification\n",
            "Author(s) : ['M. Pennacchiotti', 'Ana-Maria Popescu']\n",
            "Year : 2011 \n",
            "Abstract : This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.\n",
            "\n",
            "Paper ID : 6d9f55b445f36578802e7eef4393cfa914b11620 \tArticle : Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary\n",
            "Author(s) : ['P. D. Sahin', 'Kobus Barnard', 'João Freitas', 'D. Forsyth']\n",
            "Year : 2002 \n",
            "Abstract : We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.\n",
            "\n",
            "Paper ID : 55bc938f92ca50c617ec95691614731aa498abec \tArticle : Encyclopedia of Machine Learning\n",
            "Author(s) : ['C. Sammut', 'Geoffrey I. Webb']\n",
            "Year : 2010 \n",
            "Abstract : This comprehensive encyclopedia, with over 250 entries in an A-Z format, provides easy access to relevant information for those seeking entry into any aspect within the broad field of machine learning. Most entries in this preeminent work include useful literature references.Topics for the Encyclopedia of Machine Learning were selected by a distinguished international advisory board. These peer-reviewed, highly-structured entries include definitions, illustrations, applications, bibliographies and links to related literature, providing the reader with a portal to more detailed information on any given topic.The style of the entries in the Encyclopedia of Machine Learning is expository and tutorial, making the book a practical resource for machine learning experts, as well as professionals in other fields who need to access this vital information but may not have the time to work their way through an entire text on their topic of interest.The authoritative reference is published both in print and online. The print publication includes an index of subjects and authors. The online edition supplements this index with hyperlinks as well as internal hyperlinks to related entries in the text, CrossRef citations, and links to additional significant research.\n",
            "\n",
            "Paper ID : 0e779fd59353a7f1f5b559b9d65fa4bfe367890c \tArticle : Geometric Deep Learning: Going beyond Euclidean data\n",
            "Author(s) : ['M. Bronstein', 'Joan Bruna', 'Yann LeCun', 'Arthur D. Szlam', 'P. Vandergheynst']\n",
            "Year : 2017 \n",
            "Abstract : Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.\n",
            "\n",
            "Paper ID : 4187caa4d0d329f47e18377a6cd31ef3f580cfcc \tArticle : GraphLab: A New Framework For Parallel Machine Learning\n",
            "Author(s) : ['Y. Low', 'Joseph E. Gonzalez', 'Aapo Kyrola', 'D. Bickson', 'Carlos Guestrin', 'J. Hellerstein']\n",
            "Year : 2010 \n",
            "Abstract : Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.\n",
            "\n",
            "Paper ID : ea58af907495e97c93997119db4a59fab5cd3683 \tArticle : Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]\n",
            "Author(s) : ['I. Arel', 'Derek C. Rose', 'T. Karnowski']\n",
            "Year : 2010 \n",
            "Abstract : This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and \"weaknesses, depending on the application and context in \"which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.\n",
            "\n",
            "Paper ID : 79cf9462a583e1889781868cbf8c31e43b36dd2f \tArticle : Towards Federated Learning at Scale: System Design\n",
            "Author(s) : ['Keith Bonawitz', 'Hubert Eichner', 'W. Grieskamp', 'Dzmitry Huba', 'A. Ingerman', 'Vladimir Ivanov', 'Chloé Kiddon', 'Jakub Konecný', 'Stefano Mazzocchi', 'H. B. McMahan', 'Timon Van Overveldt', 'David Petrou', 'D. Ramage', 'Jason Roselander']\n",
            "Year : 2019 \n",
            "Abstract : Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.\n",
            "\n",
            "Paper ID : 819167ace2f0caae7745d2f25a803979be5fbfae \tArticle : The Limitations of Deep Learning in Adversarial Settings\n",
            "Author(s) : ['Nicolas Papernot', 'P. Mcdaniel', 'S. Jha', 'Matt Fredrikson', 'Z. B. Celik', 'A. Swami']\n",
            "Year : 2016 \n",
            "Abstract : Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.\n",
            "------------------------------------Extracting Page #44------------------------------------\n",
            "\n",
            "Paper ID : 4187caa4d0d329f47e18377a6cd31ef3f580cfcc \tArticle : GraphLab: A New Framework For Parallel Machine Learning\n",
            "Author(s) : ['Y. Low', 'Joseph E. Gonzalez', 'Aapo Kyrola', 'D. Bickson', 'Carlos Guestrin', 'J. Hellerstein']\n",
            "Year : 2010 \n",
            "Abstract : Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.\n",
            "\n",
            "Paper ID : ea58af907495e97c93997119db4a59fab5cd3683 \tArticle : Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]\n",
            "Author(s) : ['I. Arel', 'Derek C. Rose', 'T. Karnowski']\n",
            "Year : 2010 \n",
            "Abstract : This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and \"weaknesses, depending on the application and context in \"which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.\n",
            "\n",
            "Paper ID : 79cf9462a583e1889781868cbf8c31e43b36dd2f \tArticle : Towards Federated Learning at Scale: System Design\n",
            "Author(s) : ['Keith Bonawitz', 'Hubert Eichner', 'W. Grieskamp', 'Dzmitry Huba', 'A. Ingerman', 'Vladimir Ivanov', 'Chloé Kiddon', 'Jakub Konecný', 'Stefano Mazzocchi', 'H. B. McMahan', 'Timon Van Overveldt', 'David Petrou', 'D. Ramage', 'Jason Roselander']\n",
            "Year : 2019 \n",
            "Abstract : Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.\n",
            "\n",
            "Paper ID : 819167ace2f0caae7745d2f25a803979be5fbfae \tArticle : The Limitations of Deep Learning in Adversarial Settings\n",
            "Author(s) : ['Nicolas Papernot', 'P. Mcdaniel', 'S. Jha', 'Matt Fredrikson', 'Z. B. Celik', 'A. Swami']\n",
            "Year : 2016 \n",
            "Abstract : Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.\n",
            "\n",
            "Paper ID : 1d122a074c936fcfd95faf44608e377a9d1799c8 \tArticle : DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\n",
            "Author(s) : ['Huifeng Guo', 'Ruiming Tang', 'Yunming Ye', 'Zhenguo Li', 'Xiuqiang He']\n",
            "Year : 2017 \n",
            "Abstract : Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.\n",
            "\n",
            "Paper ID : afd0859a858481d2f36109f68090aebd77456b7f \tArticle : The security of machine learning\n",
            "Author(s) : ['M. Barreno', 'Blaine Nelson', 'A. Joseph', 'J. Tygar']\n",
            "Year : 2010 \n",
            "Abstract : Machine learning’s ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.\n",
            "\n",
            "Paper ID : a154e688baa929c335dd9a673592797ec3c27281 \tArticle : Learning from positive and unlabeled data: a survey\n",
            "Author(s) : ['Jessa Bekker', 'Jesse Davis']\n",
            "Year : 2020 \n",
            "Abstract : Learning from positive and unlabeled data or PU learning is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the machine learning literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art in PU learning. It proposes seven key research questions that commonly arise in this field and provides a broad overview of how the field has tried to address them.\n",
            "\n",
            "Paper ID : 6c16cf47f2b872e7b2ad06facb5d491857650514 \tArticle : C4.5: Programs for Machine Learning (書評)\n",
            "Author(s) : ['金田 重郎']\n",
            "Year : 1995 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f \tArticle : Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations\n",
            "Author(s) : ['M. Raissi', 'A. Yazdani', 'G. Karniadakis']\n",
            "Year : 2020 \n",
            "Abstract : Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.\n",
            "\n",
            "Paper ID : edb4643a71d734543fa4ae3cdf7a89177c5e9ebe \tArticle : Introduction to machine learning for brain imaging\n",
            "Author(s) : ['S. Lemm', 'B. Blankertz', 'T. Dickhaus', 'K. Müller']\n",
            "Year : 2011 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #45------------------------------------\n",
            "\n",
            "Paper ID : e7e25fd534e9e024da329aea546484938df305a5 \tArticle : Gaussian Processes for Machine Learning (GPML) Toolbox\n",
            "Author(s) : ['C. Rasmussen', 'H. Nickisch']\n",
            "Year : 2010 \n",
            "Abstract : The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.\n",
            "\n",
            "Paper ID : e3d772986d176057aca2f5e3eb783da53b559134 \tArticle : Unsupervised Machine Translation Using Monolingual Corpora Only\n",
            "Author(s) : ['Guillaume Lample', 'Ludovic Denoyer', \"Marc'Aurelio Ranzato\"]\n",
            "Year : 2018 \n",
            "Abstract : Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.\n",
            "\n",
            "Paper ID : b5be3165d56580b60e29ad1a4a08b124d6cb8264 \tArticle : Scaling up machine learning: parallel and distributed approaches\n",
            "Author(s) : ['R. Bekkerman', 'M. Bilenko', 'J. Langford']\n",
            "Year : 2011 \n",
            "Abstract : This tutorial gives a broad view of modern approaches for scaling up machine learning and data mining methods on parallel/distributed platforms. Demand for scaling up machine learning is task-specific: for some tasks it is driven by the enormous dataset sizes, for others by model complexity or by the requirement for real-time prediction. Selecting a task-appropriate parallelization platform and algorithm requires understanding their benefits, trade-offs and constraints. This tutorial focuses on providing an integrated overview of state-of-the-art platforms and algorithm choices. These span a range of hardware options (from FPGAs and GPUs to multi-core systems and commodity clusters), programming frameworks (including CUDA, MPI, MapReduce, and DryadLINQ), and learning settings (e.g., semi-supervised and online learning). The tutorial is example-driven, covering a number of popular algorithms (e.g., boosted trees, spectral clustering, belief propagation) and diverse applications (e.g., recommender systems and object recognition in vision).\n",
            " The tutorial is based on (but not limited to) the material from our upcoming Cambridge U. Press edited book which is currently in production.\n",
            " Visit the tutorial website at http://hunch.net/~large_scale_survey/\n",
            "\n",
            "Paper ID : c526ad0ea8b4cff9a671eb8a90ea98eb64ae17a7 \tArticle : Sparse Bayesian Extreme Learning Machine for Multi-classification\n",
            "Author(s) : ['Jiahua Luo', 'C. Vong', 'P. Wong']\n",
            "Year : 2014 \n",
            "Abstract : Extreme learning machine (ELM) has become a popular topic in machine learning in recent years. ELM is a new kind of single-hidden layer feedforward neural network with an extremely low computational cost. ELM, however, has two evident drawbacks: 1) the output weights solved by Moore-Penrose generalized inverse is a least squares minimization issue, which easily suffers from overfitting and 2) the accuracy of ELM is drastically sensitive to the number of hidden neurons so that a large model is usually generated. This brief presents a sparse Bayesian approach for learning the output weights of ELM in classification. The new model, called Sparse Bayesian ELM (SBELM), can resolve these two drawbacks by estimating the marginal likelihood of network outputs and automatically pruning most of the redundant hidden neurons during learning phase, which results in an accurate and compact model. The proposed SBELM is evaluated on wide types of benchmark classification problems, which verifies that the accuracy of SBELM model is relatively insensitive to the number of hidden neurons; and hence a much more compact model is always produced as compared with other state-of-the-art neural network classifiers.\n",
            "\n",
            "Paper ID : 713a14e98f65a74652956dae94874eeab61a71cd \tArticle : The changing science of machine learning\n",
            "Author(s) : ['P. Langley']\n",
            "Year : 2011 \n",
            "Abstract : Machine Learning saw its first issues appear in 1986. Naturally, the publication did not spring fully grown from its founders’ brows; we laid the first plans in 1983, during the Second International Workshop on Machine Learning at Allerton House, Illinois. My partners in crime were Ryszard Michalski, Jaime Carbonell, and Tom Mitchell, who had organized the first two workshops and who had edited the book based on the initial event (Michalski et al. 1983). The reasons for launching a new journal were familiar ones. The first workshop in 1980 at Carnegie-Mellon University had identified a community of researchers with common interests in computational approaches to learning and arrived at a name for its activities. Moreover, the parent fields of artificial intelligence and cognitive science were showing little interest at the time in learning-related issues, preferring to focus on the role of knowledge in intelligence, regardless of its origin. As a result, we encountered some difficulty publishing and, more generally, felt we were not getting the attention we deserved. Finally, there was the common urge of young, energetic researchers to create something of their own to which they could attach their names. At least that was part of my own motivation; I cannot speak for my co-founders. Our group agreed that the new field needed a journal, and we also decided that I would be the first Executive Editor, with the other three serving as Editors. In 1984, I moved from Carnegie-Mellon University to the University of California, Irvine, where I received department support for the enterprise and set up an editorial office. We began to explore publishers, eventually settled on Kluwer, and started to solicit submissions. We made many decisions in those early days, from selecting an initial editorial board to determining the publication’s size and format1 that I cannot detail here.\n",
            "\n",
            "Paper ID : a7fce24eb350e592338ffa9f3b7f8b2a8887b0dd \tArticle : Human Decisions and Machine Predictions\n",
            "Author(s) : ['J. Kleinberg', 'Himabindu Lakkaraju', 'J. Leskovec', 'J. Ludwig', 'S. Mullainathan']\n",
            "Year : 2018 \n",
            "Abstract : Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; and these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals. JEL Codes: C10 (Econometric and statistical methods and methodology), C55 (Large datasets: Modeling and analysis), K40 (Legal procedure, the legal system, and illegal behavior).\n",
            "\n",
            "Paper ID : 80ab1a48aac6c2716cb52334ca2f0872adfe3a6b \tArticle : Extreme learning machine: algorithm, theory and applications\n",
            "Author(s) : ['Shifei Ding', 'Han Zhao', 'Yanan Zhang', 'Xinzheng Xu', 'Ru Nie']\n",
            "Year : 2013 \n",
            "Abstract : Extreme learning machine (ELM) is a new learning algorithm for the single hidden layer feedforward neural networks. Compared with the conventional neural network learning algorithm it overcomes the slow training speed and over-fitting problems. ELM is based on empirical risk minimization theory and its learning process needs only a single iteration. The algorithm avoids multiple iterations and local minimization. It has been used in various fields and applications because of better generalization ability, robustness, and controllability and fast learning rate. In this paper, we make a review of ELM latest research progress about the algorithms, theory and applications. It first analyzes the theory and the algorithm ideas of ELM, then tracking describes the latest progress of ELM in recent years, including the model and specific applications of ELM, finally points out the research and development prospects of ELM in the future.\n",
            "\n",
            "Paper ID : 10bc3b9bf71c4448ea0f4bec441403c0e50a0691 \tArticle : Machine Learning - An Algorithmic Perspective\n",
            "Author(s) : ['S. Marsland']\n",
            "Year : 2009 \n",
            "Abstract : Written in an easily accessible style, this book provides the ideal blend of theory and practical, applicable knowledge. It covers neural networks, graphical models, reinforcement learning, evolutionary algorithms, dimensionality reduction methods, and the important area of optimization. It treads the fine line between adequate academic rigor and overwhelming students with equations and mathematical concepts. The author includes examples based on widely available datasets and practical and theoretical problems to test understanding and application of the material. The book describes algorithms with code examples backed up by a website that provides working implementations in Python.\n",
            "\n",
            "Paper ID : d5b8b4bffc457150310dd3713241310a7f5c6165 \tArticle : SystemML: Declarative machine learning on MapReduce\n",
            "Author(s) : ['A. Ghoting', 'R. Krishnamurthy', 'E. Pednault', 'B. Reinwald', 'V. Sindhwani', 'S. Tatikonda', 'Yuanyuan Tian', 'Shivakumar Vaithyanathan']\n",
            "Year : 2011 \n",
            "Abstract : MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.\n",
            "\n",
            "Paper ID : 90972e7394b5fc884470cf78a657aae3932a8d8a \tArticle : Using Machine Learning to Detect Cyberbullying\n",
            "Author(s) : ['Kelly Reynolds', 'April Kontostathis', 'Lynne Edwards']\n",
            "Year : 2011 \n",
            "Abstract : Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5% accuracy.\n",
            "------------------------------------Extracting Page #46------------------------------------\n",
            "\n",
            "Paper ID : 10bc3b9bf71c4448ea0f4bec441403c0e50a0691 \tArticle : Machine Learning - An Algorithmic Perspective\n",
            "Author(s) : ['S. Marsland']\n",
            "Year : 2009 \n",
            "Abstract : Written in an easily accessible style, this book provides the ideal blend of theory and practical, applicable knowledge. It covers neural networks, graphical models, reinforcement learning, evolutionary algorithms, dimensionality reduction methods, and the important area of optimization. It treads the fine line between adequate academic rigor and overwhelming students with equations and mathematical concepts. The author includes examples based on widely available datasets and practical and theoretical problems to test understanding and application of the material. The book describes algorithms with code examples backed up by a website that provides working implementations in Python.\n",
            "\n",
            "Paper ID : d5b8b4bffc457150310dd3713241310a7f5c6165 \tArticle : SystemML: Declarative machine learning on MapReduce\n",
            "Author(s) : ['A. Ghoting', 'R. Krishnamurthy', 'E. Pednault', 'B. Reinwald', 'V. Sindhwani', 'S. Tatikonda', 'Yuanyuan Tian', 'Shivakumar Vaithyanathan']\n",
            "Year : 2011 \n",
            "Abstract : MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.\n",
            "\n",
            "Paper ID : 90972e7394b5fc884470cf78a657aae3932a8d8a \tArticle : Using Machine Learning to Detect Cyberbullying\n",
            "Author(s) : ['Kelly Reynolds', 'April Kontostathis', 'Lynne Edwards']\n",
            "Year : 2011 \n",
            "Abstract : Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5% accuracy.\n",
            "\n",
            "Paper ID : 2a4ba0c1699965381fb2ba802157a89edd217943 \tArticle : Data Mining and Machine Learning in Cybersecurity\n",
            "Author(s) : ['S. Dua', 'Xian Du']\n",
            "Year : 2011 \n",
            "Abstract : With the rapid advancement of information discovery techniques, machine learning and data mining continue to play a significant role in cybersecurity. Although several conferences, workshops, and journals focus on the fragmented research topics in this area, there has been no single interdisciplinary resource on past and current works and possible paths for future research in this area. This book fills this need. From basic concepts in machine learning and data mining to advanced problems in the machine learning domain, Data Mining and Machine Learning in Cybersecurity provides a unified reference for specific machine learning solutions to cybersecurity problems. It supplies a foundation in cybersecurity fundamentals and surveys contemporary challengesdetailing cutting-edge machine learning and data mining techniques. It also: Unveils cutting-edge techniques for detectingnew attacks Contains in-depth discussions of machine learning solutions to detection problems Categorizes methods for detecting, scanning, and profiling intrusions and anomalies Surveys contemporary cybersecurity problems and unveils state-of-the-art machine learning and data mining solutions Details privacy-preserving data mining methods This interdisciplinary resource includes technique review tables that allow for speedy access to common cybersecurity problems and associated data mining methods. Numerous illustrative figures help readers visualize the workflow of complex techniques and more than forty case studies provide a clear understanding of the design and application of data mining and machine learning techniques in cybersecurity.\n",
            "\n",
            "Paper ID : f308f4305a7295d89ca6d287c351ce117952a710 \tArticle : Machine Learning: The ingredients of machine learning\n",
            "Author(s) : ['P. Flach']\n",
            "Year : 2012 \n",
            "Abstract : MACHINE LEARNING IS ALL ABOUT using the right features to build the right models that achieve the right tasks – this is the slogan, visualised in Figure 3 on p.11, with which we ended the Prologue. In essence, features define a ‘language’ in which we describe the relevant objects in our domain, be they e-mails or complex organic molecules. We should not normally have to go back to the domain objects themselves once we have a suitable feature representation, which is why features play such an important role in machine learning. We will take a closer look at them in Section 1.3. A task is an abstract representation of a problem we want to solve regarding those domain objects: the most common form of these is classifying them into two or more classes, but we shall encounter other tasks throughout the book. Many of these tasks can be represented as a mapping from data points to outputs. This mapping or model is itself produced as the output of a machine learning algorithm applied to training data; there is a wide variety of models to choose from, as we shall see in Section 1.2. We start this chapter by discussing tasks, the problems that can be solved with machine learning. No matter what variety of machine learning models you may encounter, you will find that they are designed to solve one of only a small number of tasks and use only a few different types of features.\n",
            "\n",
            "Paper ID : 1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0 \tArticle : PANFIS: A Novel Incremental Learning Machine\n",
            "Author(s) : ['Mahardhika Pratama', 'S. Anavatti', 'P. Angelov', 'E. Lughofer']\n",
            "Year : 2014 \n",
            "Abstract : Most of the dynamics in real-world systems are compiled by shifts and drifts, which are uneasy to be overcome by omnipresent neuro-fuzzy systems. Nonetheless, learning in nonstationary environment entails a system owning high degree of flexibility capable of assembling its rule base autonomously according to the degree of nonlinearity contained in the system. In practice, the rule growing and pruning are carried out merely benefiting from a small snapshot of the complete training data to truncate the computational load and memory demand to the low level. An exposure of a novel algorithm, namely parsimonious network based on fuzzy inference system (PANFIS), is to this end presented herein. PANFIS can commence its learning process from scratch with an empty rule base. The fuzzy rules can be stitched up and expelled by virtue of statistical contributions of the fuzzy rules and injected datum afterward. Identical fuzzy sets may be alluded and blended to be one fuzzy set as a pursuit of a transparent rule base escalating human's interpretability. The learning and modeling performances of the proposed PANFIS are numerically validated using several benchmark problems from real-world or synthetic datasets. The validation includes comparisons with state-of-the-art evolving neuro-fuzzy methods and showcases that our new method can compete and in some cases even outperform these approaches in terms of predictive fidelity and model complexity.\n",
            "\n",
            "Paper ID : 0d41dd8c5a1a1d78575bd4f4ca5d7af3d471839a \tArticle : Multiagent Systems: A Survey from a Machine Learning Perspective\n",
            "Author(s) : ['P. Stone', 'M. Veloso']\n",
            "Year : 2000 \n",
            "Abstract : Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.\n",
            "\n",
            "Paper ID : 23bd1da0ad4d2ec956346655f0bb0206e13556b8 \tArticle : Learning to Control a Brain–Machine Interface for Reaching and Grasping by Primates\n",
            "Author(s) : ['J. Carmena', 'M. Lebedev', 'R. Crist', 'J. E. O’Doherty', 'David M. Santucci', 'D. Dimitrov', 'P. Patil', 'C. Henriquez', 'M. Nicolelis']\n",
            "Year : 2003 \n",
            "Abstract : Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain–machine interface (BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations.\n",
            "\n",
            "Paper ID : 4f6487d61ba6c2afa44be0e870599bb292e27638 \tArticle : Uncovering social spammers: social honeypots + machine learning\n",
            "Author(s) : ['Kyumin Lee', 'James Caverlee', 'Steve Webb']\n",
            "Year : 2010 \n",
            "Abstract : Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.\n",
            "\n",
            "Paper ID : bb111921d2020af454ea492c97a663cbe724ce40 \tArticle : Machine learning in side-channel analysis: a first study\n",
            "Author(s) : ['Gabriel Hospodar', 'Benedikt Gierlichs', 'E. D. Mulder', 'I. Verbauwhede', 'J. Vandewalle']\n",
            "Year : 2011 \n",
            "Abstract : Electronic devices may undergo attacks going beyond traditional cryptanalysis. Side-channel analysis (SCA) is an alternative attack that exploits information leaking from physical implementations of e.g. cryptographic devices to discover cryptographic keys or other secrets. This work comprehensively investigates the application of a machine learning technique in SCA. The considered technique is a powerful kernel-based learning algorithm: the Least Squares Support Vector Machine (LS-SVM). The chosen side-channel is the power consumption and the target is a software implementation of the Advanced Encryption Standard. In this study, the LS-SVM technique is compared to Template Attacks. The results show that the choice of parameters of the machine learning technique strongly impacts the performance of the classification. In contrast, the number of power traces and time instants does not influence the results in the same proportion. This effect can be attributed to the usage of data sets with straightforward Hamming weight leakages in this first study.\n",
            "------------------------------------Extracting Page #47------------------------------------\n",
            "\n",
            "Paper ID : e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da \tArticle : Adaptive Federated Learning in Resource Constrained Edge Computing Systems\n",
            "Author(s) : ['Shiqiang Wang', 'Tiffany Tuor', 'Theodoros Salonidis', 'K. Leung', 'C. Makaya', 'T. He', 'K. Chan']\n",
            "Year : 2019 \n",
            "Abstract : Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.\n",
            "\n",
            "Paper ID : 3021b6dec80e7032fd995c0dcadf4c992b7d7506 \tArticle : A survey on semi-supervised learning\n",
            "Author(s) : ['Jesper E. van Engelen', 'H. Hoos']\n",
            "Year : 2019 \n",
            "Abstract : Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.\n",
            "\n",
            "Paper ID : 0dfcec3139b2b52a3b6a144f323f89dd37de1fa4 \tArticle : Supervised learning with quantum-enhanced feature spaces\n",
            "Author(s) : ['Vojtěch Havlíček', 'A. Córcoles', 'K. Temme', 'A. Harrow', 'A. Kandala', 'J. Chow', 'J. Gambetta']\n",
            "Year : 2019 \n",
            "Abstract : Machine learning and quantum computing are two technologies that each have the potential to alter how computation is performed to address previously untenable problems. Kernel methods for machine learning are ubiquitous in pattern recognition, with support vector machines (SVMs) being the best known method for classification problems. However, there are limitations to the successful solution to such classification problems when the feature space becomes large, and the kernel functions become computationally expensive to estimate. A core element in the computational speed-ups enabled by quantum algorithms is the exploitation of an exponentially large quantum state space through controllable entanglement and interference. Here we propose and experimentally implement two quantum algorithms on a superconducting processor. A key component in both methods is the use of the quantum state space as feature space. The use of a quantum-enhanced feature space that is only efficiently accessible on a quantum computer provides a possible path to quantum advantage. The algorithms solve a problem of supervised learning: the construction of a classifier. One method, the quantum variational classifier, uses a variational quantum circuit1,2 to classify the data in a way similar to the method of conventional SVMs. The other method, a quantum kernel estimator, estimates the kernel function on the quantum computer and optimizes a classical SVM. The two methods provide tools for exploring the applications of noisy intermediate-scale quantum computers3 to machine learning.Two classification algorithms that use the quantum state space to produce feature maps are demonstrated on a superconducting processor, enabling the solution of problems when the feature space is large and the kernel functions are computationally expensive to estimate.\n",
            "\n",
            "Paper ID : 3bff76c25f7c416834655ba664553b14eb67a11c \tArticle : Sparse Bayesian Learning and the Relevance Vector Machine\n",
            "Author(s) : ['Michael E. Tipping']\n",
            "Year : 2001 \n",
            "Abstract : This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.\n",
            "\n",
            "Paper ID : 0ecf8c56300c20622f317e1e6cefdeeb85c513e2 \tArticle : Improving propensity score weighting using machine learning\n",
            "Author(s) : ['Brian K. Lee', 'J. Lessler', 'E. Stuart']\n",
            "Year : 2010 \n",
            "Abstract : Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART‐based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non‐linear and non‐additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non‐linearity or non‐additivity alone. However, under conditions of both moderate non‐additivity and moderate non‐linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright © 2009 John Wiley & Sons, Ltd.\n",
            "\n",
            "Paper ID : 642c1b4a9da95ea4239708afc5929a5007a1870d \tArticle : Tensor2Tensor for Neural Machine Translation\n",
            "Author(s) : ['Ashish Vaswani', 'Samy Bengio', 'E. Brevdo', 'François Chollet', 'Aidan N. Gomez', 'Stephan Gouws', 'Llion Jones', 'Lukasz Kaiser', 'Nal Kalchbrenner', 'Niki Parmar', 'Ryan Sepassi', 'Noam M. Shazeer', 'Jakob Uszkoreit']\n",
            "Year : 2018 \n",
            "Abstract : Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.\n",
            "\n",
            "Paper ID : 7fcb90f68529cbfab49f471b54719ded7528d0ef \tArticle : Federated Learning: Strategies for Improving Communication Efficiency\n",
            "Author(s) : ['Jakub Konecný', 'H. B. McMahan', 'Felix X. Yu', 'Peter Richtárik', 'A. Suresh', 'D. Bacon']\n",
            "Year : 2016 \n",
            "Abstract : Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.\n",
            "\n",
            "Paper ID : 5ff135450ae462855df27f44ad76872de5f1d1ea \tArticle : Machine Learning for the Detection of Oil Spills in Satellite Radar Images\n",
            "Author(s) : ['M. Kubát', 'R. Holte', 'S. Matwin']\n",
            "Year : 2004 \n",
            "Abstract : During a project examining the use of machine learning techniques for oil spill detection, we encountered several essential questions that we believe deserve the attention of the research community. We use our particular case study to illustrate such issues as problem formulation, selection of evaluation measures, and data preparation. We relate these issues to properties of the oil spill application, such as its imbalanced class distribution, that are shown to be common to many applications. Our solutions to these issues are implemented in the Canadian Environmental Hazards Detection System (CEHDS), which is about to undergo field testing.\n",
            "\n",
            "Paper ID : f466157848d1a7772fb6d02cdac9a7a5e7ef982e \tArticle : Neural Discrete Representation Learning\n",
            "Author(s) : ['Aäron van den Oord', 'Oriol Vinyals', 'K. Kavukcuoglu']\n",
            "Year : 2017 \n",
            "Abstract : Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.\n",
            "\n",
            "Paper ID : 63861fbeb7ec41986b85965b9780b428d919919e \tArticle : Support vector machine active learning for image retrieval\n",
            "Author(s) : ['Simon Tong', 'E. Chang']\n",
            "Year : 2001 \n",
            "Abstract : Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.\n",
            "------------------------------------Extracting Page #48------------------------------------\n",
            "\n",
            "Paper ID : 78989616eeeac55b202e3e4205225e7135054185 \tArticle : An Introduction to Deep Learning for the Physical Layer\n",
            "Author(s) : [\"Tim O'Shea\", 'J. Hoydis']\n",
            "Year : 2017 \n",
            "Abstract : We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.\n",
            "\n",
            "Paper ID : 57e4afe9ca74414fa02f2e0a929b64dc9a03334d \tArticle : Application of Machine Learning To Epileptic Seizure Detection\n",
            "Author(s) : ['A. Shoeb', 'J. Guttag']\n",
            "Year : 2010 \n",
            "Abstract : We present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp EEG, a non-invasive measure of the brain's electrical activity. This problem is challenging because the brain's electrical activity is composed of numerous classes with overlapping characteristics. The key steps involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework, and identifying the features critical to separating seizure from other types of brain activity. When trained on 2 or more seizures per patient and tested on 916 hours of continuous EEG from 24 patients, our algorithm detected 96% of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. We also provide information about how to download the CHB-MIT database, which contains the data used in this study.\n",
            "\n",
            "Paper ID : 6d431f835c06afdea45dff6b24486bf301ebdef0 \tArticle : An Overview of Multi-Task Learning in Deep Neural Networks\n",
            "Author(s) : ['Sebastian Ruder']\n",
            "Year : 2017 \n",
            "Abstract : Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.\n",
            "\n",
            "Paper ID : ca011427853d34ce4ec9ccafde8a70c9eacc3e21 \tArticle : Deep Learning for Computer Vision: A Brief Review\n",
            "Author(s) : ['A. Voulodimos', 'N. Doulamis', 'A. Doulamis', 'E. Protopapadakis']\n",
            "Year : 2018 \n",
            "Abstract : Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.\n",
            "\n",
            "Paper ID : 818826f356444f3daa3447755bf63f171f39ec47 \tArticle : Active Learning Literature Survey\n",
            "Author(s) : ['Burr Settles']\n",
            "Year : 2009 \n",
            "Abstract : The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.\n",
            "\n",
            "Paper ID : 2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98 \tArticle : Deep learning for sentiment analysis: A survey\n",
            "Author(s) : ['Lei Zhang', 'Shuai Wang', 'B. Liu']\n",
            "Year : 2018 \n",
            "Abstract : Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state‐of‐the‐art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.\n",
            "\n",
            "Paper ID : 77cf2d8a174c5a9a6b41c44203695c1d7f83f391 \tArticle : Machine learning for medical diagnosis: history, state of the art and perspective\n",
            "Author(s) : ['I. Kononenko']\n",
            "Year : 2001 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 4f2e84f1c5ea7a0d5d8ebfa12a78a869f13d7b59 \tArticle : Deep Learning, Neural Networks\n",
            "Author(s) : ['I. Dinov']\n",
            "Year : 2018 \n",
            "Abstract : Deep learning is a special branch of machine learning using a collage of algorithms to model high-level data motifs. Deep learning resembles the biological communications of systems of brain neurons in the central nervous system (CNS), where synthetic graphs represent the CNS network as nodes/states and connections/edges between them. For instance, in a simple synthetic network consisting of a pair of connected nodes, an output sent by one node is received by the other as an input signal. When more nodes are present in the network, they may be arranged in multiple levels (like a multiscale object) where the ith layer output serves as the input of the next (i + 1)st layer. The signal is manipulated at each layer, sent as a layer output downstream, interpreted as an input to the next, (i + 1)st layer, and so forth. Deep learning relies on multipler layers of nodes and many edges linking the nodes forming input/output (I/O) layered grids representing a multiscale processing network. At each layer, linear and non-linear transformations are converting inputs into outputs.\n",
            "\n",
            "Paper ID : 776ec90f563cbd387a3c69032e15ea92a516ac0c \tArticle : Deep learning with coherent nanophotonic circuits\n",
            "Author(s) : ['Yichen Shen', 'N. Harris', 'D. Englund', 'M. Soljačić']\n",
            "Year : 2017 \n",
            "Abstract : Artificial Neural Networks have dramatically improved performance for many machine learning tasks. We demonstrate a new architecture for a fully optical neural network that enables a computational speed enhancement of at least two orders of magnitude and three orders of magnitude in power efficiency over state-of-the-art electronics.\n",
            "\n",
            "Paper ID : d64a0840bea331caf9b1a61611b8eb8d15ec4f9f \tArticle : Consumer Credit Risk Models Via Machine-Learning Algorithms\n",
            "Author(s) : ['A. Khandani', 'A. Kim', 'A. Lo']\n",
            "Year : 2010 \n",
            "Abstract : We apply machine-learning techniques to construct nonlinear nonparametric forecasting models of consumer credit risk. By combining customer transactions and credit bureau data from January 2005 to April 2009 for a sample of a major commercial bank's customers, we are able to construct out-of-sample forecasts that significantly improve the classification rates of credit-card-holder delinquencies and defaults, with linear regression R2's of forecasted/realized delinquencies of 85%. Using conservative assumptions for the costs and benefits of cutting credit lines based on machine-learning forecasts, we estimate the cost savings to range from 6% to 25% of total losses. Moreover, the time-series patterns of estimated delinquency rates from this model over the course of the recent financial crisis suggest that aggregated consumer credit-risk analytics may have important applications in forecasting systemic risk.\n",
            "------------------------------------Extracting Page #49------------------------------------\n",
            "\n",
            "Paper ID : 818826f356444f3daa3447755bf63f171f39ec47 \tArticle : Active Learning Literature Survey\n",
            "Author(s) : ['Burr Settles']\n",
            "Year : 2009 \n",
            "Abstract : The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.\n",
            "\n",
            "Paper ID : 2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98 \tArticle : Deep learning for sentiment analysis: A survey\n",
            "Author(s) : ['Lei Zhang', 'Shuai Wang', 'B. Liu']\n",
            "Year : 2018 \n",
            "Abstract : Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state‐of‐the‐art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.\n",
            "\n",
            "Paper ID : 77cf2d8a174c5a9a6b41c44203695c1d7f83f391 \tArticle : Machine learning for medical diagnosis: history, state of the art and perspective\n",
            "Author(s) : ['I. Kononenko']\n",
            "Year : 2001 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 776ec90f563cbd387a3c69032e15ea92a516ac0c \tArticle : Deep learning with coherent nanophotonic circuits\n",
            "Author(s) : ['Yichen Shen', 'N. Harris', 'D. Englund', 'M. Soljačić']\n",
            "Year : 2017 \n",
            "Abstract : Artificial Neural Networks have dramatically improved performance for many machine learning tasks. We demonstrate a new architecture for a fully optical neural network that enables a computational speed enhancement of at least two orders of magnitude and three orders of magnitude in power efficiency over state-of-the-art electronics.\n",
            "\n",
            "Paper ID : d64a0840bea331caf9b1a61611b8eb8d15ec4f9f \tArticle : Consumer Credit Risk Models Via Machine-Learning Algorithms\n",
            "Author(s) : ['A. Khandani', 'A. Kim', 'A. Lo']\n",
            "Year : 2010 \n",
            "Abstract : We apply machine-learning techniques to construct nonlinear nonparametric forecasting models of consumer credit risk. By combining customer transactions and credit bureau data from January 2005 to April 2009 for a sample of a major commercial bank's customers, we are able to construct out-of-sample forecasts that significantly improve the classification rates of credit-card-holder delinquencies and defaults, with linear regression R2's of forecasted/realized delinquencies of 85%. Using conservative assumptions for the costs and benefits of cutting credit lines based on machine-learning forecasts, we estimate the cost savings to range from 6% to 25% of total losses. Moreover, the time-series patterns of estimated delinquency rates from this model over the course of the recent financial crisis suggest that aggregated consumer credit-risk analytics may have important applications in forecasting systemic risk.\n",
            "\n",
            "Paper ID : b87c0cf95208caacb025bf87d9ba451a87aacaca \tArticle : Machine Health Monitoring Using Local Feature-Based Gated Recurrent Unit Networks\n",
            "Author(s) : ['Rui Zhao', 'Dongzhen Wang', 'Ruqiang Yan', 'K. Mao', 'Fei Shen', 'Jinjiang Wang']\n",
            "Year : 2018 \n",
            "Abstract : In modern industries, machine health monitoring systems (MHMS) have been applied wildly with the goal of realizing predictive maintenance including failures tracking, downtime reduction, and assets preservation. In the era of big machinery data, data-driven MHMS have achieved remarkable results in the detection of faults after the occurrence of certain failures (diagnosis) and prediction of the future working conditions and the remaining useful life (prognosis). The numerical representation for raw sensory data is the key stone for various successful MHMS. Conventional methods are the labor-extensive as they usually depend on handcrafted features, which require expert knowledge. Inspired by the success of deep learning methods that redefine representation learning from raw data, we propose local feature-based gated recurrent unit (LFGRU) networks. It is a hybrid approach that combines handcrafted feature design with automatic feature learning for machine health monitoring. First, features from windows of input time series are extracted. Then, an enhanced bidirectional GRU network is designed and applied on the generated sequence of local features to learn the representation. A supervised learning layer is finally trained to predict machine condition. Experiments on three machine health monitoring tasks: tool wear prediction, gearbox fault diagnosis, and incipient bearing fault detection verify the effectiveness and generalization of the proposed LFGRU.\n",
            "\n",
            "Paper ID : 5b350da546b6e436229d45fd043c1342a620b693 \tArticle : Voting based extreme learning machine\n",
            "Author(s) : ['Jiuwen Cao', 'Zhiping Lin', 'G. Huang', 'Nan Liu']\n",
            "Year : 2012 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 475eb84d8e056aeec4b994e94df2be50fa38dfbc \tArticle : GraphLab: A New Parallel Framework for Machine Learning\n",
            "Author(s) : ['Y. Low', 'Joseph Gonzalez', 'Aapo Kyrola', 'D. Bickson', 'Carlos Guestrin', 'J. Hellerstein']\n",
            "Year : 2010 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : f7eb3e6221dc7bbe5eda0ce796755f5d71e7d61a \tArticle : Machine learning in adversarial environments\n",
            "Author(s) : ['P. Laskov', 'R. Lippmann']\n",
            "Year : 2010 \n",
            "Abstract : Whenever machine learning is used to prevent illegal or unsanctioned activity and there is an economic incentive, adversaries will attempt to circumvent the protection provided. Constraints on how adversaries can manipulate training and test data for classifiers used to detect suspicious behavior make problems in this area tractable and interesting. This special issue highlights papers that span many disciplines including email spam detection, computer intrusion detection, and detection of web pages deliberately designed to manipulate the priorities of pages returned by modern search engines. The four papers in this special issue provide a standard taxonomy of the types of attacks that can be expected in an adversarial framework, demonstrate how to design classifiers that are robust to deleted or corrupted features, demonstrate the ability of modern polymorphic engines to rewrite malware so it evades detection by current intrusion detection and antivirus systems, and provide approaches to detect web pages designed to manipulate web page scores returned by search engines. We hope that these papers and this special issue encourages the multidisciplinary cooperation required to address many interesting problems in this relatively new area including predicting the future of the arms races created by adversarial learning, developing effective long-term defensive strategies, and creating algorithms that can process the massive amounts of training and test data available for internet-scale problems.\n",
            "\n",
            "Paper ID : 5176a2f31dace77db9135dde7020d2c37f78cca0 \tArticle : The Elements of Statistical Learning: Data Mining, Inference, and Prediction\n",
            "Author(s) : ['D. Ruppert']\n",
            "Year : 2004 \n",
            "Abstract : In the words of the authors, the goal of this book was to “bring together many of the important new ideas in learning, and explain them in a statistical framework.” The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse  elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related  elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a “data modeling culture,” where the model is paramount. Breiman argued instead for an “algorithmic modeling culture,” with emphasis on black-box types of prediction. Breiman’s article is controversial, and in his discussion, Efron objects that “prediction is certainly an interesting subject, but Leo’s paper overstates both its role and our profession’s lack of interest in it.” Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a  nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called “market basket analysis.” The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book’s usefulness as a reference, and so I am happier with the book as it was written.\n",
            "------------------------------------Extracting Page #50------------------------------------\n",
            "\n",
            "Paper ID : e6cb9e80d1894bbb01882523137145d81dfb0a3c \tArticle : The SHOGUN Machine Learning Toolbox\n",
            "Author(s) : ['S. Sonnenburg', 'G. Rätsch', 'S. Henschel', 'Christian Widmer', 'Jonas Behr', 'A. Zien', 'F. D. Bona', 'Alexander Binder', 'C. Gehl', 'Vojtech Franc']\n",
            "Year : 2010 \n",
            "Abstract : We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hidden Markov models, multiple kernel learning, linear discriminant analysis, and more. Most of the specific algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. \n",
            " \n",
            "SHOGUN is implemented in C++ and interfaces to MATLABTM, R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org.\n",
            "\n",
            "Paper ID : 395dd01c0d24777c660cf195c4cfadcdf51fb7e8 \tArticle : Learning to learn by gradient descent by gradient descent\n",
            "Author(s) : ['Marcin Andrychowicz', 'Misha Denil', 'Sergio Gomez Colmenarejo', 'Matthew W. Hoffman', 'D. Pfau', 'T. Schaul', 'N. D. Freitas']\n",
            "Year : 2016 \n",
            "Abstract : The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.\n",
            "\n",
            "Paper ID : 48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852 \tArticle : Deep learning for cellular image analysis\n",
            "Author(s) : ['Erick Moen', 'Dylan Bannon', 'Takamasa Kudo', 'William Graf', 'M. Covert', 'David Van Valen']\n",
            "Year : 2019 \n",
            "Abstract : Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field’s progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs’ experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.A Review on applications of deep machine learning in image analysis that offers practical guidance for biologists.\n",
            "\n",
            "Paper ID : cd49acefc8d51e324aa562e5337e1c2aff067053 \tArticle : An Overview of Multi-task Learning\n",
            "Author(s) : ['Yu Zhang', 'Qiang Yang']\n",
            "Year : 2018 \n",
            "Abstract : As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.\n",
            "\n",
            "Paper ID : 7ab0f0da686cd4094fd96f5a30e0b6072525fd09 \tArticle : Deep Learning in Medical Image Analysis.\n",
            "Author(s) : ['D. Shen', 'Guorong Wu', 'Heung-Il Suk']\n",
            "Year : 2017 \n",
            "Abstract : This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.\n",
            "\n",
            "Paper ID : 5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53 \tArticle : An Introduction to Support Vector Machines and Other Kernel-based Learning Methods\n",
            "Author(s) : ['N. Cristianini', 'J. Shawe-Taylor']\n",
            "Year : 2000 \n",
            "Abstract : From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.\n",
            "\n",
            "Paper ID : cb8a1b8d87a3fef15635eb4a32173f9c6f966055 \tArticle : A Survey on Deep Learning\n",
            "Author(s) : ['Samira Pouyanfar', 'Saad Sadiq', 'Yilin Yan', 'Haiman Tian', 'Yudong Tao', 'Maria E. Presa-Reyes', 'M. Shyu', 'Shu‐Ching Chen', 'S. S. Iyengar']\n",
            "Year : 2019 \n",
            "Abstract : The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.\n",
            "\n",
            "Paper ID : 38aff6df1accc456f6cda7d16d4b9ecf418ef21e \tArticle : Map-Reduce for Machine Learning on Multicore\n",
            "Author(s) : ['Cheng-Tao Chu', 'Sang Kyun Kim', 'Yi-An Lin', 'YuanYuan Yu', 'G. Bradski', 'A. Ng', 'K. Olukotun']\n",
            "Year : 2006 \n",
            "Abstract : We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.\n",
            "\n",
            "Paper ID : b57e6468740d9320f3f14c6079168b8e21366416 \tArticle : The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches\n",
            "Author(s) : ['Md. Zahangir Alom', 'T. Taha', 'C. Yakopcic', 'Stefan Westberg', 'P. Sidike', 'M. S. Nasrin', 'B. V. Essen', 'A. Awwal', 'V. Asari']\n",
            "Year : 2018 \n",
            "Abstract : Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].\n",
            "\n",
            "Paper ID : dc0c84b7c5e6521216da789f8171544709120cf0 \tArticle : Opportunities and obstacles for deep learning in biology and medicine\n",
            "Author(s) : ['T. Ching', 'Daniel S. Himmelstein', 'Brett K. Beaulieu-Jones', 'Alexandr A Kalinin', 'Brian T Do', 'Gregory P. Way', 'E. Ferrero', 'P. Agapow', 'M. Zietz', 'M. M. Hoffman', 'W. Xie', 'G. Rosen', 'Benjamin J. Lengerich', 'Johnny Israeli', 'Jack Lanchantin', 'S. Woloszynek', 'Anne E Carpenter', 'Avanti Shrikumar', 'Jinbo Xu', 'Evan M. Cofer', 'C. A. Lavender', 'Srinivas C. Turaga', 'Amr M. Alexandari', 'Zhiyong Lu', 'David J Harris', 'David DeCaprio', 'Yanjun Qi', 'A. Kundaje', 'Yifan Peng', 'L. K. Wiley', 'Marwin H. S. Segler', 'S. Boca', 'S. Joshua Joshua Swamidass', 'Austin Huang', 'A. Gitter', 'C. Greene']\n",
            "Year : 2017 \n",
            "Abstract : Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.\n",
            "------------------------------------Extracting Page #51------------------------------------\n",
            "\n",
            "Paper ID : cb8a1b8d87a3fef15635eb4a32173f9c6f966055 \tArticle : A Survey on Deep Learning\n",
            "Author(s) : ['Samira Pouyanfar', 'Saad Sadiq', 'Yilin Yan', 'Haiman Tian', 'Yudong Tao', 'Maria E. Presa-Reyes', 'M. Shyu', 'Shu‐Ching Chen', 'S. S. Iyengar']\n",
            "Year : 2019 \n",
            "Abstract : The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.\n",
            "\n",
            "Paper ID : 38aff6df1accc456f6cda7d16d4b9ecf418ef21e \tArticle : Map-Reduce for Machine Learning on Multicore\n",
            "Author(s) : ['Cheng-Tao Chu', 'Sang Kyun Kim', 'Yi-An Lin', 'YuanYuan Yu', 'G. Bradski', 'A. Ng', 'K. Olukotun']\n",
            "Year : 2006 \n",
            "Abstract : We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.\n",
            "\n",
            "Paper ID : b57e6468740d9320f3f14c6079168b8e21366416 \tArticle : The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches\n",
            "Author(s) : ['Md. Zahangir Alom', 'T. Taha', 'C. Yakopcic', 'Stefan Westberg', 'P. Sidike', 'M. S. Nasrin', 'B. V. Essen', 'A. Awwal', 'V. Asari']\n",
            "Year : 2018 \n",
            "Abstract : Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].\n",
            "\n",
            "Paper ID : dc0c84b7c5e6521216da789f8171544709120cf0 \tArticle : Opportunities and obstacles for deep learning in biology and medicine\n",
            "Author(s) : ['T. Ching', 'Daniel S. Himmelstein', 'Brett K. Beaulieu-Jones', 'Alexandr A Kalinin', 'Brian T Do', 'Gregory P. Way', 'E. Ferrero', 'P. Agapow', 'M. Zietz', 'M. M. Hoffman', 'W. Xie', 'G. Rosen', 'Benjamin J. Lengerich', 'Johnny Israeli', 'Jack Lanchantin', 'S. Woloszynek', 'Anne E Carpenter', 'Avanti Shrikumar', 'Jinbo Xu', 'Evan M. Cofer', 'C. A. Lavender', 'Srinivas C. Turaga', 'Amr M. Alexandari', 'Zhiyong Lu', 'David J Harris', 'David DeCaprio', 'Yanjun Qi', 'A. Kundaje', 'Yifan Peng', 'L. K. Wiley', 'Marwin H. S. Segler', 'S. Boca', 'S. Joshua Joshua Swamidass', 'Austin Huang', 'A. Gitter', 'C. Greene']\n",
            "Year : 2017 \n",
            "Abstract : Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.\n",
            "\n",
            "Paper ID : 4b61c25a86083c20730c9b12737ac6ac4178c364 \tArticle : An Introduction to Deep Reinforcement Learning\n",
            "Author(s) : ['Vincent François-Lavet', 'Peter Henderson', 'Riashat Islam', 'Marc G. Bellemare', 'Joelle Pineau']\n",
            "Year : 2018 \n",
            "Abstract : Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.\n",
            "\n",
            "Paper ID : 293987e14d64dc768a432115c93171ab8653e3bb \tArticle : Machine Learning in Medical Imaging\n",
            "Author(s) : ['M. Wernick', 'Yongyi Yang', 'J. Brankov', 'G. Yourganov', 'S. Strother']\n",
            "Year : 2010 \n",
            "Abstract : This article will discuss very different ways of using machine learning that may be less familiar, and we will demonstrate through examples the role of these concepts in medical imaging. Although the term machine learning is relatively recent, the ideas of machine learning have been applied to medical imaging for decades, perhaps most notably in the areas of computer-aided diagnosis (CAD) and functional brain mapping. We will not attempt in this brief article to survey the rich literature of this field. Instead our goals will be 1) to acquaint the reader with some modern techniques that are now staples of the machine-learning field and 2) to illustrate how these techniques can be employed in various ways in medical imaging.\n",
            "\n",
            "Paper ID : 7e7b9f37ce280787075046727efbaf9b5a390729 \tArticle : Applications of Machine Learning in Cancer Prediction and Prognosis\n",
            "Author(s) : ['Joseph A. Cruz', 'D. Wishart']\n",
            "Year : 2007 \n",
            "Abstract : Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling this review we conducted a broad survey of the different types of machine learning methods being used, the types of data being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted, including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more recently developed or more easily interpretable machine learning methods. A number of published studies also appear to lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine learning methods can be used to substantially (15–25%) improve the accuracy of predicting cancer susceptibility, recurrence and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic understanding of cancer development and progression.\n",
            "\n",
            "Paper ID : bee570503aaa0ed5bc5dd4cf6aa742df0b5cef87 \tArticle : The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition\n",
            "Author(s) : ['T. Hastie', 'R. Tibshirani', 'J. Friedman']\n",
            "Year : 2001 \n",
            "Abstract : During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book.\n",
            "\n",
            "This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates.\n",
            "\n",
            "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
            "\n",
            "Paper ID : da5c65b0ac8b525c3d3d4889bf44d8a48d254a07 \tArticle : Deep Bayesian Active Learning with Image Data\n",
            "Author(s) : ['Y. Gal', 'Riashat Islam', 'Zoubin Ghahramani']\n",
            "Year : 2017 \n",
            "Abstract : Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).\n",
            "\n",
            "Paper ID : 36dd3331060e5e6157d9558563b95253308709cb \tArticle : Machine learning: a review of classification and combining techniques\n",
            "Author(s) : ['S. Kotsiantis', 'I. Zaharakis', 'P. Pintelas']\n",
            "Year : 2007 \n",
            "Abstract : Supervised classification is one of the tasks most frequently carried out by so-called Intelligent Systems. Thus, a large number of techniques have been developed based on Artificial Intelligence (Logic-based techniques, Perceptron-based techniques) and Statistics (Bayesian Networks, Instance-based techniques). The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various classification algorithms and the recent attempt for improving classification accuracy—ensembles of classifiers.\n",
            "------------------------------------Extracting Page #52------------------------------------\n",
            "\n",
            "Paper ID : de9180ecc8b645aff238be05f645d50fa34a808f \tArticle : Learning Curves in Machine Learning\n",
            "Author(s) : ['Claudia Perlich']\n",
            "Year : 2010 \n",
            "Abstract : LIMITED DISTRIBUTION NOTICE: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g. , payment of royalties). Copies may be requested from IBM T. J. Definition A learning curve shows a measure of predictive performance on a given domain as a function of some measure of varying amounts of learning effort. The most common form of learning curves in the general field of machine learning shows predictive accuracy on the test examples as a function of the number of training examples as in Figure 1. Figure 1: Stylized learning curve showing the model accuracy on test examples as function of the number of training examples.\n",
            "\n",
            "Paper ID : 0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5 \tArticle : A Review on Multi-Label Learning Algorithms\n",
            "Author(s) : ['Min-Ling Zhang', 'Zhi-Hua Zhou']\n",
            "Year : 2014 \n",
            "Abstract : Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.\n",
            "\n",
            "Paper ID : c4ae802491724aee021f31f02327b9671cead3dc \tArticle : Types of Machine Learning Algorithms\n",
            "Author(s) : ['T. Ayodele']\n",
            "Year : 2010 \n",
            "Abstract : • Supervised learning --where the algorithm generates a function that maps inputs to desired outputs. One standard formulation of the supervised learning task is the classification problem: the learner is required to learn (to approximate the behavior of) a function which maps a vector into one of several classes by looking at several input-output examples of the function. • Unsupervised learning --which models a set of inputs: labeled examples are not available. • Semi-supervised learning --which combines both labeled and unlabeled examples to generate an appropriate function or classifier. • Reinforcement learning --where the algorithm learns a policy of how to act given an observation of the world. Every action has some impact in the environment, and the environment provides feedback that guides the learning algorithm. • Transduction --similar to supervised learning, but does not explicitly construct a function: instead, tries to predict new outputs based on training inputs, training outputs, and new inputs. • Learning to learn --where the algorithm learns its own inductive bias based on previous experience.\n",
            "\n",
            "Paper ID : 60f43e763b370af0028317d7f6d94885cdfe390a \tArticle : Federated Learning\n",
            "Author(s) : ['Qiang Yang', 'Yang Liu', 'Yong Cheng', 'Yan Kang', 'Tianjian Chen', 'Han Yu']\n",
            "Year : 2019 \n",
            "Abstract : How is it possible to allow multiple data owners to collaboratively train and use a shared prediction model while keeping all the local training data private? Traditional machine learning approaches need to combine all data at one location, typically a data center, which may very well violate the laws on user privacy and data confidentiality. Today, many parts of the world demand that technology companies treat user data carefully according to user-privacy laws. The European Union’s General Data Protection Regulation (GDPR) is a prime example. In this book, we describe how federated machine learning addresses this problem with novel solutions combining distributed machine learning, cryptography and security, and incentive mechanism design based on economic principles and game theory. We explain different types of privacypreserving machine learning solutions and their technological backgrounds, and highlight some representative practical use cases.We show how federated learning can become the foundation of next-generation machine learning that caters to technological and societal needs for responsible AI development and application.\n",
            "\n",
            "Paper ID : cfea40bf4e4131aab2727e3aaedcf02c1dd594ac \tArticle : Bidirectional Extreme Learning Machine for Regression Problem and Its Learning Effectiveness\n",
            "Author(s) : ['Yimin Yang', 'Yaonan Wang', 'Xiaofang Yuan']\n",
            "Year : 2012 \n",
            "Abstract : It is clear that the learning effectiveness and learning speed of neural networks are in general far slower than required, which has been a major bottleneck for many applications. Recently, a simple and efficient learning method, referred to as extreme learning machine (ELM), was proposed by Huang , which has shown that, compared to some conventional methods, the training time of neural networks can be reduced by a thousand times. However, one of the open problems in ELM research is whether the number of hidden nodes can be further reduced without affecting learning effectiveness. This brief proposes a new learning algorithm, called bidirectional extreme learning machine (B-ELM), in which some hidden nodes are not randomly selected. In theory, this algorithm tends to reduce network output error to 0 at an extremely early learning stage. Furthermore, we find a relationship between the network output error and the network output weights in the proposed B-ELM. Simulation results demonstrate that the proposed method can be tens to hundreds of times faster than other incremental ELM algorithms.\n",
            "\n",
            "Paper ID : 01f702f8b1f9d1314587015f1f038af4d5735e77 \tArticle : Opposition-Based Learning: A New Scheme for Machine Intelligence\n",
            "Author(s) : ['H. Tizhoosh']\n",
            "Year : 2005 \n",
            "Abstract : Opposition-based learning as a new scheme for machine intelligence is introduced. Estimates and counter-estimates, weights and opposite weights, and actions versus counter-actions are the foundation of this new approach. Examples are provided. Possibilities for extensions of existing learning algorithms are discussed. Preliminary results are provided\n",
            "\n",
            "Paper ID : be633c8eeffdba4499c6f53e002d663c868dc87c \tArticle : Support Vector Machine\n",
            "Author(s) : ['S. Suthaharan']\n",
            "Year : 2016 \n",
            "Abstract : Support Vector Machine is one of the classical machine learning techniques that can still help solve big data classification problems. Especially, it can help the multidomain applications in a big data environment. However, the support vector machine is mathematically complex and computationally expensive. The main objective of this chapter is to simplify this approach using process diagrams and data flow diagrams to help readers understand theory and implement it successfully. To achieve this objective, the chapter is divided into three parts: (1) modeling of a linear support vector machine; (2) modeling of a nonlinear support vector machine; and (3) Lagrangian support vector machine algorithm and its implementations. The Lagrangian support vector machine with simple examples is also implemented using the R programming platform on Hadoop and non-Hadoop systems.\n",
            "\n",
            "Paper ID : a20bfec3c95aad003dcb45a21a220c19cca8bb66 \tArticle : A Machine Learning Approach to Coreference Resolution of Noun Phrases\n",
            "Author(s) : ['W. M. Soon', 'H. Ng', 'Chung Yong Lim']\n",
            "Year : 2001 \n",
            "Abstract : In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of organization, person, or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.\n",
            "\n",
            "Paper ID : fd62a4d907ff9a98cd69926b7dd72cb980713715 \tArticle : Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources\n",
            "Author(s) : ['Xiaoxiang Zhu', 'D. Tuia', 'Lichao Mou', 'Gui-Song Xia', 'Liang-pei Zhang', 'Feng Xu', 'F. Fraundorfer']\n",
            "Year : 2017 \n",
            "Abstract : Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization.\n",
            "\n",
            "Paper ID : 8de174ab5419b9d3127695405efd079808e956e8 \tArticle : Curriculum learning\n",
            "Author(s) : ['Yoshua Bengio', 'J. Louradour', 'Ronan Collobert', 'J. Weston']\n",
            "Year : 2009 \n",
            "Abstract : Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).\n",
            "------------------------------------Extracting Page #53------------------------------------\n",
            "\n",
            "Paper ID : 9d3e0fce253a4ae4a4456b2f24c03329a2b74621 \tArticle : Deep Learning for Health Informatics\n",
            "Author(s) : ['D. Ravì', 'Charence Wong', 'F. Deligianni', 'M. Berthelot', 'Javier Andreu-Perez', 'Benny P. L. Lo', 'Guang-Zhong Yang']\n",
            "Year : 2017 \n",
            "Abstract : With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.\n",
            "\n",
            "Paper ID : bb35ef89addbbc28d960bc0cab70d8a29fdf6eee \tArticle : A Survey on Multi-Task Learning\n",
            "Author(s) : ['Yu Zhang', 'Qiang Yang']\n",
            "Year : 2017 \n",
            "Abstract : Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL. First, we classify different MTL algorithms into several categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach, and then discuss the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, batch MTL models are difficult to handle this situation and online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing are reviewed to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.\n",
            "\n",
            "Paper ID : b5ae39d8bf8a06ae91b236ef115fdab8921e6455 \tArticle : Optimization method based extreme learning machine for classification\n",
            "Author(s) : ['G. Huang', 'Xiaojian Ding', 'Hongming Zhou']\n",
            "Year : 2010 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 066dd3d09eb5815e8be1da560abd1abe08f87cb9 \tArticle : OP-ELM: Optimally Pruned Extreme Learning Machine\n",
            "Author(s) : ['Y. Miché', 'A. Sorjamaa', 'P. Bas', 'O. Simula', 'C. Jutten', 'A. Lendasse']\n",
            "Year : 2010 \n",
            "Abstract : In this brief, the optimally pruned extreme learning machine (OP-ELM) methodology is presented. It is based on the original extreme learning machine (ELM) algorithm with additional steps to make it more robust and generic. The whole methodology is presented in detail and then applied to several regression and classification problems. Results for both computational time and accuracy (mean square error) are compared to the original ELM and to three other widely used methodologies: multilayer perceptron (MLP), support vector machine (SVM), and Gaussian process (GP). As the experiments for both regression and classification illustrate, the proposed OP-ELM methodology performs several orders of magnitude faster than the other algorithms used in this brief, except the original ELM. Despite the simplicity and fast performance, the OP-ELM is still able to maintain an accuracy that is comparable to the performance of the SVM. A toolbox for the OP-ELM is publicly available online.\n",
            "\n",
            "Paper ID : c99179ca3784e3465fd9ed049d7f34b50d39393e \tArticle : Ensemble learning: A survey\n",
            "Author(s) : ['Omer Sagi', 'L. Rokach']\n",
            "Year : 2018 \n",
            "Abstract : Ensemble methods are considered the state‐of‐the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state‐of‐the‐art ensemble methods and discusses current challenges and trends in the field.\n",
            "\n",
            "Paper ID : 4d931ea98be69882f547ec6c1b42b78c3e13c36d \tArticle : Quantum circuit learning\n",
            "Author(s) : ['K. Mitarai', 'M. Negoro', 'M. Kitagawa', 'K. Fujii']\n",
            "Year : 2018 \n",
            "Abstract : We propose a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, which we call quantum circuit learning. A quantum circuit driven by our framework learns a given task by tuning parameters implemented on it. The iterative optimization of the parameters allows us to circumvent the high-depth circuit. Theoretical investigation shows that a quantum circuit can approximate nonlinear functions, which is further confirmed by numerical simulations. Hybridizing a low-depth quantum circuit and a classical computer for machine learning, the proposed framework paves the way toward applications of near-term quantum devices for quantum machine learning.\n",
            "\n",
            "Paper ID : 5bf9cebe3658cfbf7f67c0a2680c8233509aa5e4 \tArticle : UCI Repository of Machine Learning Database\n",
            "Author(s) : ['D. Newman']\n",
            "Year : 1998 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 276194e96ebd620b5cff35a9168bdda39a0be57b \tArticle : Federated Multi-Task Learning\n",
            "Author(s) : ['Virginia Smith', 'Chao-Kai Chiang', 'Maziar Sanjabi', 'Ameet S. Talwalkar']\n",
            "Year : 2017 \n",
            "Abstract : Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.\n",
            "\n",
            "Paper ID : 9f1e9e56d80146766bc2316efbc54d8b770a23df \tArticle : Deep Reinforcement Learning: An Overview\n",
            "Author(s) : ['Yuxi Li']\n",
            "Year : 2017 \n",
            "Abstract : We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. \n",
            "Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.\n",
            "\n",
            "Paper ID : 0ef9ae1ce8c91ce671a211bdda792bf3752d1522 \tArticle : A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks\n",
            "Author(s) : ['Chuanlong Yin', 'Yuefei Zhu', 'Jin-long Fei', 'Xin-Zheng He']\n",
            "Year : 2017 \n",
            "Abstract : Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.\n",
            "------------------------------------Extracting Page #54------------------------------------\n",
            "\n",
            "Paper ID : b17862e73e79a554e6ad2b27f6df44be1b533a0d \tArticle : Tuning Metaheuristics - A Machine Learning Perspective\n",
            "Author(s) : ['M. Birattari']\n",
            "Year : 2009 \n",
            "Abstract : The importance of tuning metaheuristics is widely acknowledged in scientific literature. However, there is very little dedicated research on the subject. Typically, scientists and practitioners tune metaheuristics by hand, guided only by their experience and by some rules of thumb. Tuning metaheuristics is often considered to be more of an art than a science. This book lays the foundations for a scientific approach to tuning metaheuristics. The fundamental intuition that underlies Birattari's approach is that the tuning problem has much in common with the problems that are typically faced in machine learning. By adopting a machine learning perspective, the author gives a formal definition of the tuning problem, develops a generic algorithm for tuning metaheuristics, and defines an appropriate experimental methodology for assessing the performance of metaheuristics.\n",
            "\n",
            "Paper ID : e7d4989aeb3aba1bca30035fc1da28a518f9de39 \tArticle : Open-source machine learning: R meets Weka\n",
            "Author(s) : ['K. Hornik', 'C. Buchta', 'A. Zeileis']\n",
            "Year : 2009 \n",
            "Abstract : Two of the prime open-source environments available for machine/statistical learning in data mining and knowledge discovery are the software packages Weka and R which have emerged from the machine learning and statistics communities, respectively. To make the different sets of tools from both environments available in a single unified system, an R package RWeka is suggested which interfaces Weka’s functionality to R. With only a thin layer of (mostly R) code, a set of general interface generators is provided which can set up interface functions with the usual “R look and feel”, re-using Weka’s standardized interface of learner classes (including classifiers, clusterers, associators, filters, loaders, savers, and stemmers) with associated methods.\n",
            "\n",
            "Paper ID : 12d1d070a53d4084d88a77b8b143bad51c40c38f \tArticle : Reinforcement Learning: A Survey\n",
            "Author(s) : ['L. Kaelbling', 'M. Littman', 'A. Moore']\n",
            "Year : 1996 \n",
            "Abstract : This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.\n",
            "\n",
            "Paper ID : 66405753664af1895830f147365cc5903388dea1 \tArticle : A review of machine learning approaches to Spam filtering\n",
            "Author(s) : ['Thiago S. Guzella', 'W. Caminhas']\n",
            "Year : 2009 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 7e910b031e703caf683d44d7417d702a5aba2e95 \tArticle : Handbook Of Research On Machine Learning Applications and Trends: Algorithms, Methods and Techniques (2 Volumes)\n",
            "Author(s) : ['E. S. Olivas', 'J. D. M. Guerrero', 'M. M. Sober', 'J. Benedito', 'A. López']\n",
            "Year : 2009 \n",
            "Abstract : The machine learning approach provides a useful tool when the amount of data is very large and a model is not available to explain the generation and relation of the data set. The Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques provides a set of practical applications for solving problems and applying various techniques in automatic data extraction and setting. A defining collection of field advancements, this Handbook of Research fills the gap between theory and practice, providing a strong reference for academicians, researchers, and practitioners.\n",
            "\n",
            "Paper ID : cc7827a17a7759a04aa389290d1a874db56e85e5 \tArticle : Meta-Learning: A Survey\n",
            "Author(s) : ['J. Vanschoren']\n",
            "Year : 2018 \n",
            "Abstract : Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.\n",
            "\n",
            "Paper ID : a191715b2e3e51cc08114e759b7bf7fdf9f2c206 \tArticle : Deep Learning with Python\n",
            "Author(s) : ['François Chollet']\n",
            "Year : 2017 \n",
            "Abstract : Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learninga combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Franois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.\n",
            "\n",
            "Paper ID : 4ea55038daf0907d27296de047008fb58def41c5 \tArticle : Applications of Deep Learning and Reinforcement Learning to Biological Data\n",
            "Author(s) : ['M. Mahmud', 'M. S. Kaiser', 'A. Hussain', 'S. Vassanelli']\n",
            "Year : 2018 \n",
            "Abstract : Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as omics, bioimaging, medical imaging, and (brain/body)–machine interfaces. These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.\n",
            "\n",
            "Paper ID : 90abbe1f2f78493b9a596ba13b7145a07e50df84 \tArticle : A survey of transfer learning\n",
            "Author(s) : ['K. R. Weiss', 'T. Khoshgoftaar', 'Dingding Wang']\n",
            "Year : 2016 \n",
            "Abstract : Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.\n",
            "\n",
            "Paper ID : efd68f3724942c9de5dc804d3c7cb3f70f42234b \tArticle : Deep learning for computational biology\n",
            "Author(s) : ['Christof Angermueller', 'Tanel Pärnamaa', 'L. Parts', 'O. Stegle']\n",
            "Year : 2016 \n",
            "Abstract : Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.\n",
            "------------------------------------Extracting Page #55------------------------------------\n",
            "\n",
            "Paper ID : efd68f3724942c9de5dc804d3c7cb3f70f42234b \tArticle : Deep learning for computational biology\n",
            "Author(s) : ['Christof Angermueller', 'Tanel Pärnamaa', 'L. Parts', 'O. Stegle']\n",
            "Year : 2016 \n",
            "Abstract : Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.\n",
            "\n",
            "Paper ID : 6241276a3074f73c1ce9b9ce4ac69f748732aecd \tArticle : Data Mining and Machine Learning in Astronomy\n",
            "Author(s) : ['N. Ball', 'Robert J. Brunner Herzberg Institute of Astrophysics', 'Victoria', 'Bc', 'Canada.', 'D. O. Astronomy', 'U. I. Urbana-Champaign']\n",
            "Year : 2009 \n",
            "Abstract : We review the current state of data mining and machine learning in astronomy. 'Data Mining' can have a somewhat mixed connotation from the point of view of a researcher in this field. If used correctly, it can be a powerful approach, holding the potential to fully exploit the exponentially increasing amount of available data, promising great scientific advance. However, if misused, it can be little more than the black-box application of complex computing algorithms that may give little physical insight, and provide questionable results. Here, we give an overview of the entire data mining process, from data collection through to the interpretation of results. We cover common machine learning algorithms, such as artificial neural networks and support vector machines, applications from a broad range of astronomy, emphasizing those where data mining techniques directly resulted in improved science, and important current and future directions, including probability density functions, parallel algorithms, petascale computing, and the time domain. We conclude that, so long as one carefully selects an appropriate algorithm, and is guided by the astronomical problem at hand, data mining can be very much the powerful tool, and not the questionable black box.\n",
            "\n",
            "Paper ID : 767dcaf77f73f958cfee0f54cfcde0882e8ec50e \tArticle : Deep Learning: A Practitioner's Approach\n",
            "Author(s) : ['Joshua Patterson', 'Adam Gibson']\n",
            "Year : 2017 \n",
            "Abstract : Although interest in machine learning has reached a high point, lofty expectations often scuttle projects before they get very far. How can machine learningespecially deep neural networksmake a real difference in your organization? This hands-on guide not only provides the most practical information available on the subject, but also helps you get started building efficient deep learning networks. Authors Adam Gibson and Josh Patterson provide theory on deep learning before introducing their open-source Deeplearning4j (DL4J) library for developing production-class workflows. Through real-world examples, youll learn methods and strategies for training deep network architectures and running deep learning workflows on Spark and Hadoop with DL4J. Dive into machine learning concepts in general, as well as deep learning in particular Understand how deep networks evolved from neural network fundamentals Explore the major deep network architectures, including Convolutional and Recurrent Learn how to map specific deep networks to the right problem Walk through the fundamentals of tuning general neural networks and specific deep network architectures Use vectorization techniques for different data types with DataVec, DL4Js workflow toolLearn how to use DL4J natively on Spark and Hadoop\n",
            "\n",
            "Paper ID : 1242d79573397094c5670f55e58c8333cced0beb \tArticle : Deep Learning: A Primer for Radiologists.\n",
            "Author(s) : ['G. Chartrand', 'P. Cheng', 'Eugene Vorontsov', 'M. Drozdzal', 'S. Turcotte', 'C. Pal', 'S. Kadoury', 'A. Tang']\n",
            "Year : 2017 \n",
            "Abstract : Deep learning is a class of machine learning methods that are gaining success and attracting interest in many domains, including computer vision, speech recognition, natural language processing, and playing games. Deep learning methods produce a mapping from raw inputs to desired outputs (eg, image classes). Unlike traditional machine learning methods, which require hand-engineered feature extraction from inputs, deep learning methods learn these features directly from data. With the advent of large datasets and increased computing power, these methods can produce models with exceptional performance. These models are multilayer artificial neural networks, loosely inspired by biologic neural systems. Weighted connections between nodes (neurons) in the network are iteratively adjusted based on example pairs of inputs and target outputs by back-propagating a corrective error signal through the network. For computer vision tasks, convolutional neural networks (CNNs) have proven to be effective. Recently, several clinical applications of CNNs have been proposed and studied in radiology for classification, detection, and segmentation tasks. This article reviews the key concepts of deep learning for clinical radiologists, discusses technical requirements, describes emerging applications in clinical radiology, and outlines limitations and future directions in this field. Radiologists should become familiar with the principles and potential applications of deep learning in medical imaging. ©RSNA, 2017.\n",
            "\n",
            "Paper ID : db64f424710d57025c5fb42a564551f093a4d111 \tArticle : The Extreme Value Machine\n",
            "Author(s) : ['Ethan M. Rudd', 'Lalit P. Jain', 'W. Scheirer', 'T. Boult']\n",
            "Year : 2018 \n",
            "Abstract : It is often desirable to be able to recognize when inputs to a recognition function learned in a supervised manner correspond to classes unseen at training time. With this ability, new class labels could be assigned to these inputs by a human operator, allowing them to be incorporated into the recognition function—ideally under an efficient incremental update mechanism. While good algorithms that assume inputs from a fixed set of classes exist, e.g. , artificial neural networks and kernel machines, it is not immediately obvious how to extend them to perform incremental learning in the presence of unknown query classes. Existing algorithms take little to no distributional information into account when learning recognition functions and lack a strong theoretical foundation. We address this gap by formulating a novel, theoretically sound classifier—the Extreme Value Machine (EVM). The EVM has a well-grounded interpretation derived from statistical Extreme Value Theory (EVT), and is the first classifier to be able to perform nonlinear kernel-free variable bandwidth incremental learning. Compared to other classifiers in the same deep network derived feature space, the EVM is accurate and efficient on an established benchmark partition of the ImageNet dataset.\n",
            "\n",
            "Paper ID : bcce96a2a074448953fc61a29a84afbdfc8db55a \tArticle : Online Learning and Online Convex Optimization\n",
            "Author(s) : ['S. Shalev-Shwartz']\n",
            "Year : 2012 \n",
            "Abstract : Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.\n",
            "\n",
            "Paper ID : 7c7f1bb3d483df4c4a20b78a91b833f32d6bdcc5 \tArticle : Machine learning in virtual screening.\n",
            "Author(s) : ['James L. Melville', 'E. Burke', 'J. Hirst']\n",
            "Year : 2009 \n",
            "Abstract : In this review, we highlight recent applications of machine learning to virtual screening, focusing on the use of supervised techniques to train statistical learning algorithms to prioritize databases of molecules as active against a particular protein target. Both ligand-based similarity searching and structure-based docking have benefited from machine learning algorithms, including naïve Bayesian classifiers, support vector machines, neural networks, and decision trees, as well as more traditional regression techniques. Effective application of these methodologies requires an appreciation of data preparation, validation, optimization, and search methodologies, and we also survey developments in these areas.\n",
            "\n",
            "Paper ID : 298af26244e3ad836c1aa5cf5855d05f5197063d \tArticle : Machine Learning Methods Without Tears: A Primer for Ecologists\n",
            "Author(s) : ['J. Olden', 'J. Lawler', 'N. Poff']\n",
            "Year : 2008 \n",
            "Abstract : Machine learning methods, a family of statistical techniques with origins in the field of artificial intelligence, are recognized as holding great promise for the advancement of understanding and prediction about ecological phenomena. These modeling techniques are flexible enough to handle complex problems with multiple interacting elements and typically outcompete traditional approaches (e.g., generalized linear models), making them ideal for modeling ecological systems. Despite their inherent advantages, a review of the literature reveals only a modest use of these approaches in ecology as compared to other disciplines. One potential explanation for this lack of interest is that machine learning techniques do not fall neatly into the class of statistical modeling approaches with which most ecologists are familiar. In this paper, we provide an introduction to three machine learning approaches that can be broadly used by ecologists: classification and regression trees, artificial neural networks, and evolutionary computation. For each approach, we provide a brief background to the methodology, give examples of its application in ecology, describe model development and implementation, discuss strengths and weaknesses, explore the availability of statistical software, and provide an illustrative example. Although the ecological application of machine learning approaches has increased, there remains considerable skepticism with respect to the role of these techniques in ecology. Our review encourages a greater understanding of machine learning approaches and promotes their future application and utilization, while also providing a basis from which ecologists can make informed decisions about whether to select or avoid these approaches in their future modeling endeavors.\n",
            "\n",
            "Paper ID : 5f4c2600317338978041af2aa9101f62d9f6790e \tArticle : Searching for exotic particles in high-energy physics with deep learning.\n",
            "Author(s) : ['P. Baldi', 'Peter Sadowski', 'D. Whiteson']\n",
            "Year : 2014 \n",
            "Abstract : Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine-learning approaches are often used. Standard approaches have relied on 'shallow' machine-learning models that have a limited capacity to learn complex nonlinear functions of the inputs, and rely on a painstaking search through manually constructed nonlinear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Here, using benchmark data sets, we show that deep-learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8% over the best current approaches. This demonstrates that deep-learning approaches can improve the power of collider searches for exotic particles.\n",
            "\n",
            "Paper ID : ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e \tArticle : Online Learning for Matrix Factorization and Sparse Coding\n",
            "Author(s) : ['J. Mairal', 'F. Bach', 'J. Ponce', 'G. Sapiro']\n",
            "Year : 2010 \n",
            "Abstract : Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.\n",
            "------------------------------------Extracting Page #56------------------------------------\n",
            "\n",
            "Paper ID : a206216c3f67605ac6e25b0178c3f156dc0f7ba0 \tArticle : WEKA: a machine learning workbench\n",
            "Author(s) : ['G. Holmes', 'A. Donkin', 'I. Witten']\n",
            "Year : 1994 \n",
            "Abstract : WEKA is a workbench for machine learning that is intended to aid in the application of machine learning techniques to a variety of real-world problems, in particular, those arising from agricultural and horticultural domains. Unlike other machine learning projects, the emphasis is on providing a working environment for the domain specialist rather than the machine learning expert. Lessons learned include the necessity of providing a wealth of interactive tools for data manipulation, result visualization, database linkage, and cross-validation and comparison of rule sets, to complement the basic machine learning tools.<<ETX>>\n",
            "\n",
            "Paper ID : b51a3a7d676df7947a0b28fb902a5a9f0bdf54ee \tArticle : IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning\n",
            "Author(s) : ['Kaspar Riesen', 'H. Bunke']\n",
            "Year : 2008 \n",
            "Abstract : In recent years the use of graph based representation has gained popularity in pattern recognition and machine learning. As a matter of fact, object representation by means of graphs has a number of advantages over feature vectors. Therefore, various algorithms for graph based machine learning have been proposed in the literature. However, in contrast with the emerging interest in graph based representation, a lack of standardized graph data sets for benchmarking can be observed. Common practice is that researchers use their own data sets, and this behavior cumbers the objective evaluation of the proposed methods. In order to make the different approaches in graph based machine learning better comparable, the present paper aims at introducing a repository of graph data sets and corresponding benchmarks, covering a wide spectrum of different applications.\n",
            "\n",
            "Paper ID : 5b59992ca6b77aaec066a0d3142336d2cb1028f1 \tArticle : A survey of deep learning-based network anomaly detection\n",
            "Author(s) : ['Donghwoon Kwon', 'Hyunjoo Kim', 'Jinoh Kim', 'S. Suh', 'Ikkyun Kim', 'Kuinam J. Kim']\n",
            "Year : 2017 \n",
            "Abstract : A great deal of attention has been given to deep learning over the past several years, and new deep learning techniques are emerging with improved functionality. Many computer and network applications actively utilize such deep learning algorithms and report enhanced performance through them. In this study, we present an overview of deep learning methodologies, including restricted Bolzmann machine-based deep belief network, deep neural network, and recurrent neural network, as well as the machine learning techniques relevant to network anomaly detection. In addition, this article introduces the latest work that employed deep learning techniques with the focus on network anomaly detection through the extensive literature survey. We also discuss our local experiments showing the feasibility of the deep learning approach to network traffic analysis.\n",
            "\n",
            "Paper ID : 9512facd37bba2ff1ece31900c08901bb011f1ce \tArticle : Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners\n",
            "Author(s) : ['Shike Mei', 'Xiaojin Zhu']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            " \n",
            " We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understanding training-set attacks is important as more intelligent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This paper identifies the optimal training-set attack on a broad family of machine learners. First we show that optimal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gradient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support VectorMachines, logistic regression, and linear regression with extensive experiments. Finally, we discuss potential defenses against such attacks.\n",
            " \n",
            "\n",
            "\n",
            "Paper ID : 9c4da62e9e89e65ac78ee271e424e8b498053e8c \tArticle : Advances in kernel methods: support vector learning\n",
            "Author(s) : ['B. Schölkopf', 'C. Burges', 'Alex Smola']\n",
            "Year : 1999 \n",
            "Abstract : Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.\n",
            "\n",
            "Paper ID : 70bcdbac2c70c9f107123262cf5257b57839f708 \tArticle : Machine Learning\n",
            "Author(s) : ['A. Hoffmann', 'Ashesh Mahidadia']\n",
            "Year : 2010 \n",
            "Abstract : fo im sy su bstract — mponents a stem (AIS) mune syst lection and say thoroug Based on t plemented s a smart v pable of det ve the abi pular class e integrate tection abili eywords: An tificial intel I. INTR At the mom ethods have b Data-based rmful files. A gnatures whi mplicated to em. Behavior-ba stem’s ap quences, sys aracteristics ]. Although omising resu lse positive solved [3]. The disadva the require ethods. Am namic and pability of A rpose of inte Besides th llowing cha mune system stem; IV. mmary. RES\n",
            "\n",
            "Paper ID : 3c1dfe0b52c0638ff69a187ae183e8cdeebff303 \tArticle : Machine-Learning Research Four Current Directions\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 1997 \n",
            "Abstract : The last five years have seen an explosion in machine-learning research. This explosion has many causes: First, separate research communities in symbolic machine learning, computational learning theory, neural networks, statistics, and pattern recognition have discovered one another and begun to work together. Second, machine-learning techniques are being applied to new kinds of problem, including knowledge discovery in databases, language processing, robot control, and combinatorial optimization, as well as to more traditional problems such as speech recognition, face recognition, handwriting recognition, medical data analysis, and game playing. In this article, I selected four topics within machine learning where there has been a lot of recent activity. The purpose of the article is to describe the results in these areas to a broader AI audience and to sketch some of the open research problems. The topic areas are (1) ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models. The reader should be cautioned that this article is not a comprehensive review of each of these topics. Rather, my goal is to provide a representative sample of the research in each of these four areas. In each of the areas, there are many other papers that describe relevant work. I apologize to those authors whose work I was unable to include in the article. Ensembles of Classifiers\n",
            "\n",
            "Paper ID : 872352b0a53ab6cbb4420f81df64d215d86c7d9b \tArticle : Emotions from Text: Machine Learning for Text-based Emotion Prediction\n",
            "Author(s) : ['Cecilia Ovesdotter Alm', 'D. Roth', 'R. Sproat']\n",
            "Year : 2005 \n",
            "Abstract : In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a naive baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations. In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions.\n",
            "\n",
            "Paper ID : c9d60858f1cbe6b7eb36473b7d37ff4b73c31af8 \tArticle : Machine Learning from Imbalanced Data Sets 101\n",
            "Author(s) : ['F. Provost']\n",
            "Year : 2008 \n",
            "Abstract : For research to progress most effectively, we first should establish common ground regarding just what is the problem that imbalanced data sets present to machine learning systems. Why and when should imbalanced data sets be problematic? When is the problem simply an artifact of easily rectified design choices? I will try to pick the low-hanging fruit and share them with the rest of the workshop participants. Specifically, I would like to discuss what the problem is not. I hope this will lead to a profitable discussion of what the problem indeed is, and how it might be addressed most effectively.\n",
            "\n",
            "Paper ID : 50feafd2cdafdfb1eead14388f6210f6b467eaa0 \tArticle : Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies\n",
            "Author(s) : ['Yaochu Jin', 'B. Sendhoff']\n",
            "Year : 2008 \n",
            "Abstract : Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested.\n",
            "------------------------------------Extracting Page #57------------------------------------\n",
            "\n",
            "Paper ID : d517b13f2b152c913b81ce534a149493517dbdad \tArticle : Big Data Deep Learning: Challenges and Perspectives\n",
            "Author(s) : ['Xue-wen Chen', 'Xiaotong Lin']\n",
            "Year : 2014 \n",
            "Abstract : Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.\n",
            "\n",
            "Paper ID : 3e397756f7c64c2ce77510f9f0539fae1171ee92 \tArticle : An Insight into Extreme Learning Machines: Random Neurons, Random Features and Kernels\n",
            "Author(s) : ['G. Huang']\n",
            "Year : 2014 \n",
            "Abstract : Extreme learning machines (ELMs) basically give answers to two fundamental learning problems: (1) Can fundamentals of learning (i.e., feature learning, clustering, regression and classification) be made without tuning hidden neurons (including biological neurons) even when the output shapes and function modeling of these neurons are unknown? (2) Does there exist unified framework for feedforward neural networks and feature space methods? ELMs that have built some tangible links between machine learning techniques and biological learning mechanisms have recently attracted increasing attention of researchers in widespread research areas. This paper provides an insight into ELMs in three aspects, viz: random neurons, random features and kernels. This paper also shows that in theory ELMs (with the same kernels) tend to outperform support vector machine and its variants in both regression and classification applications with much easier implementation.\n",
            "\n",
            "Paper ID : 5726c7b40fcc454b77d989656c085520bf6c15fa \tArticle : Multimodal learning with deep Boltzmann machines\n",
            "Author(s) : ['Nitish Srivastava', 'R. Salakhutdinov']\n",
            "Year : 2012 \n",
            "Abstract : Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.\n",
            "\n",
            "Paper ID : 7550a05bf00f7b24aed9c1ac3ef000575388d21c \tArticle : Making large scale SVM learning practical\n",
            "Author(s) : ['T. Joachims']\n",
            "Year : 1998 \n",
            "Abstract : Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.\n",
            "\n",
            "Paper ID : 9011405b759b492b1132aea7b6165c9d1b0513e7 \tArticle : Reconciling schemas of disparate data sources: a machine-learning approach\n",
            "Author(s) : ['A. Doan', 'Pedro M. Domingos', 'A. Halevy']\n",
            "Year : 2001 \n",
            "Abstract : A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.\n",
            "\n",
            "Paper ID : 391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b \tArticle : Deep Learning for Content-Based Image Retrieval: A Comprehensive Study\n",
            "Author(s) : ['Ji Wan', 'Dayong Wang', 'S. Hoi', 'Pengcheng Wu', 'Jianke Zhu', 'Yongdong Zhang', 'Jintao Li']\n",
            "Year : 2014 \n",
            "Abstract : Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.\n",
            "\n",
            "Paper ID : a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4 \tArticle : Exploiting Machine Learning to Subvert Your Spam Filter\n",
            "Author(s) : ['Blaine Nelson', 'M. Barreno', 'F. J. Chi', 'A. Joseph', 'Benjamin I. P. Rubinstein', 'Udam Saini', 'Charles Sutton', 'J. Tygar', 'Kai Xia']\n",
            "Year : 2008 \n",
            "Abstract : Using statistical machine learning for making security decisions introduces new vulnerabilities in large scale systems. This paper shows how an adversary can exploit statistical machine learning, as used in the SpamBayes spam filter, to render it useless--even if the adversary's access is limited to only 1% of the training messages. We further demonstrate a new class of focused attacks that successfully prevent victims from receiving specific email messages. Finally, we introduce two new types of defenses against these attacks.\n",
            "\n",
            "Paper ID : 5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7 \tArticle : Can machine learning be secure?\n",
            "Author(s) : ['M. Barreno', 'Blaine Nelson', 'R. Sears', 'A. Joseph', 'J. Tygar']\n",
            "Year : 2006 \n",
            "Abstract : Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, \"Can machine learning be secure?\" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems.\n",
            "\n",
            "Paper ID : ae20c630759847cd88655631d0041b0be127fd0c \tArticle : Transfer Learning for Visual Categorization: A Survey\n",
            "Author(s) : ['L. Shao', 'F. Zhu', 'Xuelong Li']\n",
            "Year : 2015 \n",
            "Abstract : Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.\n",
            "\n",
            "Paper ID : 07295d6836d8fadd4150328d33659e5022fccc2f \tArticle : Structured machine learning: the next ten years\n",
            "Author(s) : ['Thomas G. Dietterich', 'Pedro M. Domingos', 'L. Getoor', 'S. Muggleton', 'Prasad Tadepalli']\n",
            "Year : 2008 \n",
            "Abstract : The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.\n",
            "------------------------------------Extracting Page #58------------------------------------\n",
            "\n",
            "Paper ID : 07295d6836d8fadd4150328d33659e5022fccc2f \tArticle : Structured machine learning: the next ten years\n",
            "Author(s) : ['Thomas G. Dietterich', 'Pedro M. Domingos', 'L. Getoor', 'S. Muggleton', 'Prasad Tadepalli']\n",
            "Year : 2008 \n",
            "Abstract : The field of inductive logic programming (ILP) has made steady progress, since the first ILP workshop in 1991, based on a balance of developments in theory, implementations and applications. More recently there has been an increased emphasis on Probabilistic ILP and the related fields of Statistical Relational Learning (SRL) and Structured Prediction. The goal of the current paper is to consider these emerging trends and chart out the strategic directions and open problems for the broader area of structured machine learning for the next 10 years.\n",
            "\n",
            "Paper ID : 464ebd4c86542e1610d67a9bc8810b8a7c109efe \tArticle : Convex incremental extreme learning machine\n",
            "Author(s) : ['G. Huang', 'Lei Chen']\n",
            "Year : 2007 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 59882b92d0183163e897a671b8c9298f89df5df3 \tArticle : Task-Driven Dictionary Learning\n",
            "Author(s) : ['J. Mairal', 'F. Bach', 'J. Ponce']\n",
            "Year : 2012 \n",
            "Abstract : Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience, and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.\n",
            "\n",
            "Paper ID : 4417f78b31546227784941bbd6f6532a177e60b8 \tArticle : Deep Learning using Linear Support Vector Machines\n",
            "Author(s) : ['Yichuan Tang']\n",
            "Year : 2013 \n",
            "Abstract : Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these \"deep learning\" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.\n",
            "\n",
            "Paper ID : 664b701a39371c5356754dc72cea1349233c8506 \tArticle : Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems\n",
            "Author(s) : ['S. Weiss', 'C. Kulikowski']\n",
            "Year : 1990 \n",
            "Abstract : Preface 1 Overview of Learning Systems 1.1 What is a Learning System? 1.2 Motivation for Building Learning Systems 1.3 Types of Practical Empirical Learning Systems 1.3.1 Common Theme: The Classification Model 1.3.2 Let the Data Speak 1.4 What's New in Learning Methods 1.4.1 The Impact of New Technology 1.5 Outline of the Book 1.6 Bibliographical and Historical Remarks 2 How to Estimate the True Performance of a Learning System 2.1 The Importance of Unbiased Error Rate Estimation 2.2. What is an Error? 2.2.1 Costs and Risks 2.3 Apparent Error Rate Estimates 2.4 Too Good to Be True: Overspecialization 2.5 True Error Rate Estimation 2.5.1 The Idealized Model for Unlimited Samples 2.5.2 Train-and Test Error Rate Estimation 2.5.3 Resampling Techniques 2.5.4 Finding the Right Complexity Fit 2.6 Getting the Most Out of the Data 2.7 Classifier Complexity and Feature Dimensionality 2.7.1 Expected Patterns of Classifier Behavior 2.8 What Can Go Wrong? 2.8.1 Poor Features, Data Errors, and Mislabeled Classes 2.8.2 Unrepresentative Samples 2.9 How Close to the Truth? 2.10 Common Mistakes in Performance Analysis 2.11 Bibliographical and Historical Remarks 3 Statistical Pattern Recognition 3.1 Introduction and Overview 3.2 A Few Sample Applications 3.3 Bayesian Classifiers 3.3.1 Direct Application of the Bayes Rule 3.4 Linear Discriminants 3.4.1 The Normality Assumption and Discriminant Functions 3.4.2 Logistic Regression 3.5 Nearest Neighbor Methods 3.6 Feature Selection 3.7 Error Rate Analysis 3.8 Bibliographical and Historical Remarks 4 Neural Nets 4.1 Introduction and Overview 4.2 Perceptrons 4.2.1 Least Mean Square Learning Systems 4.2.2 How Good Is a Linear Separation Network? 4.3 Multilayer Neural Networks 4.3.1 Back-Propagation 4.3.2 The Practical Application of Back-Propagation 4.4 Error Rate and Complexity Fit Estimation 4.5 Improving on Standard Back-Propagation 4.6 Bibliographical and Historical Remarks 5 Machine Learning: Easily Understood Decision Rules 5.1 Introduction and Overview 5.2 Decision Trees 5.2.1 Finding the Perfect Tree 5.2.2 The Incredible Shrinking Tree 5.2.3 Limitations of Tree Induction Methods 5.3 Rule Induction 5.3.1 Predictive Value Maximization 5.4 Bibliographical and Historical Remarks 6 Which Technique is Best? 6.1 What's Important in Choosing a Classifier? 6.1.1 Prediction Accuracy 6.1.2 Speed of Learning and Classification 6.1.3 Explanation and Insight 6.2 So, How Do I Choose a Learning System? 6.3 Variations on the Standard Problem 6.3.1 Missing Data 6.3.2 Incremental Learning 6.4 Future Prospects for Improved Learning Methods 6.5 Bibliographical and Historical Remarks 7 Expert Systems 7.1 Introduction and Overview 7.1.1 Why Build Expert Systems? New vs. Old Knowledge 7.2 Estimating Error Rates for Expert Systems 7.3 Complexity of Knowledge Bases 7.3.1 How Many Rules Are Too Many? 7.4 Knowledge Base Example 7.5 Empirical Analysis of Knowledge Bases 7.6 Future: Combined Learning and Expert Systems 7.7 Bibliographical and Historical Remarks References Author Index Subject Index\n",
            "\n",
            "Paper ID : 028165965fdf066821e1b65ac1de3ae6c503c30d \tArticle : Editorial: On Machine Learning\n",
            "Author(s) : ['P. Langley']\n",
            "Year : 2004 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 83cf4b2f39bcc802b09fd59b69e23068447b26b7 \tArticle : Multi-Task Learning for Multiple Language Translation\n",
            "Author(s) : ['Daxiang Dong', 'Hua Wu', 'W. He', 'Dianhai Yu', 'Haifeng Wang']\n",
            "Year : 2015 \n",
            "Abstract : In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.\n",
            "\n",
            "Paper ID : 035256a8a6d8a73af2adb38245f4130daa1f0535 \tArticle : Machine learning in bioinformatics\n",
            "Author(s) : ['P. Larrañaga', 'Borja Calvo', 'Roberto Santana', 'C. Bielza', 'Josu Galdiano', 'Iñaki Inza', 'J. A. Lozano', 'Rubén Armañanzas', 'G. Santafé', 'Aritz Pérez Martínez', 'V. Robles']\n",
            "Year : 2006 \n",
            "Abstract : This article reviews machine learning methods for bioinformatics. It presents modelling methods, such as supervised classification, clustering and probabilistic graphical models for knowledge discovery, as well as deterministic and stochastic heuristics for optimization. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown.\n",
            "\n",
            "Paper ID : 94ac58d1b6132b07ef66c47af8bd506bdb67e0c3 \tArticle : Investigating statistical machine learning as a tool for software development\n",
            "Author(s) : ['Kayur Patel', 'J. Fogarty', 'J. Landay', 'B. Harrison']\n",
            "Year : 2008 \n",
            "Abstract : As statistical machine learning algorithms and techniques continue to mature, many researchers and developers see statistical machine learning not only as a topic of expert study, but also as a tool for software development. Extensive prior work has studied software development, but little prior work has studied software developers applying statistical machine learning. This paper presents interviews of eleven researchers experienced in applying statistical machine learning algorithms and techniques to human-computer interaction problems, as well as a study of ten participants working during a five-hour study to apply statistical machine learning algorithms and techniques to a realistic problem. We distill three related categories of difficulties that arise in applying statistical machine learning as a tool for software development: (1) difficulty pursuing statistical machine learning as an iterative and exploratory process, (2) difficulty understanding relationships between data and the behavior of statistical machine learning algorithms, and (3) difficulty evaluating the performance of statistical machine learning algorithms and techniques in the context of applications. This paper provides important new insight into these difficulties and the need for development tools that better support the application of statistical machine learning.\n",
            "\n",
            "Paper ID : 1f135e98e867ffcde5b359e7b817bbe21f80cfce \tArticle : Deep Learning and Its Application to LHC Physics\n",
            "Author(s) : ['D. Guest', 'K. Cranmer', 'D. Whiteson']\n",
            "Year : 2018 \n",
            "Abstract : Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis are explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LHC data, and discussion of future prospects and concerns.\n",
            "------------------------------------Extracting Page #59------------------------------------\n",
            "\n",
            "Paper ID : cfacdf62b0f3e50993a6a55250127d694a2efdce \tArticle : The city as a machine for learning.\n",
            "Author(s) : ['Colin McFarlane']\n",
            "Year : 2011 \n",
            "Abstract : Abstract Despite its centrality to urban politics, economies and life, learning remains a neglected and undertheorised domain in urban geography. In this paper, I address this by exploring a politics of learning through two key sites: first, tactical learning; second, urban learning forums. I offer a conception of learning based on three processes: translation , or the relational distributions through which learning is produced as a sociomaterial epistemology of displacement and change; coordination , or the construction of functional systems that enable learning as a means of linking different forms of knowledge, coping with complexity and facilitating adaptation; and dwelling , or the education of attention through which learning operates as a way of seeing and inhabiting the world. I then consider this conception of learning in relation to tactical learning, i.e. the resources marginal groups use to cope with, negotiate and resist in the city, and urban learning forums, i.e. the possibilities for progressive forms of learning between different constituencies in the city. I conclude with an outline of a critical urbanism of learning.\n",
            "\n",
            "Paper ID : 76f96dadd80b19bde49e0e1f07bfa9fe8485eeec \tArticle : Learning with Kernels: support vector machines, regularization, optimization, and beyond\n",
            "Author(s) : ['B. Schölkopf', 'Alex Smola']\n",
            "Year : 2002 \n",
            "Abstract : From the Publisher: \n",
            "In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. \n",
            "Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.\n",
            "\n",
            "Paper ID : 0628fdf728d0aba31be803a7d834c7f4b569408d \tArticle : Imbalanced Learning: Foundations, Algorithms, and Applications\n",
            "Author(s) : ['Haibo He', 'Yunqian Ma']\n",
            "Year : 2013 \n",
            "Abstract : The first book of its kind to review the current status and future direction of the exciting new branch of machine learning/data mining called imbalanced learningImbalanced learning focuses on how an intelligent system can learn when it is provided with imbalanced data. Solving imbalanced learning problems is critical in numerous data-intensive networked systems, including surveillance, security, Internet, finance, biomedical, defense, and more. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. The first comprehensive look at this new branch of machine learning, this book offers a critical review of the problem of imbalanced learning, covering the state of the art in techniques, principles, and real-world applications. Featuring contributions from experts in both academia and industry, Imbalanced Learning: Foundations, Algorithms, and Applications provides chapter coverage on:Foundations of Imbalanced LearningImbalanced Datasets: From Sampling to ClassifiersEnsemble Methods for Class Imbalance LearningClass Imbalance Learning Methods for Support Vector MachinesClass Imbalance and Active LearningNonstationary Stream Data Learning with Imbalanced Class DistributionAssessment Metrics for Imbalanced LearningImbalanced Learning: Foundations, Algorithms, and Applications will help scientists and engineers learn how to tackle the problem of learning from imbalanced datasets, and gain insight into current developments in the field as well as future research directions.\n",
            "\n",
            "Paper ID : 6f0cde3fcab0044f386b5b8a4244c371507bec15 \tArticle : A Survey on Metric Learning for Feature Vectors and Structured Data\n",
            "Author(s) : ['A. Bellet', 'Amaury Habrard', 'M. Sebban']\n",
            "Year : 2013 \n",
            "Abstract : The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.\n",
            "\n",
            "Paper ID : cf80cc34528273d8fbe17783efe802a6509e1562 \tArticle : Online dictionary learning for sparse coding\n",
            "Author(s) : ['J. Mairal', 'F. Bach', 'J. Ponce', 'G. Sapiro']\n",
            "Year : 2009 \n",
            "Abstract : Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.\n",
            "\n",
            "Paper ID : bad620c25920edbaba8836032459b135669171c3 \tArticle : Machine Learning and Its Applications to Biology\n",
            "Author(s) : ['A. Tarca', 'V. Carey', 'Xue-wen Chen', 'R. Romero', 'S. Drăghici']\n",
            "Year : 2007 \n",
            "Abstract : The term machine learning refers to a set of topics dealing with the creation and evaluation of algorithms that facilitate pattern recognition, classification, and prediction, based on models derived from existing data. Two facets of mechanization should be acknowledged when considering machine learning in broad terms. Firstly, it is intended that the classification and prediction tasks can be accomplished by a suitably programmed computing machine. That is, the product of machine learning is a classifier that can be feasibly used on available hardware. Secondly, it is intended that the creation of the classifier should itself be highly mechanized, and should not involve too much human input. This second facet is inevitably vague, but the basic objective is that the use of automatic algorithm construction methods can minimize the possibility that human biases could affect the selection and performance of the algorithm. Both the creation of the algorithm and its operation to classify objects or predict events are to be based on concrete, observable data. \n",
            " \n",
            "The history of relations between biology and the field of machine learning is long and complex. An early technique [1] for machine learning called the perceptron constituted an attempt to model actual neuronal behavior, and the field of artificial neural network (ANN) design emerged from this attempt. Early work on the analysis of translation initiation sequences [2] employed the perceptron to define criteria for start sites in Escherichia coli. Further artificial neural network architectures such as the adaptive resonance theory (ART) [3] and neocognitron [4] were inspired from the organization of the visual nervous system. In the intervening years, the flexibility of machine learning techniques has grown along with mathematical frameworks for measuring their reliability, and it is natural to hope that machine learning methods will improve the efficiency of discovery and understanding in the mounting volume and complexity of biological data. \n",
            " \n",
            "This tutorial is structured in four main components. Firstly, a brief section reviews definitions and mathematical prerequisites. Secondly, the field of supervised learning is described. Thirdly, methods of unsupervised learning are reviewed. Finally, a section reviews methods and examples as implemented in the open source data analysis and visualization language R (http://www.r-project.org).\n",
            "\n",
            "Paper ID : 1bcbf2a4500d27d036e0f9d36d7af71c72f8ab61 \tArticle : Recognizing facial expression: machine learning and application to spontaneous behavior\n",
            "Author(s) : ['M. Bartlett', 'G. Littlewort', 'M. Frank', 'C. Lainscsek', 'I. Fasel', 'J. Movellan']\n",
            "Year : 2005 \n",
            "Abstract : We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including AdaBoost, support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of AdaBoost for feature selection prior to classification by SVM or LDA. Best results were obtained by selecting a subset of Gabor filters using AdaBoost followed by classification with support vector machines. The system operates in real-time, and obtained 93% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset. The outputs of the classifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions (FACS). The present system classifies 17 action units, whether they occur singly or in combination with other actions, with a mean accuracy of 94.8%. We present preliminary results for applying this system to spontaneous facial expressions.\n",
            "\n",
            "Paper ID : fcb926027ba5001f8f69dc0f1de5ded7d003b6af \tArticle : A comparison of machine learning techniques for phishing detection\n",
            "Author(s) : ['Saeed Abu-Nimeh', 'D. Nappa', 'Xinlei Wang', 'S. Nair']\n",
            "Year : 2007 \n",
            "Abstract : There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.\n",
            "\n",
            "Paper ID : 48234756b7cf798bfeb47328f7c5d597fd4838c2 \tArticle : ADASYN: Adaptive synthetic sampling approach for imbalanced learning\n",
            "Author(s) : ['Haibo He', 'Yang Bai', 'E. A. Garcia', 'Shutao Li']\n",
            "Year : 2008 \n",
            "Abstract : This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.\n",
            "\n",
            "Paper ID : c0927ffb79810318daab8821068629975cab67ad \tArticle : Deep Learning for Classification of Malware System Call Sequences\n",
            "Author(s) : ['Bojan Kolosnjaji', 'Apostolis Zarras', 'George D. Webster', 'C. Eckert']\n",
            "Year : 2016 \n",
            "Abstract : The increase in number and variety of malware samples amplifies the need for improvement in automatic detection and classification of the malware variants. Machine learning is a natural choice to cope with this increase, because it addresses the need of discovering underlying patterns in large-scale datasets. Nowadays, neural network methodology has been grown to the state that can surpass limitations of previous machine learning methods, such as Hidden Markov Models and Support Vector Machines. As a consequence, neural networks can now offer superior classification accuracy in many domains, such as computer vision or natural language processing. This improvement comes from the possibility of constructing neural networks with a higher number of potentially diverse layers and is known as Deep Learning.\n",
            "------------------------------------Extracting Page #60------------------------------------\n",
            "\n",
            "Paper ID : 48234756b7cf798bfeb47328f7c5d597fd4838c2 \tArticle : ADASYN: Adaptive synthetic sampling approach for imbalanced learning\n",
            "Author(s) : ['Haibo He', 'Yang Bai', 'E. A. Garcia', 'Shutao Li']\n",
            "Year : 2008 \n",
            "Abstract : This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.\n",
            "\n",
            "Paper ID : c0927ffb79810318daab8821068629975cab67ad \tArticle : Deep Learning for Classification of Malware System Call Sequences\n",
            "Author(s) : ['Bojan Kolosnjaji', 'Apostolis Zarras', 'George D. Webster', 'C. Eckert']\n",
            "Year : 2016 \n",
            "Abstract : The increase in number and variety of malware samples amplifies the need for improvement in automatic detection and classification of the malware variants. Machine learning is a natural choice to cope with this increase, because it addresses the need of discovering underlying patterns in large-scale datasets. Nowadays, neural network methodology has been grown to the state that can surpass limitations of previous machine learning methods, such as Hidden Markov Models and Support Vector Machines. As a consequence, neural networks can now offer superior classification accuracy in many domains, such as computer vision or natural language processing. This improvement comes from the possibility of constructing neural networks with a higher number of potentially diverse layers and is known as Deep Learning.\n",
            "\n",
            "Paper ID : e541c475457a731d7d434c4302867fc45af5876f \tArticle : Active Learning\n",
            "Author(s) : ['Burr Settles']\n",
            "Year : 2012 \n",
            "Abstract : The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns. An active learner may pose \"queries,\" usually in the form of unlabeled data instances to be labeled by an \"oracle\" (e.g., a human annotator) that already understands the nature of the problem. This sort of approach is well-motivated in many modern machine learning and data mining applications, where unlabeled data may be abundant or easy to come by, but training labels are difficult, time-consuming, or expensive to obtain. This book is a general introduction to active learning. It outlines several scenarios in which queries might be formulated, and details many query selection algorithms which have been organized into four broad categories, or \"query selection frameworks.\" We also touch on some of the theoretical foundations of active learning, and conclude with an overview of the strengths and weaknesses of these approaches in practice, including a summary of ongoing work to address these open challenges and opportunities.\n",
            "\n",
            "Paper ID : 3493e0285fe329a710f54be2ef82350fdaafc991 \tArticle : Applications of Machine Learning to Cognitive Radio Networks\n",
            "Author(s) : ['T. Clancy', 'Joe Hecker', 'E. P. Stuntebeck', \"Tim O'Shea\"]\n",
            "Year : 2007 \n",
            "Abstract : Cognitive radio offers the promise of intelligent radios that can learn from and adapt to their environment. To date, most cognitive radio research has focused on policy-based radios that are hard-coded with a list of rules on how the radio should behave in certain scenarios. Some work has been done on radios with learning engines tailored for very specific applications. This article describes a concrete model for a generic cognitive radio to utilize a learning engine. The goal is to incorporate the results of the learning engine into a predicate calculus-based reasoning engine so that radios can remember lessons learned in the past and act quickly in the future. We also investigate the differences between reasoning and learning, and the fundamentals of when a particular application requires learning, and when simple reasoning is sufficient. The basic architecture is consistent with cognitive engines seen in AI research. The focus of this article is not to propose new machine learning algorithms, but rather to formalize their application to cognitive radio and develop a framework from within which they can be useful. We describe how our generic cognitive engine can tackle problems such as capacity maximization and dynamic spectrum access.\n",
            "\n",
            "Paper ID : 9dc64d487a73c4493dd6e846783fdbfdd06f6ad6 \tArticle : What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt’s Dream and John von Neumann’s Puzzle\n",
            "Author(s) : ['G. Huang']\n",
            "Year : 2015 \n",
            "Abstract : The emergent machine learning technique—extreme learning machines (ELMs)—has become a hot area of research over the past years, which is attributed to the growing research activities and significant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that (1) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes/neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes/neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc.). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; (2) ELM is proposed for both generalized single-hidden-layer feedforward network and multi-hidden-layer feedforward networks (including biological neural networks); (3) homogeneous architecture-based ELM is proposed for feature learning, clustering, regression and (binary/multi-class) classification. (4) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi-hidden-layer feedforward networks either.\n",
            "\n",
            "Paper ID : 08c81389b3ac4b8253d718a7cebe04a5536efa78 \tArticle : Improving Machine Learning Approaches to Coreference Resolution\n",
            "Author(s) : ['Vincent Ng', 'Claire Gardent']\n",
            "Year : 2002 \n",
            "Abstract : We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.\n",
            "\n",
            "Paper ID : c2d0846fb06d1bb478159eec23ad438576492a89 \tArticle : Machine Learning and Data Mining: Introduction to Principles and Algorithms\n",
            "Author(s) : ['I. Kononenko', 'M. Kukar']\n",
            "Year : 2007 \n",
            "Abstract : Introduction Learning and intelligence Machine learning basics Knowledge representation Learning as search Attribute quality measures Data pre-processing Constructive induction Symbolic learning Statistical learning Artificial neural networks Cluster analysis Learning theory Computational learning theory Definitions References and index.\n",
            "\n",
            "Paper ID : ab08f2a0b98fe7938d08875eb6125fa518620222 \tArticle : The Need for Open Source Software in Machine Learning\n",
            "Author(s) : ['S. Sonnenburg', 'M. Braun', 'Cheng Soon Ong', 'Samy Bengio', 'L. Bottou', 'G. Holmes', 'Yann LeCun', 'K. Müller', 'Fernando C Pereira', 'C. Rasmussen', 'Gunnar Rätsch', 'B. Schölkopf', 'Alex Smola', 'Pascal Vincent', 'J. Weston', 'R. C. Williamson']\n",
            "Year : 2007 \n",
            "Abstract : Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.\n",
            "\n",
            "Paper ID : f4055c2f7198443038ec3a5ff44c3bae4f3f5fea \tArticle : Weka: Practical machine learning tools and techniques with Java implementations\n",
            "Author(s) : ['I. Witten', 'Eibe Frank', 'Leonard E. Trigg', 'M. Hall', 'G. Holmes', 'S. Cunningham']\n",
            "Year : 1999 \n",
            "Abstract : The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [1] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform.\n",
            "\n",
            "Paper ID : eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece \tArticle : Adaptive computation and machine learning\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 1998 \n",
            "Abstract : All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from this is the last candidate. next esc will revert to uncompleted text. he publisher. Overview Dataset shift is a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages. Covariate shift is a simpler particular case of dataset shift where only the input distribution changes (covariate denotes input), while the conditional distribution of the outputs given the inputs p(y|x) remains unchanged. Dataset shift is present in most practical applications for reasons ranging from the bias introduced by experimental design, to the mere irreproducibility of the testing conditions at training time. For example, in an image classification task, training data might have been recorded under controlled laboratory conditions, whereas the test data may show different lighting conditions. In other applications, the process that generates data is in itself adaptive. Some of our authors consider the problem of spam email filtering: successful \" spammers \" will try to build spam in a form that differs from the spam the automatic filter has been built on. Dataset shift seems to have raised relatively little interest in the machine learning community until very recently. Indeed, many machine learning algorithms are based on the assumption that the training data is drawn from exactly the same distribution as the test data on which the model will later be evaluated. Semi-supervised learning and active learning, two problems that seem very similar to covariate shift have received much more attention. How do they differ from covariate shift? Semi-supervised learning is designed to take advantage of unlabeled data present at training time, but is not conceived to be robust against changes in the input distribution. In fact, one can easily construct examples of covariate shift for which common SSL strategies such as the \" cluster assumption \" will lead to disaster. In active learning the algorithm is asked to select from the available unlabeled inputs those for which obtaining the label will be most beneficial for learning. This is very relevant in contexts where labeling data is very costly, but active learning strategies 2 Contents are not specifically design for dealing with covariate shift. This book attempts to give an overview of the different recent efforts that are being …\n",
            "------------------------------------Extracting Page #61------------------------------------\n",
            "\n",
            "Paper ID : f4055c2f7198443038ec3a5ff44c3bae4f3f5fea \tArticle : Weka: Practical machine learning tools and techniques with Java implementations\n",
            "Author(s) : ['I. Witten', 'Eibe Frank', 'Leonard E. Trigg', 'M. Hall', 'G. Holmes', 'S. Cunningham']\n",
            "Year : 1999 \n",
            "Abstract : The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [1] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform.\n",
            "\n",
            "Paper ID : eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece \tArticle : Adaptive computation and machine learning\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 1998 \n",
            "Abstract : All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from this is the last candidate. next esc will revert to uncompleted text. he publisher. Overview Dataset shift is a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages. Covariate shift is a simpler particular case of dataset shift where only the input distribution changes (covariate denotes input), while the conditional distribution of the outputs given the inputs p(y|x) remains unchanged. Dataset shift is present in most practical applications for reasons ranging from the bias introduced by experimental design, to the mere irreproducibility of the testing conditions at training time. For example, in an image classification task, training data might have been recorded under controlled laboratory conditions, whereas the test data may show different lighting conditions. In other applications, the process that generates data is in itself adaptive. Some of our authors consider the problem of spam email filtering: successful \" spammers \" will try to build spam in a form that differs from the spam the automatic filter has been built on. Dataset shift seems to have raised relatively little interest in the machine learning community until very recently. Indeed, many machine learning algorithms are based on the assumption that the training data is drawn from exactly the same distribution as the test data on which the model will later be evaluated. Semi-supervised learning and active learning, two problems that seem very similar to covariate shift have received much more attention. How do they differ from covariate shift? Semi-supervised learning is designed to take advantage of unlabeled data present at training time, but is not conceived to be robust against changes in the input distribution. In fact, one can easily construct examples of covariate shift for which common SSL strategies such as the \" cluster assumption \" will lead to disaster. In active learning the algorithm is asked to select from the available unlabeled inputs those for which obtaining the label will be most beneficial for learning. This is very relevant in contexts where labeling data is very costly, but active learning strategies 2 Contents are not specifically design for dealing with covariate shift. This book attempts to give an overview of the different recent efforts that are being …\n",
            "\n",
            "Paper ID : e6dd83b2aa34c806596fc619ff3fbccf5f9830ab \tArticle : Unsupervised Learning by Probabilistic Latent Semantic Analysis\n",
            "Author(s) : ['Thomas Hofmann']\n",
            "Year : 2004 \n",
            "Abstract : This paper presents a novel statistical method for factor analysis of binary and count data which is closely related to a technique known as Latent Semantic Analysis. In contrast to the latter method which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed technique uses a generative latent class model to perform a probabilistic mixture decomposition. This results in a more principled approach with a solid foundation in statistical inference. More precisely, we propose to make use of a temperature controlled version of the Expectation Maximization algorithm for model fitting, which has shown excellent performance in practice. Probabilistic Latent Semantic Analysis has many applications, most prominently in information retrieval, natural language processing, machine learning from text, and in related areas. The paper presents perplexity results for different types of text and linguistic data collections and discusses an application in automated document indexing. The experiments indicate substantial and consistent improvements of the probabilistic method over standard Latent Semantic Analysis.\n",
            "\n",
            "Paper ID : 0948365ef39ef153e61e9569ade541cf881c7c2a \tArticle : Learning the Kernel Matrix with Semidefinite Programming\n",
            "Author(s) : ['G. Lanckriet', 'N. Cristianini', 'P. Bartlett', 'L. Ghaoui', 'Michael I. Jordan']\n",
            "Year : 2004 \n",
            "Abstract : Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.\n",
            "\n",
            "Paper ID : 571929abb1f8dbb244f416ee470c4a30d255cde7 \tArticle : Map-Reduce for Machine Learning on Multicore\n",
            "Author(s) : ['B. Schölkopf', 'J. Platt', 'T. Hofmann']\n",
            "Year : 2007 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 60caa5b3d066e13feac496fd0736e976970eb09f \tArticle : Overview of machine learning\n",
            "Author(s) : ['K. Murphy']\n",
            "Year : 2007 \n",
            "Abstract : The most widely studied problem in machine learning is supervised learning. We are given a labeled training set of input-output pairs, D = (xi, yi)i=1, and have to learn a way to predict the output or target ỹ for a novel test input x̃ (i.e, for x̃ 6∈ D). (We use the tilde notation to denote test cases that we have not seen before.) Some examples include: predicting if someone has cancer ỹ ∈ {0, 1} given some measured variables x̃; predicting the stock price tomorrow ỹ ∈ IR given the stock prices today x̃; etc. A common approach is to just predict one’s “best guess”, such as ŷ(x̃). However, we prefer to compute a probability distribution over the output, p(ỹ|x̃), since it is very useful to have a measure of confidence associated with one’s prediction, especially in medical and financial domains. In addition, probabilistic methods are essential for unsupervised learning, as we discuss in Section 3. If y is discrete or categorical, say y ∈ {1, 2, . . . , C}, this problem is called classification or pattern recognition. If there are C = 2 classes or labels, the problem is called binary classification (see Figure 1 for an example), otherwise it is called multi-class classification. We usually assume the classes are mutually exclusive, so y can only be in one possible state. If we want to allow multiple labels, we can represent y by a bit-vector of length C, so yj = 1 if y belongs to class j. If y is continuous, say y ∈ IR, this problem is called regression. If y is multidimensional, say y ∈ IR , we call it multivariate regression. If y is discrete, but ordered (e.g., y ∈ {low,medium,high}), the problem is called ordinal regression. A priori, our prediction might be quite poor, but we are provided with a labeled training set of input-output pairs, D = (xi, yi) n i=1, which provides a set of examples of the “right response” for a set of possible inputs. If each input\n",
            "\n",
            "Paper ID : 56b37f88b109fd455a389642e1747f766f1be471 \tArticle : Using machine learning to focus iterative optimization\n",
            "Author(s) : ['F. Agakov', 'Edwin V. Bonilla', 'J. Cavazos', 'Björn Franke', 'G. Fursin', 'M. O’Boyle', 'J. Thomson', 'M. Toussaint', 'Christopher K. I. Williams']\n",
            "Year : 2006 \n",
            "Abstract : Iterative compiler optimization has been shown to outperform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper develops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an independent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C67I3 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magnitude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.\n",
            "\n",
            "Paper ID : f2642db17084b14068d56f332de2f2d5a1622c5a \tArticle : Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning\n",
            "Author(s) : ['Guorui Feng', 'G. Huang', 'Qingping Lin', 'R. Gay']\n",
            "Year : 2009 \n",
            "Abstract : One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.\n",
            "\n",
            "Paper ID : 016fb4a9312b8b7fcf4fe1d96b85054cc52cc5cb \tArticle : Weka-A Machine Learning Workbench for Data Mining\n",
            "Author(s) : ['Eibe Frank', 'M. Hall', 'G. Holmes', 'Richard Kirkby', 'B. Pfahringer', 'I. Witten', 'Leonard E. Trigg']\n",
            "Year : 2005 \n",
            "Abstract : The Weka workbench is an organized collection of state-of-the-art machine learning algorithms and data preprocessing tools. The basic way of interacting with these methods is by invoking them from the command line. However, convenient interactive graphical user interfaces are provided for data exploration, for setting up large-scale experiments on distributed computing platforms, and for designing configurations for streamed data processing. These interfaces constitute an advanced environment for experimental data mining. The system is written in Java and distributed under the terms of the GNU General Public License.\n",
            "\n",
            "Paper ID : c2c4db02486ea139e8142295888ff7b075575fba \tArticle : Text Classification Using Machine Learning Techniques\n",
            "Author(s) : ['M. Ikonomakis', 'S. Kotsiantis', 'V. Tampakas']\n",
            "Year : 2005 \n",
            "Abstract : Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question- answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.\n",
            "------------------------------------Extracting Page #62------------------------------------\n",
            "\n",
            "Paper ID : 23c3953fb45536c9129e86ac7a23098bd9f1381d \tArticle : Machine Learning for Sequential Data: A Review\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 2002 \n",
            "Abstract : Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues.\n",
            "\n",
            "Paper ID : 9111e0a578ae34664dddcd76794bbe20e168a1ff \tArticle : Usilng Machine Learning Technliques to Identify Botnet Traffic\n",
            "Author(s) : ['C. Livadas', 'R. Walsh', 'D. Lapsley', 'W. Strayer']\n",
            "Year : 2006 \n",
            "Abstract : To date, techniques to counter cyber-attacks have predominantly been reactive; they focus on monitoring network traffic, detecting anomalies and cyber-attack traffic patterns, and, a posteriori, combating the cyber-attacks and mitigating their effects. Contrary to such approaches, we advocate proactively detecting and identifying botnets prior to their being used as part of a cyber-attack (Strayer et al., 2006). In this paper, we present our work on using machine learning-based classification techniques to identify the command and control (C2) traffic of IRC-based botnets - compromised hosts that are collectively commanded using Internet relay chat (IRC). We split this task into two stages: (I) distinguishing between IRC and non-IRC traffic, and (II) distinguishing between botnet and real IRC traffic. For stage I, we compare the performance of J48, naive Bayes, and Bayesian network classifiers, identify the features that achieve good overall classification accuracy, and determine the classification sensitivity to the training set size. While sensitive to the training data and the attributes used to characterize communication flows, machine learning-based classifiers show promise in identifying IRC traffic. Using classification in stage II is trickier, since accurately labeling IRC traffic as botnet and non-botnet is challenging. We are currently exploring labeling flows as suspicious and non-suspicious based on telltales of hosts being compromised\n",
            "\n",
            "Paper ID : 75e56ef7924972fde2ffc32d7071cd182d0f0f21 \tArticle : Selection of Relevant Features in Machine Learning\n",
            "Author(s) : ['P. Langley']\n",
            "Year : 1994 \n",
            "Abstract : In this paper, we review the problem of selecting rele- vant features for use in machine learning. We describe this problem in terms of heuristic search through a space of feature sets, and we identify four dimensions along which approaches to the problem can vary. We consider recent work on feature selection in terms of this framework, then close with some challenges for future work in the area. 1. The Problem of Irrelevant Features accuracy) to grow slowly with the number of irrele- vant attributes. Theoretical results for algorithms that search restricted hypothesis spaces are encouraging. For instance, the worst-case number of errors made by Littlestone's (1987) WINNOW method grows only logarithmically with the number of irrelevant features. Pazzani and Sarrett's (1992) average-case analysis for WHOLIST, a simple conjunctive algorithm, and Lang- ley and Iba's (1993) treatment of the naive Bayesian classifier, suggest that their sample complexities grow at most linearly with the number of irrelevant features. However, the theoretical results are less optimistic for induction methods that search a larger space of concept descriptions. For example, Langley and Iba's (1993) average-case analysis of simple nearest neighbor indicates that its sample complexity grows exponen- tially with the number of irrelevant attributes, even for conjunctive target concepts. Experimental stud- ies of nearest neighbor are consistent with this conclu- sion, and other experiments suggest that similar results hold even for induction algorithms that explicitly se- lect features. For example, the sample complexity for decision-tree methods appears to grow linearly with the number of irrelevants for conjunctive concepts, but exponentially for parity concepts, since the evaluation metric cannot distinguish relevant from irrelevant fea- tures in the latter situation (Langley & Sage, in press). Results of this sort have encouraged machine learn- ing researchers to explore more sophisticated methods for selecting relevant features. In the sections that fol- low, we present a general framework for this task, and then consider some recent examples of work on this important problem.\n",
            "\n",
            "Paper ID : e6276780b23ce2cff6e19397b5e726842f8a24d4 \tArticle : Ensemble Based Extreme Learning Machine\n",
            "Author(s) : ['Nan Liu', 'Han Wang']\n",
            "Year : 2010 \n",
            "Abstract : Extreme learning machine (ELM) was proposed as a new class of learning algorithm for single-hidden layer feedforward neural network (SLFN). To achieve good generalization performance, ELM minimizes training error on the entire training data set, therefore it might suffer from overfitting as the learning model will approximate all training samples well. In this letter, an ensemble based ELM (EN-ELM) algorithm is proposed where ensemble learning and cross-validation are embedded into the training phase so as to alleviate the overtraining problem and enhance the predictive stability. Experimental results on several benchmark databases demonstrate that EN-ELM is robust and efficient for classification.\n",
            "\n",
            "Paper ID : 7975195638f3574ac02975df6f4048558ec1bc96 \tArticle : Extreme learning machines: a survey\n",
            "Author(s) : ['G. Huang', 'Dianhui Wang', 'Y. Lan']\n",
            "Year : 2011 \n",
            "Abstract : Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.\n",
            "\n",
            "Paper ID : fc82788021963ff8e318ffe955829bc68e48943a \tArticle : Machine Learning of Temporal Relations\n",
            "Author(s) : ['I. Mani', 'M. Verhagen', 'Ben Wellner', 'Chong Min Lee', 'J. Pustejovsky']\n",
            "Year : 2006 \n",
            "Abstract : This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.\n",
            "\n",
            "Paper ID : 160a4786dd643d9f758b9cc0758bdd2581524941 \tArticle : Machine learning for detection and diagnosis of disease.\n",
            "Author(s) : ['P. Sajda']\n",
            "Year : 2006 \n",
            "Abstract : Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.\n",
            "\n",
            "Paper ID : 06757c457ec442eb35af6ea45d8d0e2339415178 \tArticle : The Interplay of Optimization and Machine Learning Research\n",
            "Author(s) : ['Kristin P. Bennett', 'E. Parrado-Hernández']\n",
            "Year : 2006 \n",
            "Abstract : The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.\n",
            "\n",
            "Paper ID : dc418bccc06fdf3873b0241c97af4219dfca794a \tArticle : Machine Learning in a Quantum World\n",
            "Author(s) : ['Esma Aïmeur', 'G. Brassard', 'S. Gambs']\n",
            "Year : 2006 \n",
            "Abstract : Quantum Information Processing (QIP) performs wonders in a world that obeys the laws of quantum mechanics, whereas Machine Learning (ML) is generally assumed to be done in a classical world. We initiate an investigation of the encounter of ML with QIP by defining and studying novel learning tasks that correspond to Machine Learning in a world in which the information is fundamentally quantum mechanical. We shall see that this paradigm shift has a profound impact on the learning process and that our classical intuition is often challenged.\n",
            "\n",
            "Paper ID : cfc04be0eaf30ad81dcaaada8d6e6171aa058432 \tArticle : Flow Clustering Using Machine Learning Techniques\n",
            "Author(s) : ['A. McGregor', 'M. Hall', 'Perry Lorier', 'James Brunskill']\n",
            "Year : 2004 \n",
            "Abstract : Packet header traces are widely used in network analysis. Header traces are the aggregate of traffic from many concurrent applications. We present a methodology, based on machine learning, that can break the trace down into clusters of traffic where each cluster has different traffic characteristics. Typical clusters include bulk transfer, single and multiple transactions and interactive traffic, amongst others. The paper includes a description of the methodology, a visualisation of the attribute statistics that aids in recognising cluster types and a discussion of the stability and effectiveness of the methodology.\n",
            "------------------------------------Extracting Page #63------------------------------------\n",
            "\n",
            "Paper ID : 78a907839eadb15730ed51b574009067a5ac394a \tArticle : Orange: From Experimental Machine Learning to Interactive Data Mining\n",
            "Author(s) : ['J. Demšar', 'B. Zupan', 'Gregor Leban', 'Tomaž Curk']\n",
            "Year : 2004 \n",
            "Abstract : Orange (www.ailab.si/orange) is a suite for machine learning and data mining. For researchers in machine learning, Orange offers scripting to easily prototype new algorithms and experimental procedures. For explorative data analysis, it provides a visual programming framework with emphasis on interactions and creative combinations of visual components.\n",
            "\n",
            "Paper ID : 29cedf9f9edd5179aa2e78654a7b81c9da45f0d0 \tArticle : Learning and optimization using the clonal selection principle\n",
            "Author(s) : ['L. Castro', 'F. V. Zuben']\n",
            "Year : 2002 \n",
            "Abstract : The clonal selection principle is used to explain the basic features of an adaptive immune response to an antigenic stimulus. It establishes the idea that only those cells that recognize the antigens (Ag's) are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective Ag's. This paper proposes a computational implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. The general algorithm, named CLONALG, is derived primarily to perform machine learning and pattern recognition tasks, and then it is adapted to solve optimization problems, emphasizing multimodal and combinatorial optimization. Two versions of the algorithm are derived, their computational cost per iteration is presented, and a sensitivity analysis in relation to the user-defined parameters is given. CLONALG is also contrasted with evolutionary algorithms. Several benchmark problems are considered to evaluate the performance of CLONALG and it is also compared to a niching method for multimodal function optimization.\n",
            "\n",
            "Paper ID : d91ea30f4f9de817b29bb4ece00f43cb971822b4 \tArticle : Machine Learning Benchmarks and Random Forest Regression\n",
            "Author(s) : ['M. Segal']\n",
            "Year : 2004 \n",
            "Abstract : Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called ‘random forests’. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.\n",
            "\n",
            "Paper ID : fb829c5e6b406bb325fa5a02e05073df12b1772b \tArticle : Classes of Kernels for Machine Learning: A Statistics Perspective\n",
            "Author(s) : ['M. Genton']\n",
            "Year : 2001 \n",
            "Abstract : In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.\n",
            "\n",
            "Paper ID : 49ca42e19bd1c647d099f9accd44e31e2e8e0b5a \tArticle : Machine-Learning Research\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 1997 \n",
            "Abstract : Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.\n",
            "\n",
            "Paper ID : 8b8570a9892c487ac05dc196e2bfe23faa34d99e \tArticle : Interactive machine learning\n",
            "Author(s) : ['Jerry Alan Fails', 'D. Olsen']\n",
            "Year : 2003 \n",
            "Abstract : Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers. We propose an interactive machine-learning (IML) model that allows users to train, classify/view and correct the classifications. The concept and implementation details of IML are discussed and contrasted with classical machine learning models. Evaluations of two algorithms are also presented. We also briefly describe Image Processing with Crayons (Crayons), which is a tool for creating new camera-based interfaces using a simple painting metaphor. The Crayons tool embodies our notions of interactive machine learning\n",
            "\n",
            "Paper ID : f5e04e45416dbfb66c3e132570f897d67f13d474 \tArticle : Using GPUs for machine learning algorithms\n",
            "Author(s) : ['Dave Steinkrau', 'P. Simard', 'I. Buck']\n",
            "Year : 2005 \n",
            "Abstract : Using dedicated hardware to do machine learning typically ends up in disaster because of cost, obsolescence, and poor software. The popularization of graphic processing units (GPUs), which are now available on every PC, provides an attractive alternative. We propose a generic 2-layer fully connected neural network GPU implementation which yields over 3/spl times/ speedup for both training and testing with respect to a 3 GHz P4 CPU.\n",
            "\n",
            "Paper ID : 463565c30b7a9c12c2ef0558a51cfc7b05055737 \tArticle : Semi-Supervised Learning (Adaptive Computation and Machine Learning)\n",
            "Author(s) : ['O. Chapelle', 'B. Schölkopf', 'A. Zien']\n",
            "Year : 2006 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 8d1b6888aa08b0a590c9e5d1c563d5e646f0505e \tArticle : Multi-Objective Machine Learning\n",
            "Author(s) : ['Yaochu Jin']\n",
            "Year : 2006 \n",
            "Abstract : Multi-Objective Clustering, Feature Extraction and Feature Selection.- Feature Selection Using Rough Sets.- Multi-Objective Clustering and Cluster Validation.- Feature Selection for Ensembles Using the Multi-Objective Optimization Approach.- Feature Extraction Using Multi-Objective Genetic Programming.- Multi-Objective Learning for Accuracy Improvement.- Regression Error Characteristic Optimisation of Non-Linear Models.- Regularization for Parameter Identification Using Multi-Objective Optimization.- Multi-Objective Algorithms for Neural Networks Learning.- Generating Support Vector Machines Using Multi-Objective Optimization and Goal Programming.- Multi-Objective Optimization of Support Vector Machines.- Multi-Objective Evolutionary Algorithm for Radial Basis Function Neural Network Design.- Minimizing Structural Risk on Decision Tree Classification.- Multi-objective Learning Classifier Systems.- Multi-Objective Learning for Interpretability Improvement.- Simultaneous Generation of Accurate and Interpretable Neural Network Classifiers.- GA-Based Pareto Optimization for Rule Extraction from Neural Networks.- Agent Based Multi-Objective Approach to Generating Interpretable Fuzzy Systems.- Multi-objective Evolutionary Algorithm for Temporal Linguistic Rule Extraction.- Multiple Objective Learning for Constructing Interpretable Takagi-Sugeno Fuzzy Model.- Multi-Objective Ensemble Generation.- Pareto-Optimal Approaches to Neuro-Ensemble Learning.- Trade-Off Between Diversity and Accuracy in Ensemble Generation.- Cooperative Coevolution of Neural Networks and Ensembles of Neural Networks.- Multi-Objective Structure Selection for RBF Networks and Its Application to Nonlinear System Identification.- Fuzzy Ensemble Design through Multi-Objective Fuzzy Rule Selection.- Applications of Multi-Objective Machine Learning.- Multi-Objective Optimisation for Receiver Operating Characteristic Analysis.- Multi-Objective Design of Neuro-Fuzzy Controllers for Robot Behavior Coordination.- Fuzzy Tuning for the Docking Maneuver Controller of an Automated Guided Vehicle.- A Multi-Objective Genetic Algorithm for Learning Linguistic Persistent Queries in Text Retrieval Environments.- Multi-Objective Neural Network Optimization for Visual Object Detection.\n",
            "\n",
            "Paper ID : 69ae9989ecc0f72a79b339bde11d68a53b9c7c46 \tArticle : Pattern Recognition and Machine Learning\n",
            "Author(s) : ['K. Fu']\n",
            "Year : 2006 \n",
            "Abstract : \n",
            "------------------------------------Extracting Page #64------------------------------------\n",
            "\n",
            "Paper ID : 8df9c71f09eb0dabf5adf17bee0f6b36190b52b2 \tArticle : Representational Learning with Extreme Learning Machine for Big Data Liyanaarachchi\n",
            "Author(s) : ['L. C. Kasun', 'Hongming Zhou', 'G. Huang', 'C. Vong']\n",
            "Year :  \n",
            "Abstract : Restricted Boltzmann Machines (RBM) and auto encoders, learns to represent features in a dataset meaningfully and used as the basic building blocks to create deep networks. This paper introduces Extreme Learning Machine based Auto Encoder (ELM-AE), which learns feature representations using singular values and is used as the basic building block for Multi Layer Extreme Learning Machine (ML-ELM). ML-ELM performance is better than auto encoders based deep networks and Deep Belief Networks (DBN), while in par with Deep Boltzmann Machines (DBM) for MNIST dataset. However MLELM is significantly faster than any state−of−the−art deep networks.\n",
            "\n",
            "Paper ID : 32d12b621e75cdb5942896c16421496bdfd4a6fa \tArticle : Ontology Matching: A Machine Learning Approach\n",
            "Author(s) : ['A. Doan', 'J. Madhavan', 'Pedro M. Domingos', 'A. Halevy']\n",
            "Year : 2004 \n",
            "Abstract : This chapter studies ontology matching: the problem of finding the semantic mappings between two given ontologies. This problem lies at the heart of numerous information processing applications. Virtually any application that involves multiple ontologies must establish semantic mappings among them, to ensure interoperability. Examples of such applications arise in myriad domains, including e-commerce, knowledge management, e-learning, information extraction, bio-informatics, web services, and tourism (see Part D of this book on ontology applications).\n",
            "\n",
            "Paper ID : 9874b4cfd9e8ef89fd0b753af18c14cbc7c42744 \tArticle : What do you mean by collaborative learning\n",
            "Author(s) : ['P. Dillenbourg']\n",
            "Year : 1999 \n",
            "Abstract : This book arises from a series of workshops on collaborative learning, that gathered together 20 scholars from the disciplines of psychology, education and computer science. The series was part of a research program entitled 'Learning in Humans and Machines' (LHM), launched by Peter Reimann and Hans Spada, and funded by the European Science Foundation. This program aimed to develop a multidisciplinary dialogue on learning, involving mainly scholars from cognitive psychology, educational science, and artificial intelligence (including machine learning). During the preparation of the program, Agnes Blaye, Claire O'Malley, Michael Baker and I developed a theme on collaborative learning. When the program officially began, 12 members were selected to work on this theme and formed the so-called 'task force 5'. I became the coordinator of the group. This group organised two workshops, in Sitges (Spain, 1994) and Aix-en-Provence (France, 1995). In 1996, the group was enriched with new members to reach its final size. Around 20 members met in the subsequent workshops, at Samoens (France, 1996), Houthalen (Belgium, 1996) and Mannheim (Germany, 1997). Several individuals joined the group for some time but have not written a chapter. I would nevertheless like to acknowledge their contributions to our activities: George Bilchev, Stevan Harnad, Calle Jansson and Claire O'Malley.\n",
            "\n",
            "Paper ID : 1f7ed2ebc6b641e3804cf177fd42a1b8de95003b \tArticle : Machine-learning techniques and their applications in manufacturing\n",
            "Author(s) : ['D. T. Pham', 'A. Afify']\n",
            "Year : 2005 \n",
            "Abstract : Abstract Machine learning is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Manufacturing is an area where the application of machine learning can be very fruitful. However, little has been published about the use of machine-learning techniques in the manufacturing domain. This paper evaluates several machine-learning techniques and examines applications in which they have been successfully deployed. Special attention is given to inductive learning, which is among the most mature of the machine-learning approaches currently available. Current trends and recent developments in machine-learning research are also discussed. The paper concludes with a summary of some of the key research issues in machine learning.\n",
            "\n",
            "Paper ID : e45ead3629cdda9ab60a28b6040623ea84fc1a31 \tArticle : Machine Learning for User Modeling\n",
            "Author(s) : ['Geoffrey I. Webb', 'M. Pazzani', 'Daniel Billsus']\n",
            "Year : 2004 \n",
            "Abstract : At first blush, user modeling appears to be a prime candidate for straightforward application of standard machine learning techniques. Observations of the user's behavior can provide training examples that a machine learning system can use to form a model designed to predict future actions. However, user modeling poses a number of challenges for machine learning that have hindered its application in user modeling, including: the need for large data sets; the need for labeled data; concept drift; and computational complexity. This paper examines each of these issues and reviews approaches to resolving them.\n",
            "\n",
            "Paper ID : 2ff6fcebe5561433a2d2abeb8b30b2fbf3c0e303 \tArticle : Introduction to Semi-Supervised Learning\n",
            "Author(s) : ['Xiaojin Zhu', 'A. Goldberg']\n",
            "Year : 2009 \n",
            "Abstract : Semi-supervised learning is a learning paradigm concerned with the study of how computers and natural systems such as humans learn in the presence of both labeled and unlabeled data. Traditionally, learning has been studied either in the unsupervised paradigm (e.g., clustering, outlier detection) where all the data is unlabeled, or in the supervised paradigm (e.g., classification, regression) where all the data is labeled.The goal of semi-supervised learning is to understand how combining labeled and unlabeled data may change the learning behavior, and design algorithms that take advantage of such a combination. Semi-supervised learning is of great interest in machine learning and data mining because it can use readily available unlabeled data to improve supervised learning tasks when the labeled data is scarce or expensive. Semi-supervised learning also shows potential as a quantitative tool to understand human category learning, where most of the input is self-evidently unlabeled. In this introductory book, we present some popular semi-supervised learning models, including self-training, mixture models, co-training and multiview learning, graph-based methods, and semi-supervised support vector machines. For each model, we discuss its basic mathematical formulation. The success of semi-supervised learning depends critically on some underlying assumptions. We emphasize the assumptions made by each model and give counterexamples when appropriate to demonstrate the limitations of the different models. In addition, we discuss semi-supervised learning for cognitive psychology. Finally, we give a computational learning theoretic perspective on semi-supervised learning, and we conclude the book with a brief discussion of open questions in the field.\n",
            "\n",
            "Paper ID : 629cc74dcaf655feea40f64cd74617ac884ed0f8 \tArticle : Graphical Models for Machine Learning and Digital Communication\n",
            "Author(s) : ['B. Frey']\n",
            "Year : 1998 \n",
            "Abstract : Probabilistic inference in graphical models pattern classification unsupervised learning data compression channel coding future research directions.\n",
            "\n",
            "Paper ID : 22d6d9c1b7ac2738b51d93be45ac8f753f81867c \tArticle : Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data\n",
            "Author(s) : ['Hoo-Chang Shin', 'M. Orton', 'D. Collins', 'S. Doran', 'M. Leach']\n",
            "Year : 2013 \n",
            "Abstract : Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.\n",
            "\n",
            "Paper ID : ae73fa99d777efd07ed5a73cdc695191862f9d9e \tArticle : Drug Design by Machine Learning: Support Vector Machines for Pharmaceutical Data Analysis\n",
            "Author(s) : ['R. Burbidge', 'M. Trotter', 'B. Buxton', 'S. Holden']\n",
            "Year : 2002 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 464d94b3dc9a109dd64008a41a00181830f285aa \tArticle : Torch: a modular machine learning software library\n",
            "Author(s) : ['Ronan Collobert', 'Samy Bengio', 'J. Mariéthoz']\n",
            "Year : 2002 \n",
            "Abstract : Keywords: learning Reference EPFL-REPORT-82802 URL: http://publications.idiap.ch/downloads/reports/2002/rr02-46.pdf Record created on 2006-03-10, modified on 2017-05-10\n",
            "------------------------------------Extracting Page #65------------------------------------\n",
            "\n",
            "Paper ID : 07abd02f02774d178f26ca99937e5f94001a9ec9 \tArticle : Learning to link with wikipedia\n",
            "Author(s) : ['David N. Milne', 'I. Witten']\n",
            "Year : 2008 \n",
            "Abstract : This paper describes how to automatically cross-reference documents with Wikipedia: the largest knowledge base ever known. It explains how machine learning can be used to identify significant terms within unstructured text, and enrich it with links to the appropriate Wikipedia articles. The resulting link detector and disambiguator performs very well, with recall and precision of almost 75%. This performance is constant whether the system is evaluated on Wikipedia articles or \"real world\" documents.\n",
            " This work has implications far beyond enriching documents with explanatory links. It can provide structured knowledge about any unstructured fragment of text. Any task that is currently addressed with bags of words - indexing, clustering, retrieval, and summarization to name a few - could use the techniques described here to draw on a vast network of concepts and semantics.\n",
            "\n",
            "Paper ID : 58de638505046e7de5fe7cc0660b4c6d79247488 \tArticle : Machine Learning for Information Extraction in Informal Domains\n",
            "Author(s) : ['D. Freitag']\n",
            "Year : 2004 \n",
            "Abstract : We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems.\n",
            "\n",
            "Paper ID : e4de0f69cd867dbcae88211ac05318be17615a66 \tArticle : Regularized Extreme Learning Machine\n",
            "Author(s) : ['W. Deng', 'Qinghua Zheng', 'Lin Chen']\n",
            "Year : 2009 \n",
            "Abstract : Extreme Learning Machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called Regularized Extreme Learning Machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.\n",
            "\n",
            "Paper ID : c565b6750a7c4c122695daab8463c53b818699a6 \tArticle : Elements of Machine Learning\n",
            "Author(s) : ['P. Langley']\n",
            "Year : 1994 \n",
            "Abstract : Elements of Machine Learning by Pat Langley Preface 1. An overview of machine learning 1.1 The science of machine learning 1.2 Nature of the environment 1.3 Nature of representation and performance 1.4 Nature of the learning component 1.5 Five paradigms for machine learning 1.6 Summary of the chapter 2. The induction of logical conjunctions 2.1 General issues in logical induction 2.2 Nonincremental induction of logical conjunctions 2.3 Heuristic induction of logical conjunctions 2.4 Incremental induction of logical conjunctions 2.5 Incremental hill climbing for logical conjunctions 2.6 Genetic algorithms for logical concept induction 2.7 Summary of the chapter 3. The induction of threshold concepts 3.1 General issues for threshold concepts 3.2 Induction of criteria tables 3.3 Induction of linear threshold units 3.4 Induction of spherical threshold units 3.5 Summary of the chapter 4. The induction of competitive concepts 4.1 Instance-based learning 4.2 Learning probabilistic concept descriptions 4.3 Summary of the chapter 5. The construction of decision lists 5.1 General issues in disjunctive concept induction 5.2 Nonincremental learning using separate and conquer 5.3 Incremental induction using separate and conquer 5.4 Induction of decision lists through exceptions 5.5 Induction of competitive disjunctions 5.6 Instance-storing algorithms 5.7 Complementary beam search for disjunctive concepts 5.8 Summary of the chapter 6. Revision and extension of inference networks 6.1 General issues surrounding inference network 6.2 Extending an incomplete inference network 6.3 Inducing specialized concepts with inference networks 6.4 Revising an incorrect inference network 6.5 Network construction and term generation 6.6 Summary of the chapter 7. The formation of concept hierarchies 7.1 General issues concerning concept hierarchies 7.2 Nonincremental divisive formation of hierarchies 7.3 Incremental formation of concept hierarchies 7.4 Agglomerative formation of concept hierarchies 7.5 Variations on hierarchies into other structures 7.7 Summary of the chapter 8. Other issues in concept induction 8.1 Overfitting and pruning 8.2 Selecting useful features 8.3 Induction for numeric prediction 8.4 Unsupervised concept induction 8.5 Inducing relational concepts 8.6 Handling missing features 8.7 Summary of the chapter 9. The formation of transition networks 9.1 General issues for state-transition networks 9.2 Constructing finite-state transition networks 9.3 Forming recursive transition networks 9.4 Learning rules and networks for prediction 9.5 Summary of the chapter 10. The acquisition of search-control knowledge 10.1 General issues in search control 10.2 Reinforcement learning 10.3 Learning state-space heuristics from solution traces 10.4 Learning control knowledge for problem reduction 10.5 Learning control knowledge for means-ends analysis 10.6 The utility of search-control knowledge 10.7 Summary of the chapter 11. The formation of macro-operators 11.1 General issues related to macro-operators 11.2 The creation of simple macro-operators 11.3 The formation of flexible macro-operators 11.4 Problem solving by analogy 11.5 The utility of macro-operators 11.6 Summary of the chapter 12. Prospects for machine learning 12.1 Additional areas of machine learning 12.2 Methodological trends in machine learning 12.3 The future of machine learning References Index\n",
            "\n",
            "Paper ID : 554894f70b28dba58b396c2d84080ac01051261b \tArticle : Gaussian Processes For Machine Learning\n",
            "Author(s) : ['M. Seeger']\n",
            "Year : 2004 \n",
            "Abstract : Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other \"kernel machines\" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.\n",
            "\n",
            "Paper ID : ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6 \tArticle : Practical feature subset selection for machine learning\n",
            "Author(s) : ['M. Hall', 'L. A. Smith']\n",
            "Year : 1998 \n",
            "Abstract : Machine learning algorithms automatically extract knowledge from machine readable information. Unfortunately, their success is usually dependant on the quality of the data that they operate on. If the data is inadequate, or contains extraneous and irrelevant information, machine learning algorithms may produce less accurate and less understandable results, or may fail to discover anything of use at all. Feature subset selectors are algorithms that attempt to identify and remove as much irrelevant and redundant information as possible prior to learning. Feature subset selection can result in enhanced performance, a reduced hypothesis search space, and, in some cases, reduced storage requirement. This paper describes a new feature selection algorithm that uses a correlation based heuristic to determine the “goodness” of feature subsets, and evaluates its effectiveness with three common machine learning algorithms. Experiments using a number of standard machine learning data sets are presented. Feature subset selection gave significant improvement for all three algorithms.\n",
            "\n",
            "Paper ID : cf171d57f8232ba90a0696f8cb46144b39380d0b \tArticle : Bioinformatics - The Machine Learning Approach\n",
            "Author(s) : ['G. Grant']\n",
            "Year : 2000 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da \tArticle : Applications of machine learning and rule induction\n",
            "Author(s) : ['P. Langley', 'H. Simon']\n",
            "Year : 1995 \n",
            "Abstract : Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain-specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide increasing levels of automation in the knowledge engineering process, replacing much time-consuming human activity with automatic techniques that improve accuracy or efficiency by discovering and exploiting regularities in training data. The ultimate test of machine learning is its ability to produce systems that are used regularly in industry, education, and elsewhere.\n",
            "\n",
            "Paper ID : 467568f1777bc51a15a5100516cd4fe8de62b9ab \tArticle : Transfer Learning for Reinforcement Learning Domains: A Survey\n",
            "Author(s) : ['Matthew E. Taylor', 'P. Stone']\n",
            "Year : 2009 \n",
            "Abstract : The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.\n",
            "\n",
            "Paper ID : 7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724 \tArticle : Machine Learning Approaches to Estimating Software Development Effort\n",
            "Author(s) : ['K. Srinivasan', 'D. Fisher']\n",
            "Year : 1995 \n",
            "Abstract : Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data. >\n",
            "------------------------------------Extracting Page #66------------------------------------\n",
            "\n",
            "Paper ID : cf171d57f8232ba90a0696f8cb46144b39380d0b \tArticle : Bioinformatics - The Machine Learning Approach\n",
            "Author(s) : ['G. Grant']\n",
            "Year : 2000 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da \tArticle : Applications of machine learning and rule induction\n",
            "Author(s) : ['P. Langley', 'H. Simon']\n",
            "Year : 1995 \n",
            "Abstract : Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain-specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide increasing levels of automation in the knowledge engineering process, replacing much time-consuming human activity with automatic techniques that improve accuracy or efficiency by discovering and exploiting regularities in training data. The ultimate test of machine learning is its ability to produce systems that are used regularly in industry, education, and elsewhere.\n",
            "\n",
            "Paper ID : 467568f1777bc51a15a5100516cd4fe8de62b9ab \tArticle : Transfer Learning for Reinforcement Learning Domains: A Survey\n",
            "Author(s) : ['Matthew E. Taylor', 'P. Stone']\n",
            "Year : 2009 \n",
            "Abstract : The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.\n",
            "\n",
            "Paper ID : 7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724 \tArticle : Machine Learning Approaches to Estimating Software Development Effort\n",
            "Author(s) : ['K. Srinivasan', 'D. Fisher']\n",
            "Year : 1995 \n",
            "Abstract : Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data. >\n",
            "\n",
            "Paper ID : 3ecdaaa55313520b50ae17de9f4f6650403754a3 \tArticle : Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993\n",
            "Author(s) : ['S. Salzberg']\n",
            "Year : 2004 \n",
            "Abstract : Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.\n",
            "\n",
            "Paper ID : b9af24fa3faf6dbdc5e952857697588708fad8f5 \tArticle : Guest Editors' Introduction: On Applied Research in Machine Learning\n",
            "Author(s) : ['F. Provost', 'Ron Kohavi']\n",
            "Year : 2004 \n",
            "Abstract : Common arguments for including applications papers in the Machine Learning literatureare often based on the papers’ value for advertising success stories and for boosting morale.Forexample,high-proﬁleapplicationscanhelptosecurefundingforfutureresearchandcanhelp to attract high caliber students. However, there is another reason why such papers areof value to the ﬁeld, which is, arguably, even more vital. Application papers are essential inorder for Machine Learning to remain a viable science. They focus research on importantunsolved problems that currently restrict the practical applicability of machine learningmethods.Muchofthe“science”ofMachineLearningisascienceofengineering.\n",
            "\n",
            "Paper ID : ea2c5c6e84ce5eaea52ecb8bc01a738005cf2092 \tArticle : LEARNABLE EVOLUTION MODEL: Evolutionary Processes Guided by Machine Learning\n",
            "Author(s) : ['R. Michalski']\n",
            "Year : 2004 \n",
            "Abstract : A new class of evolutionary computation processes is presented, called Learnable Evolution Model or LEM. In contrast to Darwinian-type evolution that relies on mutation, recombination, and selection operators, LEM employs machine learning to generate new populations. Specifically, in Machine Learning mode, a learning system seeks reasons why certain individuals in a population (or a collection of past populations) are superior to others in performing a designated class of tasks. These reasons, expressed as inductive hypotheses, are used to generate new populations. A remarkable property of LEM is that it is capable of quantum leaps (“insight jumps”) of the fitness function, unlike Darwinian-type evolution that typically proceeds through numerous slight improvements. In our early experimental studies, LEM significantly outperformed evolutionary computation methods used in the experiments, sometimes achieving speed-ups of two or more orders of magnitude in terms of the number of evolutionary steps. LEM has a potential for a wide range of applications, in particular, in such domains as complex optimization or search problems, engineering design, drug design, evolvable hardware, software engineering, economics, data mining, and automatic programming.\n",
            "\n",
            "Paper ID : 5ee8a371fc5adc5469435020a52fb815f3b57a71 \tArticle : Semi-Supervised Learning\n",
            "Author(s) : ['O. Chapelle', 'Bernhard Schlkopf', 'A. Zien']\n",
            "Year : 2006 \n",
            "Abstract : In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series\n",
            "\n",
            "Paper ID : b8eb7da56dae58f77788c33a57b5b810ca930527 \tArticle : The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics\n",
            "Author(s) : ['Peter A. Flach']\n",
            "Year : 2003 \n",
            "Abstract : Many different metrics are used in machine learning and data mining to build and evaluate models. However, there is no general theory of machine learning metrics, that could answer questions such as: When we simultaneously want to optimise two criteria, how can or should they be traded off? Some metrics are inherently independent of class and misclassification cost distributions, while other are not -- can this be made more precise? This paper provides a derivation of ROC space from first principles through 3D ROC space and the skew ratio, and redefines metrics in these dimensions. The paper demonstrates that the graphical depiction of machine learning metrics by means of ROC isometrics gives many useful insights into the characteristics of these metrics, and provides a foundation on which a theory of machine learning metrics can be built.\n",
            "\n",
            "Paper ID : 158edd3fe6212306487e7173a5de9383a55b59bb \tArticle : Advanced Lectures on Machine Learning\n",
            "Author(s) : ['O. Bousquet', 'U. V. Luxburg', 'G. Rätsch']\n",
            "Year : 2004 \n",
            "Abstract : This chapter describes Lagrange multipliers and some selected subtopics from matrix analysis from a machine learning perspective. The goal is to give a detailed description of a number of mathematical constructions that are widely used in applied machine learning.\n",
            "------------------------------------Extracting Page #67------------------------------------\n",
            "\n",
            "Paper ID : ca799d07d8508df7949e7bea1c353d2a987a2417 \tArticle : Machine Learning for Fast Quadrupedal Locomotion\n",
            "Author(s) : ['Nate Kohl', 'P. Stone']\n",
            "Year : 2004 \n",
            "Abstract : For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform.\n",
            "\n",
            "Paper ID : 50ec005395794592f6c977f6d273635ef563c241 \tArticle : Collaborative Filtering: A Machine Learning Perspective\n",
            "Author(s) : ['Benjamin M Marlin']\n",
            "Year : 2004 \n",
            "Abstract : Collaborative Filtering: A Machine Learning Perspective Benjamin Marlin Master of Science Graduate Department of Computer Science University of Toronto 2004 Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modifications of one or more standard machine learning methods for classification, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.\n",
            "\n",
            "Paper ID : b54c5dd03ca2c0b9b17b9fc1d4ebe4c2b6478982 \tArticle : MACHINE LEARNING TECHNIQUES FOR BRAIN-COMPUTER INTERFACES\n",
            "Author(s) : ['K.-R. Muller', 'M. Krauledat', 'G. Dornhege', 'G. Curio', 'B. Blankertz']\n",
            "Year : 2004 \n",
            "Abstract : This review discusses machine learning methods and their application to Brain-Computer Interfacing. A particular focus is placed on feature selection. We also point out common flaws when validating machine learning methods in the context of BCI. Finally we provide a brief overview on the Berlin-Brain Computer Interface (BBCI).\n",
            "\n",
            "Paper ID : 43456f4e5f2a7c56b781afe2e2d4e4aed297ceb0 \tArticle : Bayesian Inference: An Introduction to Principles and Practice in Machine Learning\n",
            "Author(s) : ['Michael E. Tipping']\n",
            "Year : 2003 \n",
            "Abstract : This article gives a basic introduction to the principles of Bayesian inference in a machine learning context, with an emphasis on the importance of marginalisation for dealing with uncertainty. We begin by illustrating concepts via a simple regression task before relating ideas to practical, contemporary, techniques with a description of ‘sparse Bayesian’ models and the ‘relevance vector machine’.\n",
            "\n",
            "Paper ID : b379c5eb2f8cc501e855d295fa5712294ca2b3ed \tArticle : Application of Machine Learning Algorithms to KDD Intrusion Detection Dataset within Misuse Detection Context\n",
            "Author(s) : ['Maheshkumar Sabhnani', 'G. Serpen']\n",
            "Year : 2003 \n",
            "Abstract : A small subset of machine learning algorithms, mostly inductive learning based, applied to the KDD 1999 Cup intrusion detection dataset resulted in dismal performance for user-to-root and remote-to-local attack categories as reported in the recent literature. The uncertainty to explore if other machine learning algorithms can demonstrate better performance compared to the ones already employed constitutes the motivation for the study reported herein. Specifically, exploration of if certain algorithms perform better for certain attack classes and consequently, if a multi-expert classifier design can deliver desired performance measure is of high interest. This paper evaluates performance of a comprehensive set of pattern recognition and machine learning algorithms on four attack categories as found in the KDD 1999 Cup intrusion detection dataset. Results of simulation study implemented to that effect indicated that certain classification algorithms perform better for certain attack categories: a specific algorithm specialized for a given attack category . Consequently, a multi-classifier model, where a specific detection algorithm is associated with an attack category for which it is the most promising, was built. Empirical results obtained through simulation indicate that noticeable performance improvement was achieved for probing, denial of service, and user-to-root\n",
            "\n",
            "Paper ID : d4849d29b27a8e5230edafe4cd4cdda395fb24d4 \tArticle : Meta optimization: improving compiler heuristics with machine learning\n",
            "Author(s) : ['M. Stephenson', 'Saman P. Amarasinghe', 'M. Martin', \"U. O'Reilly\"]\n",
            "Year : 2003 \n",
            "Abstract : Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-specific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23% (up to 73%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25% on our training set, and 9% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.\n",
            "\n",
            "Paper ID : 58e03fdbc096684dd3185ec538e082dc6cc6a464 \tArticle : Experiments with random projections for machine learning\n",
            "Author(s) : ['Dmitriy Fradkin', 'D. Madigan']\n",
            "Year : 2003 \n",
            "Abstract : Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.\n",
            "\n",
            "Paper ID : 5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c \tArticle : Readings in Machine Learning\n",
            "Author(s) : ['J. Shavlik', 'T. Deitterich', 'Thomas G. Dietterich']\n",
            "Year : 1991 \n",
            "Abstract : From the Publisher: \n",
            "The ability to learn is a fundamental characteristic of intelligent behavior. Consequently, machine learning has been a focus of artificial intelligence since the beginnings of AI in the 1950s. The 1980s saw tremendous growth in the field, and this growth promises to continue with valuable contributions to science, engineering, and business. \n",
            " \n",
            "Readings in Machine Learning collects the best of the published machine learning literature, including papers that address a wide range of learning tasks, and that introduce a variety of techniques for giving machines the ability to learn. The editors, in cooperation with a group of expert referees, have chosen important papers that empirically study, theoretically analyze, or psychologically justify machine learning algorithms. The papers are grouped into a dozen categories, each of which is introduced by the editors.\n",
            "\n",
            "Paper ID : 50a869bcd6d45ec7fdb317877c3d2a047c2cfc38 \tArticle : Overfitting and undercomputing in machine learning\n",
            "Author(s) : ['Thomas G. Dietterich']\n",
            "Year : 1995 \n",
            "Abstract : A central problem in machine learning is supervised learning—that is, learning from labeled training data. For example, a learning system for medical diagnosis might be trained with examples of patients whose case records (medical tests, clinical observations) and diagnoses were known. The task of the learning system is to infer a function that predicts the diagnosis of a patient from his or her case records. The function to be learned might be represented as a set of rules, a decision tree, a Bayes network, or a neural network. Learning algorithms essentially operate by searching some space of functions (usually called the hypothesis class) for a function that fits the given data. Because there are usually exponentially many functions, this search cannot actually examine individual hypothesis functions but instead must use some more direct method of constructing the hypothesis functions from the data. This search can usually be formalized by defining an objective function (e.g., number of data points predicted incorrectly) and applying various algorithms to find a function that minimizes this objective function is NP-hard. For example, fitting the weights of a neural network or finding the smallest decision tree are both NP-complete problems [Blum and Rivest, 1989; Quinlan and Rivest 1989]. Hence, heuristic algorithms such as gradient descent (for neural networks) and greedy search (for decision trees) have been applied with great success. Of course, the suboptimality of such heuristic algorithms ~mmediately suggests a reas&able line of research: find ~lgorithms that can search the hypothesis class better. Hence, there has been extensive research in applying secondorder methods to fit neural networks and in conducting much more thorough searches in learning decision trees and rule sets. Ironically, when these algorithms were tested on real datasets, it was found that their performance was often worse than simrde szradient descent or greedy sear~h [&inlan and Cameron-Jones 1995; Weigend 1994]. In short: it appears to be bet~er not to optimize! One of the other important trends in machine-learning research has been the establishment and nurturing of connections between various previously disparate fields, including computational learning theory, connectionist learning, symbolic learning. and statistics. The . connection to statistics was crucial in resolvins$ this naradox. The-key p~oblem arises from the structure of the machine-learning task, A learning algorithm is trained on a set of training data, but then it is applied to make predictions on new data points. The goal is to maximize its predictive accuracy on the new data points—not necessarily its accuracy on the trammg data. Indeed, if we work too hard to find the very best fit to the training data, there is a risk that we will fit the noise in the data by memorizing various peculiarities\n",
            "\n",
            "Paper ID : 7b7222ac076d211d7fcae7d012bebcc4ea71e952 \tArticle : An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods\n",
            "Author(s) : ['S. Weiss', 'I. Kapouleas']\n",
            "Year : 1989 \n",
            "Abstract : Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty; there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.\n",
            "------------------------------------Extracting Page #68------------------------------------\n",
            "\n",
            "Paper ID : c6efa1ce0177d6990ad6580013e04b281c4a5295 \tArticle : Machine learning for science: state of the art and future prospects.\n",
            "Author(s) : ['E. Mjolsness', 'D. DeCoste']\n",
            "Year : 2001 \n",
            "Abstract : Recent advances in machine learning methods, along with successful applications across a wide variety of fields such as planetary science and bioinformatics, promise powerful new tools for practicing scientists. This viewpoint highlights some useful characteristics of modern machine learning methods and their relevance to scientific applications. We conclude with some speculations on near-term progress and promising directions.\n",
            "\n",
            "Paper ID : dd7cee21074ea6b346011d7463f7387ad9bfcc2a \tArticle : Information Extraction from HTML: Application of a General Machine Learning Approach\n",
            "Author(s) : ['D. Freitag']\n",
            "Year : 1998 \n",
            "Abstract : Because the World Wide Web consists primarily of text, information extraction is central to any effort that would use the Web as a resource for knowledge discovery. We show how information extraction can be cast as a standard machine learning problem, and argue for the suitability of relational learning in solving it. The implementation of a general-purpose relational learner for information extraction, SRV, is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document structure and the kinds of information available for use in learning extraction patterns. Instead, structural and other information is supplied as input in the form of an extensible token-oriented feature set. We demonstrate the effectiveness of this approach by adapting SRV for use in learning extraction rules for a domain consisting of university course and research project pages sampled from the Web. Making SRV Web-ready only involves adding several simple HTML-specific features to its basic feature set.\n",
            "\n",
            "Paper ID : 180882b3f2ea5dfe0a554deead2c0ceb837ee933 \tArticle : Machine learning as an experimental science\n",
            "Author(s) : ['P. Langley']\n",
            "Year : 2004 \n",
            "Abstract : Machine learning is a scientific discipline and, like the fields of AI and computer science, has both theoretical and empirical aspects. Although recent progress has occurred on the theoretical front (see Machine Learning, volulne 2, number 4), most learning algorithms are too complex for formal analysis. Thus, the field promises to have a significant empirical component for the foreseeable future. And unlike some empirical sciences, machine learning is fortunate enough to have experimental control over a wide range of factors, making it more akin to physics and chemistry than astronomy or sociology.\n",
            "\n",
            "Paper ID : 5052871d430b803b4c59f459bb26b1f76e56736e \tArticle : Evaluation and selection of biases in machine learning\n",
            "Author(s) : ['D. Spears', 'M. desJardins']\n",
            "Year : 2004 \n",
            "Abstract : In this introduction, we define the termbias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating and selecting biases using a framework of bias selection as search in bias and meta-bias spaces. Recent research in the field of machine learning bias is summarized.\n",
            "\n",
            "Paper ID : 3efcb97c1de1c87832a7a1d99e91801992a938ec \tArticle : Crafting Papers on Machine Learning\n",
            "Author(s) : ['P. Langley']\n",
            "Year : 2000 \n",
            "Abstract : This essay gives advice to authors of papers on machine learning, although much of it carries over to other computational disciplines. The issues covered include the material that should appear in a well-balanced paper, factors that arise in diierent approaches to evaluation , and ways to improve a submission's ability to communicate ideas to its readers.\n",
            "\n",
            "Paper ID : 07dc576e87368680e1cfafe9336853979168c2bd \tArticle : An application of machine learning to network intrusion detection\n",
            "Author(s) : ['Chris Sinclair', 'L. Pierce', 'S. Matzner']\n",
            "Year : 1999 \n",
            "Abstract : Differentiating anomalous network activity from normal network traffic is difficult and tedious. A human analyst must search through vast amounts of data to find anomalous sequences of network connections. To support the analyst's job, we built an application which enhances domain knowledge with machine learning techniques to create rules for an intrusion detection expert system. We employ genetic algorithms and decision trees to automatically generate rules for classifying network connections. This paper describes the machine learning methodology and the applications employing this methodology.\n",
            "\n",
            "Paper ID : 701d2f7c45277e74cc1f047d7681010a233ecc66 \tArticle : An Application of Machine Learning to Anomaly Detection\n",
            "Author(s) : ['T. Lane']\n",
            "Year : 1999 \n",
            "Abstract : The anomaly detection problem has been widely studied in the computer security literature. In this paper we present a machine learning approach to anomaly detection. Our system builds user profiles based on command sequences and compares current input sequences to the profile using a similarity measure. The system must learn to classify current behavior as consistent or anomalous with past behavior using only positive examples of the account's valid user. Our empirical results demonstrate that this is a promising approach to distinguishing the legitamate user from an intruder.\n",
            "\n",
            "Paper ID : 1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425 \tArticle : A Machine Learning Approach to Workflow Management\n",
            "Author(s) : ['J. Herbst']\n",
            "Year : 2000 \n",
            "Abstract : There has recently been some interest in applying machine learning techniques to support the acquisition and adaptation of workflow models. The different learning algorithms, that have been proposed, share some restrictions, which may prevent them from being used in practice. Approaches applying techniques from grammatical inference are restricted to sequential workflows. Other algorithms allowing concurrency require unique activity nodes. This contribution shows how the basic principle of our previous approach to sequential workflow induction can be generalized, so that it is able to deal with concurrency. It does not require unique activity nodes. The presented approach uses a log-likelihood guided search in the space of workflow models, that starts with a most general workflow model containing unique activity nodes. Two split operators are available for specialization.\n",
            "\n",
            "Paper ID : 9a4da6802d1eca17a07ded017a87bef7001189c4 \tArticle : Introduction to Machine Learning (Adaptive Computation and Machine Learning)\n",
            "Author(s) : ['Ethem Alpaydin']\n",
            "Year : 2004 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : c5ca3b6ad0e74ab8a83d77ebef0c5a498ba5d781 \tArticle : On-line Algorithms in Machine Learning\n",
            "Author(s) : ['A. Blum']\n",
            "Year : 1996 \n",
            "Abstract : Abstract : The areas of On-Line Algorithms and Machine Learning are both concerned with problems of making decisions about the present based only on knowledge of the past. Although these areas differ in terms of their emphasis and the problems typically studied, there are a collection of results in Computational Learning Theory that fit nicely into the 'on-line algorithms' framework. This survey article discusses some of the results, models, and open problems from Computational Learning Theory that seem particularly interesting from the point of view of on-line algorithms research. The emphasis in this article is on describing some of the simpler, more intuitive results, whose proofs can be given in their entirety. Pointers to the literature are given for more sophisticated versions of these algorithms.\n",
            "------------------------------------Extracting Page #69------------------------------------\n",
            "\n",
            "Paper ID : d50c8fd5448e4149b470aeb92eefb270088adc44 \tArticle : Machine Learning Methods for Ecological Applications\n",
            "Author(s) : ['A. Fielding']\n",
            "Year : 1999 \n",
            "Abstract : From the Publisher: \n",
            "This is the first text aimed at introducing machine learning methods to a readership of professional ecologists. All but one chapter have been written by ecologists and biologists who highlight the application of a particular method to a particular class of problem. Examples include the identification of species, optimal mate choice, predicting species distributions and modelling landscape features. A group of experienced machine learning workers, who have become interested in environmental problems, have written a chapter that demonstrates how machine learning methods can be used to discover equations that describe the dynamic behaviour of ecological systems. The final chapter reviews 'real learning', offering the potential for greater dialogue between the biological and machine learning communities.\n",
            "\n",
            "Paper ID : f4971ff0f6ae626e78131bafa012eadfe8e238e2 \tArticle : Machine learning: an artificial intelligence approach volume III\n",
            "Author(s) : ['Y. Kodratoff', 'R. Michalski']\n",
            "Year : 1990 \n",
            "Abstract : This book reflects the expansion of machine learning research through presentation of recent advances in the field. The book provides an account of current research directions. Major topics covered include the following: learning concepts and rules from examples; cognitive aspects of learning; learning by analogy; learning by observation and discovery; and an exploration of general aspects of learning.\n",
            "\n",
            "Paper ID : c5ca3b6ad0e74ab8a83d77ebef0c5a498ba5d781 \tArticle : On-line Algorithms in Machine Learning\n",
            "Author(s) : ['A. Blum']\n",
            "Year : 1996 \n",
            "Abstract : Abstract : The areas of On-Line Algorithms and Machine Learning are both concerned with problems of making decisions about the present based only on knowledge of the past. Although these areas differ in terms of their emphasis and the problems typically studied, there are a collection of results in Computational Learning Theory that fit nicely into the 'on-line algorithms' framework. This survey article discusses some of the results, models, and open problems from Computational Learning Theory that seem particularly interesting from the point of view of on-line algorithms research. The emphasis in this article is on describing some of the simpler, more intuitive results, whose proofs can be given in their entirety. Pointers to the literature are given for more sophisticated versions of these algorithms.\n",
            "\n",
            "Paper ID : e9415d9a6e0065b46acd99ba4ff8b89bd1435fc8 \tArticle : Machine Learning and Software Engineering\n",
            "Author(s) : ['Du Zhang', 'J. Tsai']\n",
            "Year : 2002 \n",
            "Abstract : Machine learning deals with the issue of how to build programs that improve their performance at some task through experience. Machine learning algorithms have proven to be of great practical value in a variety of application domains. They are particularly useful for (a) poorly understood problem domains where little knowledge exists for the humans to develop effective algorithms; (b) domains where there are large databases containing valuable implicit regularities to be discovered; or (c) domains where programs must adapt to changing conditions. Not surprisingly, the field of software engineering turns out to be a fertile ground where many software development and maintenance tasks could be formulated as learning problems and approached in terms of learning algorithms. This paper deals with the subject of applying machine learning in software engineering. In the paper, we first provide the characteristics and applicability of some frequently utilized machine learning algorithms. We then summarize and analyze the existing work and discuss some general issues in this niche area. Finally we offer some guidelines on applying machine learning methods to software engineering tasks and use some software development and maintenance tasks as examples to show how they can be formulated as learning problems and approached in terms of learning algorithms.\n",
            "\n",
            "Paper ID : d1e5263787d888da20a28254fd7bd581618e3a06 \tArticle : An Overview of Machine Learning\n",
            "Author(s) : ['J. Carbonell', 'R. Michalski', 'Tom Michael Mitchell']\n",
            "Year : 1983 \n",
            "Abstract : Learning is a many-faceted phenomenon. Learning processes include the acquisition of new declarative knowledge, the development of motor and cognitive skills through instruction or practice, the organization of new knowledge into general, effective representations, and the discovery of new facts and theories through observation and experimentation. Since the inception of the computer era, researchers have been striving to implant such capabilities in computers. Solving this problem has been, and remains, a most challenging and fascinating long-range goal in artificial intelligence (AI). The study and computer modeling of learning processes in their multiple manifestations constitutes the subject matter of machine learning.\n",
            "\n",
            "Paper ID : 635cf1572f0194d900c11b6b34e41e3b85616863 \tArticle : MLC++: a machine learning library in C++\n",
            "Author(s) : ['Ron Kohavi', 'George H. John', 'Richard Long', 'David Manley', 'Karl Pfleger']\n",
            "Year : 1994 \n",
            "Abstract : We present MLC++, a library of C++ classes and tools for supervised machine learning. While MLC++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC++ is can attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC++ aims to solve, the design of MLC++, and the current functionality.<<ETX>>\n",
            "\n",
            "Paper ID : 9820a5eeada755155f246f26ea0959e0d50dd57b \tArticle : A review of machine learning in scheduling\n",
            "Author(s) : ['H. Aytug', 'S. Bhattacharyya', 'G. J. Koehler', 'J. Snowdon']\n",
            "Year : 1994 \n",
            "Abstract : This paper has two primary purposes: to motivate the need for machine learning in scheduling systems and to survey work on machine learning in scheduling. In order to motivate the need for machine learning in scheduling, we briefly motivate the need for systems employing artificial intelligence methods for scheduling. This leads to a need for incorporating adaptive methods-learning. >\n",
            "\n",
            "Paper ID : 451f86130daf5632b87cd4c0f417b245c6ebb582 \tArticle : Does Machine Learning Really Work?\n",
            "Author(s) : ['Tom Michael Mitchell']\n",
            "Year : 1997 \n",
            "Abstract : Does machine learning really work? Yes. Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value. Machine-learning algorithms have now learned to detect credit card fraud by mining data on past transactions, learned to steer vehicles driving autonomously on public highways at 70 miles an hour, and learned the reading interests of many individuals to assemble personally customized electronic newsAbstracts. A new computational theory of learning is beginning to shed light on fundamental issues, such as the trade-off among the number of training examples available, the number of hypotheses considered, and the likely accuracy of the learned hypothesis. Newer research is beginning to explore issues such as long-term learning of new representations, the integration of Bayesian inference and induction, and life-long cumulative learning. This article, based on the keynote talk presented at the Thirteenth National Conference on Artificial Intelligence, samples a number of recent accomplishments in machine learning and looks at where the field might be headed. [Copyright restrictions preclude electronic publication of this article.]\n",
            "\n",
            "Paper ID : a54657b8de38a18f30fd154d713f9522f705166c \tArticle : Computational complexity of machine learning\n",
            "Author(s) : ['M. Kearns']\n",
            "Year : 1990 \n",
            "Abstract : This thesis is a study of the computational complexity of machine learning from examples in the distribution-free model introduced by L. G. Valiant (V84). In the distribution-free model, a learning algorithm receives positive and negative examples of an unknown target set (or concept) that is chosen from some known class of sets (or concept class). These examples are generated randomly according to a fixed but unknown probability distribution representing Nature, and the goal of the learning algorithm is to infer an hypothesis concept that closely approximates the target concept with respect to the unknown distribution. This thesis is concerned with proving theorems about learning in this formal mathematical model. \n",
            "We are interested in the phenomenon of efficient learning in the distribution-free model, in the standard polynomial-time sense. Our results include general tools for determining the polynomial-time learnability of a concept class, an extensive study of efficient learning when errors are present in the examples, and lower bounds on the number of examples required for learning in our model. A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes. These results are obtained by reducing some apparently hard number-theoretic problems from cryptography to the learning problems. The hard-to-learn concept classes include the sets represented by Boolean formulae, deterministic finite automata and a simplified form of neural networks. We also give algorithms for learning powerful concept classes under the uniform distribution, and give equivalences between natural models of efficient learnability. \n",
            "This thesis also includes detailed definitions and motivation for the distribution-free model, a chapter discussing past research in this model and related models, and a short list of important open problems.\n",
            "\n",
            "Paper ID : 14f2b886678251cdd80dc9701c889bc55de7940d \tArticle : Student Modeling and Machine Learning\n",
            "Author(s) : ['Raymund C. Sison', 'M. Shimura']\n",
            "Year : 1998 \n",
            "Abstract : After identifying essential student modeling issues and machine learning approaches, this paper examines how machine learning techniques have been used to automate the construction of student models as well as the background knowledge necessary for student modeling. In the process, the paper sheds light on the difficulty, suitability and potential of using machine learning for student modeling processes, and, to a lesser extent, the potential of using student modeling techniques in machine learning. (http://aied.inf.ed.ac.uk/members98/archive/vol_9/sison/full.html)\n",
            "------------------------------------Extracting Page #70------------------------------------\n",
            "\n",
            "Paper ID : 1574d71de4270c65e2bff693ed126f480daf10e2 \tArticle : Job shop scheduling with a genetic algorithm and machine learning\n",
            "Author(s) : ['Chung-Yee Lee', 'S. Piramuthu', 'Y. Tsai']\n",
            "Year : 1997 \n",
            "Abstract : Dynamic job shop scheduling has been proven to be an intractable problem for analytical procedures. Recent advances in computing technology, especially in artificial intelligence, have alleviated this problem by intelligently restricting the search space considered, thus opening the possibility of obtaining better results. Researchers have used various techniques that were developed under the general rubric of artificial intelligence to solve job shop scheduling problems. The most common of these have been expert systems, genetic algorithms and machine learning. Of these, we identify machine learning and genetic algorithms to be promising for scheduling applications in a job shop. In this paper, we propose to combine complementarily the strengths of genetic algorithms and induced decision trees, a machine learning technique, to develop a job shop scheduling system. Empirical results, using machine learning for releasing jobs into the shop floor and a genetic algorithm to dispatch jobs at each machine, are...\n",
            "\n",
            "Paper ID : ad58594194155af8b48eaf3ab9525a1fafd24e58 \tArticle : Estimating Probabilities: A Crucial Task in Machine Learning\n",
            "Author(s) : ['B. Cestnik']\n",
            "Year : 1990 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 65df2d9b3c656ca85e4d66c327cfd8c8d1182df3 \tArticle : Machine Learning: Neural Networks, Genetic Algorithms, and Fuzzy Systems\n",
            "Author(s) : ['H. Adeli', 'S. Hung']\n",
            "Year : 1994 \n",
            "Abstract : Perceptron Learning with a Hidden Layer An Object-Oriented Backpropagation Learning Model Concurrent Backpropagation Learning Algorithms An Adaptive Conjugate Gradient Learning Algorithm for Efficient Training of Neural Networks A Concurrent Adaptive Conjugate Gradient Learning Algorithm on MIMD Shared Memory Machines A Concurrent Genetic/Neural Network Learning Algorithm for MIMD Shared Memory Machines A Hybrid Learning Algorithm for Distributed Memory Multicomputers A Fuzzy Neural Network Learning Model Appendices References Index.\n",
            "\n",
            "Paper ID : 1efb61a9490374c4fbc04eeb4841a4d52e04a4ce \tArticle : Ensemble Learning\n",
            "Author(s) : ['Gavin Brown']\n",
            "Year : 2010 \n",
            "Abstract : In this paper, we consider ensemble classifiers, that is, machine learning based classifiers that utilize a combination of scoring functions. We provide a framework for categorizing such classifiers, and we outline several ensemble techniques, discussing how each fits into our framework. From this general introduction, we then pivot to the topic of ensemble learning within the context of malware analysis. We present a brief survey of some of the ensemble techniques that have been used in malware (and related) research. We conclude with an extensive set of experiments, where we apply ensemble techniques to a large and challenging malware dataset. While many of these ensemble techniques have appeared in the malware literature, previously there has been no way to directly compare results such as these, as different datasets and different measures of success are typically used. Our common framework and empirical results are an effort to bring some sense of order to the chaos that is evident in the evolving field of ensemble learning—both within the narrow confines of the malware analysis problem, and in the larger realm of machine learning in general.\n",
            "\n",
            "Paper ID : 88c8a70562e0a722e6b17fc65c80633b9d34ec18 \tArticle : Optimization for Machine Learning\n",
            "Author(s) : ['Jean-Yves Audibert']\n",
            "Year : 1995 \n",
            "Abstract : This is a draft containing only sra chapter.tex and an abbreviated front matter. Please check that the formatting and small changes have been performed correctly. Please verify the affiliation. Please use this version for sending us future modifications.\n",
            "\n",
            "Paper ID : 22faafeba7d7443da14c1e23e549b94e40d7d6ee \tArticle : “Memo” Functions and Machine Learning\n",
            "Author(s) : ['D. Michie']\n",
            "Year : 1968 \n",
            "Abstract : It would be useful if computers could learn from experience and thus automatically improve the efficiency of their own programs during execution. A simple but effective rote-learning facility can be provided within the framework of a suitable programming language.\n",
            "\n",
            "Paper ID : fe78338ee1ce7e202b4b901943f59d16090ae4f3 \tArticle : Machine learning and data mining\n",
            "Author(s) : ['Tom Michael Mitchell']\n",
            "Year : 1999 \n",
            "Abstract : Over the past decade many organizations have begun to routinely capture huge volumes of historical data describing their operations, their products, and their customers. At the same time, scientists and engineers in many elds nd themselves capturing increasingly complex experimental datasets, such as the gigabytes of functional MRI data that describe brain activity in humans. The eld of data mining addresses the question of how best to use this historical data to discover general regularities and to improve future decisions.\n",
            "\n",
            "Paper ID : 025d7241e095ebc7f1d615fb9d178a1098f33cb0 \tArticle : Machine Learning and Data Mining\n",
            "Author(s) : ['J. Palous']\n",
            "Year : 2002 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 72282cfbb493c487f9f43a6270b5bd9d94b57d94 \tArticle : Machine Learning: ECML-98\n",
            "Author(s) : ['C. Nédellec', 'C. Rouveirol']\n",
            "Year : 1998 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 2097ff87df3cb9427c7388bc7b997ed56907d45b \tArticle : Optimization and Machine Learning\n",
            "Author(s) : ['D. Goldberg']\n",
            "Year : 1989 \n",
            "Abstract : Problem 1 (20%) Consider the function f(x1, x2) = (x1 + x 2 2) 2 At the point xk = [1, 0] T , find (a) the gradient descent direction (b) xk+1 by exact line search on the gradient descent direction (c) the Newton direction (d) xk+1 by exact line search on the Newton direction Problem 2 (40%) Consider the following quadratic function: f(x) = 1 2 xQx− bx Assume Q is symmetric and positive definite. (a) What’s the gradient of f(x)?\n",
            "------------------------------------Extracting Page #71------------------------------------\n",
            "\n",
            "Paper ID : 72282cfbb493c487f9f43a6270b5bd9d94b57d94 \tArticle : Machine Learning: ECML-98\n",
            "Author(s) : ['C. Nédellec', 'C. Rouveirol']\n",
            "Year : 1998 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 2097ff87df3cb9427c7388bc7b997ed56907d45b \tArticle : Optimization and Machine Learning\n",
            "Author(s) : ['D. Goldberg']\n",
            "Year : 1989 \n",
            "Abstract : Problem 1 (20%) Consider the function f(x1, x2) = (x1 + x 2 2) 2 At the point xk = [1, 0] T , find (a) the gradient descent direction (b) xk+1 by exact line search on the gradient descent direction (c) the Newton direction (d) xk+1 by exact line search on the Newton direction Problem 2 (40%) Consider the following quadratic function: f(x) = 1 2 xQx− bx Assume Q is symmetric and positive definite. (a) What’s the gradient of f(x)?\n",
            "\n",
            "Paper ID : fc09717ba476ae2408c454e5557276a9fc4d093d \tArticle : Machine Learning: A Theoretical Approach\n",
            "Author(s) : ['B. Natarajan']\n",
            "Year : 1991 \n",
            "Abstract : Chapter 1 Introduction Chapter 2 Learning Concept on Countable Domains Chapter 3 Time Complexity of Concept Learning Chapter 4 Learning Concepts on Uncoutable Domains Chapter 5 Learning Functions Chapter 6 Finite Automata Chapter 7 Neural Networks Chapter 8 Generalizing the Learning Model Chapter 9 Conclusion\n",
            "\n",
            "Paper ID : 4a6a88fcd374e4fdf2120f65b82c1382bdccfa2d \tArticle : Genetic Algorithms in Machine Learning\n",
            "Author(s) : ['J. Shapiro']\n",
            "Year : 2001 \n",
            "Abstract : Genetic algorithms are stochastic search algorithms which act on a population of possible solutions. They are loosely based on the mechanics of population genetics and selection. The potential solutions are encoded as ‘genes’ — strings of characters from some alphabet. New solutions can be produced by ‘mutating’ members of the current population, and by ‘mating’ two solutions together to form a new solution. The better solutions are selected to breed and mutate and the worse ones are discarded. They are probabilistic search methods; this means that the states which they explore are not determined solely by the properties of the problems. A random process helps to guide the search. Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence — to search a space of potential solutions to find one which solves the problem.\n",
            "\n",
            "Paper ID : 26e85b438135f6defa1326d73db147e8bd51ae41 \tArticle : Learning machine learning\n",
            "Author(s) : ['A. V. Lamsweerde']\n",
            "Year : 1991 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 34ebde68c7bbc5c95eacd212e7277e529536e922 \tArticle : Machine Learning Approaches to Manufacturing\n",
            "Author(s) : ['L. Monostori', 'A. Márkus', 'H. V. Brussel', 'E. Westkämpfer']\n",
            "Year : 1996 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 43d563584f118e9047f072093d25af94dd323958 \tArticle : Learning Thermodynamics with Boltzmann Machines\n",
            "Author(s) : ['G. Torlai', 'R. Melko']\n",
            "Year : 2016 \n",
            "Abstract : A Boltzmann machine is a stochastic neural network that has been extensively used in the layers of deep architectures for modern machine learning applications. In this paper, we develop a Boltzmann machine that is capable of modeling thermodynamic observables for physical systems in thermal equilibrium. Through unsupervised learning, we train the Boltzmann machine on data sets constructed with spin configurations importance sampled from the partition function of an Ising Hamiltonian at different temperatures using Monte Carlo (MC) methods. The trained Boltzmann machine is then used to generate spin states, for which we compare thermodynamic observables to those computed by direct MC sampling. We demonstrate that the Boltzmann machine can faithfully reproduce the observables of the physical system. Further, we observe that the number of neurons required to obtain accurate results increases as the system is brought close to criticality.\n",
            "\n",
            "Paper ID : 9a24a993288119c9fd94cf2b43e394ad72dd59ee \tArticle : Efficient Learning Machines\n",
            "Author(s) : ['M. Awad', 'R. Khanna']\n",
            "Year : 2015 \n",
            "Abstract : Machine learning techniques provide cost-effective alternatives to traditional methods for extracting underlying relationships between information and data and for predicting future events by processing existing information to train models. Efficient Learning Machines explores the major topics of machine learning, including knowledge discovery, classifications, genetic algorithms, neural networking, kernel methods, and biologically-inspired techniques. Mariette Awad and Rahul Khannas synthetic approach weaves together the theoretical exposition, design principles, and practical applications of efficient machine learning. Their experiential emphasis, expressed in their close analysis of sample algorithms throughout the book, aims to equip engineers, students of engineering, and system designers to design and create new and more efficient machine learning systems. Readers of Efficient Learning Machines will learn how to recognize and analyze the problems that machine learning technology can solve for them, how to implement and deploy standard solutions to sample problems, and how to design new systems and solutions. Advances in computing performance, storage, memory, unstructured information retrieval, and cloud computing have coevolved with a new generation of machine learning paradigms and big data analytics, which the authors present in the conceptual context of their traditional precursors. Awad and Khanna explore current developments in the deep learning techniques of deep neural networks, hierarchical temporal memory, and cortical algorithms. Nature suggests sophisticated learning techniques that deploy simple rules to generate highly intelligent and organized behaviors with adaptive, evolutionary, and distributed properties. The authors examine the most popular biologically-inspired algorithms, together with a sample application to distributed datacenter management. They also discuss machine learning techniques for addressing problems of multi-objective optimization in which solutions in real-world systems are constrained and evaluated based on how well they perform with respect to multiple objectives in aggregate. Two chapters on support vector machines and their extensions focus on recent improvements to the classification and regression techniques at the core of machine learning. What youll learn Efficient Learning Machines systematically guides readers to an understanding and practical mastery of the following techniques:the machine learning techniques most commonly used to solve complex real-world problemsrecent improvements to classification and regression techniquesthe application of bio-inspired techniques to real-life problemsnew deep learning techniques that exploit advances in computing performance and storagemachine learning techniques for solving multi-objective optimization problems with nondominated methods that minimize distance to the Pareto front Who this book is for Efficient Learning Machines equips engineers, students of engineering, and system designers with the knowledge and guidance to design and create new and more efficient machine learning systems.\n",
            "\n",
            "Paper ID : b3852f0113fcf8a3913c55ae92393ae6ccde347e \tArticle : Self-taught learning: transfer learning from unlabeled data\n",
            "Author(s) : ['R. Raina', 'Alexis Battle', 'Honglak Lee', 'Ben Packer', 'A. Ng']\n",
            "Year : 2007 \n",
            "Abstract : We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.\n",
            "\n",
            "Paper ID : 884895a86fe15cb9601df4a15a1475c07f28da3c \tArticle : Boosting for transfer learning\n",
            "Author(s) : ['Wenyuan Dai', 'Qiang Yang', 'Gui-Rong Xue', 'Yong Yu']\n",
            "Year : 2007 \n",
            "Abstract : Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.\n",
            "------------------------------------Extracting Page #72------------------------------------\n",
            "\n",
            "Paper ID : 48e752c719d33ff55b3b3bec3538727f8ce69399 \tArticle : Ontology Learning for the Semantic Web\n",
            "Author(s) : ['A. Maedche', 'Steffen Staab']\n",
            "Year : 2001 \n",
            "Abstract : The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.\n",
            "\n",
            "Paper ID : 02e68b069d9cf13c082049429ffed18a5ca5f6d0 \tArticle : Support Vector Machines for Multiple-Instance Learning\n",
            "Author(s) : ['Stuart Andrews', 'Ioannis Tsochantaridis', 'Thomas Hofmann']\n",
            "Year : 2002 \n",
            "Abstract : This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristic ally. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization.\n",
            "\n",
            "Paper ID : 89b22325e7d72d11c5bad8f3893d45d0e184fa9b \tArticle : Neural Networks and Statistical Learning\n",
            "Author(s) : ['Ke-Lin Du', 'M. Swamy']\n",
            "Year : 2013 \n",
            "Abstract : Providing a broad but in-depth introduction to neural network and machine learning in a statistical framework, this book provides a single, comprehensive resource for study and further research. All the major popular neural network models and statistical learning approaches are covered with examples and exercises in every chapter to develop a practical working understanding of the content. Each of the twenty-five chapters includes state-of-the-art descriptions and important research results on the respective topics. The broad coverage includes the multilayer perceptron, the Hopfield network, associative memory models, clustering models and algorithms, the radial basis function network, recurrent neural networks, principal component analysis, nonnegative matrix factorization, independent component analysis, discriminant analysis, support vector machines, kernel methods, reinforcement learning, probabilistic and Bayesian networks, data fusion and ensemble learning, fuzzy sets and logic, neurofuzzy models, hardware implementations, and some machine learning topics. Applications to biometric/bioinformatics and data mining are also included. Focusing on the prominent accomplishments and their practical aspects, academic and technical staff, graduate students and researchers will find that this provides a solid foundation and encompassing reference for the fields of neural networks, pattern recognition, signal processing, machine learning, computational intelligence, and data mining.\n",
            "\n",
            "Paper ID : e2d4321c99b74859b8aa57daac7df4f1c11291cb \tArticle : The children's machine: rethinking school in the age of the computer\n",
            "Author(s) : ['S. Papert']\n",
            "Year : 1993 \n",
            "Abstract : Yearners and Schoolers Personal Thinking School: Change and Resistance to Change Teachers A World for Learning An Anthology of Learning Stories Instructionism versus Constructionism Computerists Yearners and Schoolers Cybernetics What can be done?\n",
            "\n",
            "Paper ID : 66f44806cd46a27f02ceb74bdfd9ad6e77e044ca \tArticle : Toward an Online Anomaly Intrusion Detection System Based on Deep Learning\n",
            "Author(s) : ['Khaled Alrawashdeh', 'C. Purdy']\n",
            "Year : 2016 \n",
            "Abstract : In the past twenty years, progress in intrusion detection has been steady but slow. The biggest challenge is to detect new attacks in real time. In this work, a deep learning approach for anomaly detection using a Restricted Boltzmann Machine (RBM) and a deep belief network are implemented. Our method uses a one-hidden layer RBM to perform unsupervised feature reduction. The resultant weights from this RBM are passed to another RBM producing a deep belief network. The pre-trained weights are passed into a fine tuning layer consisting of a Logistic Regression (LR) classifier with multi-class soft-max. We have implemented the deep learning architecture in C++ in Microsoft Visual Studio 2013 and we use the DARPA KDDCUP'99 dataset to evaluate its performance. Our architecture outperforms previous deep learning methods implemented by Li and Salama in both detection speed and accuracy. We achieve a detection rate of 97.9% on the total 10% KDDCUP'99 test dataset. By improving the training process of the simulation, we are also able to produce a low false negative rate of 2.47%. Although the deficiencies in the KDDCUP'99 dataset are well understood, it still presents machine learning approaches for predicting attacks with a reasonable challenge. Our future work will include applying our machine learning strategy to larger and more challenging datasets, which include larger classes of attacks.\n",
            "\n",
            "Paper ID : 1695dbabf8e905db0b391ff522c323db5fc8b958 \tArticle : Learning to select and generalize striking movements in robot table tennis\n",
            "Author(s) : ['Katharina Muelling', 'J. Kober', 'Oliver Kroemer', 'Jan Peters']\n",
            "Year : 2012 \n",
            "Abstract : Learning new motor tasks from physical interactions is an important goal for both robotics and machine learning. However, when moving beyond basic skills, most monolithic machine learning approaches fail to scale. For more complex skills, methods that are tailored for the domain of skill learning are needed. In this paper, we take the task of learning table tennis as an example and present a new framework that allows a robot to learn cooperative table tennis from physical interaction with a human. The robot first learns a set of elementary table tennis hitting movements from a human table tennis teacher by kinesthetic teach-in, which is compiled into a set of motor primitives represented by dynamical systems. The robot subsequently generalizes these movements to a wider range of situations using our mixture of motor primitives approach. The resulting policy enables the robot to select appropriate motor primitives as well as to generalize between them. Finally, the robot plays with a human table tennis partner and learns online to improve its behavior. We show that the resulting setup is capable of playing table tennis using an anthropomorphic robot arm.\n",
            "\n",
            "Paper ID : b06e3e2732c41305b27a62c5f2952de44644969d \tArticle : Machine Learning\n",
            "Author(s) : ['J. Carbonell']\n",
            "Year : 1983 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 0d7ee3f200a044b594606c9067bbcd1638ec32e3 \tArticle : Semi-supervised learning for peptide identification from shotgun proteomics datasets\n",
            "Author(s) : ['L. Käll', 'J. Canterbury', 'J. Weston', 'William Stafford Noble', 'M. MacCoss']\n",
            "Year : 2007 \n",
            "Abstract : Shotgun proteomics uses liquid chromatography–tandem mass spectrometry to identify proteins in complex biological samples. We describe an algorithm, called Percolator, for improving the rate of confident peptide identifications from a collection of tandem mass spectra. Percolator uses semi-supervised machine learning to discriminate between correct and decoy spectrum identifications, correctly assigning peptides to 17% more spectra from a tryptic Saccharomyces cerevisiae dataset, and up to 77% more spectra from non-tryptic digests, relative to a fully supervised approach.\n",
            "\n",
            "Paper ID : 27609ed3fda10c0f35eaac014f42640ebff3df8d \tArticle : Evolutionary extreme learning machine\n",
            "Author(s) : ['Q. Zhu', 'A. K. Qin', 'P. Suganthan', 'G. Huang']\n",
            "Year : 2005 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 944e1a7b2c5c62e952418d7684e3cade89c76f87 \tArticle : A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data\n",
            "Author(s) : ['R. Ando', 'Tong Zhang']\n",
            "Year : 2005 \n",
            "Abstract : One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.\n",
            "------------------------------------Extracting Page #73------------------------------------\n",
            "\n",
            "Paper ID : 5ed59f49c1bb7de06cfa2a9467d5efb535103277 \tArticle : Temporal difference learning and TD-Gammon\n",
            "Author(s) : ['G. Tesauro']\n",
            "Year : 1995 \n",
            "Abstract : Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.\n",
            "\n",
            "Paper ID : fae8bbf868681b83d91b2fec6c840d4d2b32005b \tArticle : Intrinsic Motivation and Reinforcement Learning\n",
            "Author(s) : ['A. Barto']\n",
            "Year : 2013 \n",
            "Abstract : Psychologists distinguish between extrinsically motivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically motivated behavior, which is behavior done for its own sake. Is an analogous distinction meaningful for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, is it possible to provide it with an analog of intrinsic motivation? Despite the fact that a formal distinction between extrinsic and intrinsic motivation is elusive, this chapter argues that the answer to both questions is assuredly “yes” and that the machine learning framework of reinforcement learning is particularly appropriate for bringing learning together with what in animals one would call motivation. Despite the common perception that a reinforcement learning agent’s reward has to be extrinsic because the agent has a distinct input channel for reward signals, reinforcement learning provides a natural framework for incorporating principles of intrinsic motivation.\n",
            "\n",
            "Paper ID : 85d727b119304dde458bcd8cf5cb87a906fb41ba \tArticle : Using AUC and accuracy in evaluating learning algorithms\n",
            "Author(s) : ['Jin Huang', 'C. Ling']\n",
            "Year : 2005 \n",
            "Abstract : The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.\n",
            "\n",
            "Paper ID : 09cb062324960b44b3e1b7956b11127ceed92a5f \tArticle : Information Theoretic Learning - Renyi's Entropy and Kernel Perspectives\n",
            "Author(s) : ['J. Príncipe']\n",
            "Year : 2010 \n",
            "Abstract : This book presents the first cohesive treatment of Information Theoretic Learning (ITL) algorithms to adapt linear or nonlinear learning machines both in supervised or unsupervised paradigms. ITL is a framework where the conventional concepts of second order statistics (covariance, L2 distances, correlation functions) are substituted by scalars and functions with information theoretic underpinnings, respectively entropy, mutual information and correntropy. ITL quantifies the stochastic structure of the data beyond second order statistics for improved performance without using full-blown Bayesian approaches that require a much larger computational cost. This is possible because of a non-parametric estimator of Renyis quadratic entropy that is only a function of pairwise differences between samples. The book compares the performance of ITL algorithms with the second order counterparts in many engineering and machine learning applications. Students, practitioners and researchers interested in statistical signal processing, computational intelligence, and machine learning will find in this book the theory to understand the basics, the algorithms to implement applications, and exciting but still unexplored leads that will provide fertile ground for future research.\n",
            "\n",
            "Paper ID : a8582979cc4337bd6349d55ac6a7112a07add3a5 \tArticle : Single-machine scheduling with learning considerations\n",
            "Author(s) : ['D. Biskup']\n",
            "Year : 1999 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : b5028fb7d55d3045cda0c290ded57c4c0b3cfdc9 \tArticle : Foundations of Rule Learning\n",
            "Author(s) : ['Johannes Fürnkranz', 'D. Gamberger', 'N. Lavrac']\n",
            "Year : 2012 \n",
            "Abstract : Rules the clearest, most explored and best understood form of knowledge representation are particularly important for data mining, as they offer the best tradeoff between human and machine understandability. This book presents the fundamentals of rule learning as investigated in classical machine learning and modern data mining. It introduces a feature-based view, as a unifying framework for propositional and relational rule learning, thus bridging the gap between attribute-value learning and inductive logic programming, and providing complete coverage of most important elements of rule learning. The book can be used as a textbook for teaching machine learning, as well as a comprehensive reference to research in the field of inductive rule learning. As such, it targets students, researchers and developers of rule learning algorithms, presenting the fundamental rule learning concepts in sufficient breadth and depth to enable the reader to understand, develop and apply rule learning techniques to real-world data.\n",
            "\n",
            "Paper ID : 7b2dd79083a74699e4e0509ac3f0a8a302b4eabe \tArticle : On the mathematical foundations of learning\n",
            "Author(s) : ['F. Cucker', 'S. Smale']\n",
            "Year : 2001 \n",
            "Abstract : (1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In\n",
            "\n",
            "Paper ID : 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04 \tArticle : Scaling learning algorithms towards AI\n",
            "Author(s) : ['Yoshua Bengio', 'Yann LeCun']\n",
            "Year : 2007 \n",
            "Abstract : One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.\n",
            "\n",
            "Paper ID : 2a1724427b9c473e73f6abff648e7416269d3a68 \tArticle : Cooperative Multi-Agent Learning: The State of the Art\n",
            "Author(s) : ['Liviu Panait', 'S. Luke']\n",
            "Year : 2005 \n",
            "Abstract : Cooperative multi-agent systems (MAS) are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to MAS problems has spawned increasing interest in machine learning techniques to automate the search and optimization process. We provide a broad survey of the cooperative multi-agent learning literature. Previous surveys of this area have largely focused on issues common to specific subareas (for example, reinforcement learning, RL or robotics). In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including RL, evolutionary computation, game theory, complex systems, agent modeling, and robotics. We find that this broad view leads to a division of the work into two categories, each with its own special issues: applying a single learner to discover joint solutions to multi-agent problems (team learning), or using multiple simultaneous learners, often one per agent (concurrent learning). Additionally, we discuss direct and indirect communication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics. We conclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources.\n",
            "\n",
            "Paper ID : 2c8ac3e1f0edeed1fbd76813e61efdc384c319c7 \tArticle : Learning Question Classifiers\n",
            "Author(s) : ['Xin Li', 'D. Roth']\n",
            "Year : 2002 \n",
            "Abstract : In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.\n",
            "------------------------------------Extracting Page #74------------------------------------\n",
            "\n",
            "Paper ID : a13efb90f0b56417bf5dd5b6219681c4259ff355 \tArticle : The Cerebellum: A Neuronal Learning Machine?\n",
            "Author(s) : ['J. Raymond', 'S. Lisberger', 'M. Mauk']\n",
            "Year : 1996 \n",
            "Abstract : Comparison of two seemingly quite different behaviors yields a surprisingly consistent picture of the role of the cerebellum in motor learning. Behavioral and physiological data about classical conditioning of the eyelid response and motor learning in the vestibulo-ocular reflex suggest that (i) plasticity is distributed between the cerebellar cortex and the deep cerebellar nuclei; (ii) the cerebellar cortex plays a special role in learning the timing of movement; and (iii) the cerebellar cortex guides learning in the deep nuclei, which may allow learning to be transferred from the cortex to the deep nuclei. Because many of the similarities in the data from the two systems typify general features of cerebellar organization, the cerebellar mechanisms of learning in these two systems may represent principles that apply to many motor systems.\n",
            "\n",
            "Paper ID : 0ca26f9a98dda0abb737692f72ffa682df14cb2f \tArticle : Sparse Bayesian learning for basis selection\n",
            "Author(s) : ['D. Wipf', 'B. Rao']\n",
            "Year : 2004 \n",
            "Abstract : Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving several results about the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the /spl lscr//sub 0/-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with basis pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.\n",
            "\n",
            "Paper ID : 70ee44786510db7c361aeaab6f37cc008bc9701e \tArticle : The elements of statistical learning: data mining, inference and prediction\n",
            "Author(s) : ['J. Franklin']\n",
            "Year : 2005 \n",
            "Abstract : The Elements of Statistical LearningAn Introduction to Statistical LearningPattern Recognition and Machine LearningData Mining IVStatistics for Machine LearningStatistical Learning for Biomedical DataGeocomputation with RThe Science of Bradley EfronProbability for Statistics and Machine LearningElectronicsMachine LearningOutlines and Highlights for the Elements of Statistical Learning by Hastie, IsbnA Computational Approach to Statistical LearningEffective Computation in PhysicsMathematics for Machine LearningStatistical Learning with Math and PythonA First Course in Machine LearningUsing R for Introductory Statistics, Second EditionBayesian Reasoning and Machine LearningStatistical Learning with SparsityApplied Predictive ModelingStatistical Machine Learning with ApplicationsFoundations of Machine Learning, second editionComputer Age Statistical InferenceThe Elements of Statistical LearningThe Elements of Statistical LearningAn Elementary Introduction to Statistical Learning TheoryComputer Age Statistical Inference, Student EditionAn Introduction to the BootstrapIntroduction to ProbabilityStatistical Challenges in Modern AstronomyThe Elements of Statistical LearningThe Nature of Statistical Learning TheoryMachine LearningIntroduction to Machine LearningStatistical Learning from a Regression PerspectiveThe Art of StatisticsAll of StatisticsStatistical ModelsChemical Engineering Design\n",
            "\n",
            "Paper ID : efbeedfbf13db70878618553f0c4a0fec6f493fe \tArticle : Learning Collaborative Information Filters\n",
            "Author(s) : ['Daniel Billsus', 'M. Pazzani']\n",
            "Year : 1998 \n",
            "Abstract : Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms.\n",
            "\n",
            "Paper ID : e65ff1c44144d89684d089b5bad5405dc373fe30 \tArticle : Fully complex extreme learning machine\n",
            "Author(s) : ['Ming-Bin Li', 'G. Huang', 'P. Saratchandran', 'N. Sundararajan']\n",
            "Year : 2005 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 73e1c4a1152a75ec7310adfb4b8daea16d627bc7 \tArticle : Learning to learn with the informative vector machine\n",
            "Author(s) : ['Neil D. Lawrence', 'John C. Platt']\n",
            "Year : 2004 \n",
            "Abstract : This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.\n",
            "\n",
            "Paper ID : 605402e235bd62437baf3c9ebefe77fb4d92ee95 \tArticle : The Helmholtz Machine\n",
            "Author(s) : ['P. Dayan', 'Geoffrey E. Hinton', 'Radford M. Neal', 'R. Zemel']\n",
            "Year : 1995 \n",
            "Abstract : Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.\n",
            "\n",
            "Paper ID : 2ebdfa3852e4ad68a0cfde9f0f69b95953d69178 \tArticle : On-Line Sequential Extreme Learning Machine\n",
            "Author(s) : ['G. Huang', 'Nan-Ying Liang', 'Hai-Jun Rong', 'P. Saratchandran', 'N. Sundararajan']\n",
            "Year : 2005 \n",
            "Abstract : The primitive Extreme Learning Machine (ELM) [1, 2, 3] with additive neurons and RBF kernels was implemented in batch mode. In this paper, its sequential modification based on recursive least-squares (RLS) algorithm, which referred as Online Sequential Extreme Learning Machine (OS-ELM), is introduced. Based on OS-ELM, Online Sequential Fuzzy Extreme Learning Machine (Fuzzy-ELM) is also introduced to implement zero order TSK model and first order TSK model. The performance of OS-ELM and Fuzzy-ELM are evaluated and compared with other popular sequential learning algorithms, and experimental results on some real benchmark regression problems show that the proposedOnlineSequentialExtreme Learning Machine (OS-ELM) produces better generalization performance at very fast learning speed.\n",
            "\n",
            "Paper ID : 3b2c67905dfa03e3624a5db4a14e3d7fded478d1 \tArticle : Extreme learning machine: RBF network case\n",
            "Author(s) : ['G. Huang', 'C. Siew']\n",
            "Year : 2004 \n",
            "Abstract : A new learning algorithm called extreme learning machine (ELM) has recently been proposed for single-hidden layer feedforward neural networks (SLFNs) to easily achieve good generalization performance at extremely fast learning speed. ELM randomly chooses the input weights and analytically determines the output weights of SLFNs. This paper shows that ELM can be extended to radial basis function (RBF) network case, which allows the centers and impact widths of RBF kernels to be randomly generated and the output weights to be simply analytically calculated instead of iteratively tuned. Interestingly, the experimental results show that the ELM algorithm for RBF networks can complete learning at extremely fast speed and produce generalization performance very close to that of SVM in many artificial and real benchmarking function approximation and classification problems. Since ELM does not require validation and human-intervened parameters for given network architectures, ELM can be easily used.\n",
            "\n",
            "Paper ID : b36a5bb1707bb9c70025294b3a310138aae8327a \tArticle : Automatic differentiation in PyTorch\n",
            "Author(s) : ['Adam Paszke', 'S. Gross', 'Soumith Chintala', 'Gregory Chanan', 'E. Yang', 'Zach DeVito', 'Zeming Lin', 'Alban Desmaison', 'L. Antiga', 'Adam Lerer']\n",
            "Year : 2017 \n",
            "Abstract : In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.\n",
            "------------------------------------Extracting Page #75------------------------------------\n",
            "\n",
            "Paper ID : b36a5bb1707bb9c70025294b3a310138aae8327a \tArticle : Automatic differentiation in PyTorch\n",
            "Author(s) : ['Adam Paszke', 'S. Gross', 'Soumith Chintala', 'Gregory Chanan', 'E. Yang', 'Zach DeVito', 'Zeming Lin', 'Alban Desmaison', 'L. Antiga', 'Adam Lerer']\n",
            "Year : 2017 \n",
            "Abstract : In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.\n",
            "\n",
            "Paper ID : 81600fd653a828d69f6160705be6814dd101beb7 \tArticle : From local explanations to global understanding with explainable AI for trees\n",
            "Author(s) : ['Scott M. Lundberg', 'G. Erion', 'Hugh Chen', 'A. DeGrave', 'J. Prutkin', 'B. Nair', 'R. Katz', 'J. Himmelfarb', 'N. Bansal', 'Su-In Lee']\n",
            "Year : 2020 \n",
            "Abstract : Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model’s performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains. Tree-based machine learning models are widely used in domains such as healthcare, finance and public services. The authors present an explanation method for trees that enables the computation of optimal local explanations for individual predictions, and demonstrate their method on three medical datasets.\n",
            "\n",
            "Paper ID : 3a288c63576fc385910cb5bc44eaea75b442e62e \tArticle : UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\n",
            "Author(s) : ['Leland McInnes', 'John Healy']\n",
            "Year : 2018 \n",
            "Abstract : UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.\n",
            "\n",
            "Paper ID : 34f25a8704614163c4095b3ee2fc969b60de4698 \tArticle : Dropout: a simple way to prevent neural networks from overfitting\n",
            "Author(s) : ['Nitish Srivastava', 'Geoffrey E. Hinton', 'A. Krizhevsky', 'Ilya Sutskever', 'R. Salakhutdinov']\n",
            "Year : 2014 \n",
            "Abstract : Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\n",
            "\n",
            "Paper ID : bee044c8e8903fb67523c1f8c105ab4718600cdb \tArticle : Explaining and Harnessing Adversarial Examples\n",
            "Author(s) : ['Ian J. Goodfellow', 'Jonathon Shlens', 'Christian Szegedy']\n",
            "Year : 2015 \n",
            "Abstract : Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.\n",
            "\n",
            "Paper ID : 81a4fd3004df0eb05d6c1cef96ad33d5407820df \tArticle : A Comprehensive Survey on Graph Neural Networks\n",
            "Author(s) : ['Zonghan Wu', 'Shirui Pan', 'Fengwen Chen', 'Guodong Long', 'Chengqi Zhang', 'Philip S. Yu']\n",
            "Year : 2019 \n",
            "Abstract : Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.\n",
            "\n",
            "Paper ID : 26bc9195c6343e4d7f434dd65b4ad67efe2be27a \tArticle : XGBoost: A Scalable Tree Boosting System\n",
            "Author(s) : ['Tianqi Chen', 'Carlos Guestrin']\n",
            "Year : 2016 \n",
            "Abstract : Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.\n",
            "\n",
            "Paper ID : 5091316bb1c6db6c6a813f4391911a5c311fdfe0 \tArticle : \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier\n",
            "Author(s) : ['Marco Tulio Ribeiro', 'Sameer Singh', 'Carlos Guestrin']\n",
            "Year : 2016 \n",
            "Abstract : Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.\n",
            "\n",
            "Paper ID : 899defb6a100af509547b8d74bb626533ee87da4 \tArticle : Measuring the VC-Dimension of a Learning Machine\n",
            "Author(s) : ['V. Vapnik', 'E. Levin', 'Yann LeCun']\n",
            "Year : 1994 \n",
            "Abstract : A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.\n",
            "\n",
            "Paper ID : 273dfbcb68080251f5e9ff38b4413d7bd84b10a1 \tArticle : LIBSVM: A library for support vector machines\n",
            "Author(s) : ['Chih-Chung Chang', 'Chih-Jen Lin']\n",
            "Year : 2011 \n",
            "Abstract : LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.\n",
            "------------------------------------Extracting Page #76------------------------------------\n",
            "\n",
            "Paper ID : 273dfbcb68080251f5e9ff38b4413d7bd84b10a1 \tArticle : LIBSVM: A library for support vector machines\n",
            "Author(s) : ['Chih-Chung Chang', 'Chih-Jen Lin']\n",
            "Year : 2011 \n",
            "Abstract : LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.\n",
            "\n",
            "Paper ID : 46bed4c578e96e05fa3e5704620c4ffa0746d78f \tArticle : A Learning Machine: Part I\n",
            "Author(s) : ['R. Friedberg']\n",
            "Year : 1958 \n",
            "Abstract : Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.\n",
            "\n",
            "Paper ID : 12cac86b9cb5557d7f75b6fbcab0bac40b5f7995 \tArticle : Explainable Artificial Intelligence (XAI)\n",
            "Author(s) : ['M. Ridley']\n",
            "Year : 2022 \n",
            "Abstract : The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.\n",
            "\n",
            "Paper ID : 554f3b32b956035fbfabba730c6f0300d6955dce \tArticle : Learning logical definitions from relations\n",
            "Author(s) : ['J. R. Quinlan']\n",
            "Year : 2004 \n",
            "Abstract : This paper describesfoil, a system that learns Horn clauses from data expressed as relations.foil is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature.\n",
            "\n",
            "Paper ID : 41fef1a197fab9684a4608b725d3ae72e1ab4b39 \tArticle : Sparse Feature Learning for Deep Belief Networks\n",
            "Author(s) : [\"Marc'Aurelio Ranzato\", 'Y-Lan Boureau', 'Yann LeCun']\n",
            "Year : 2007 \n",
            "Abstract : Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.\n",
            "\n",
            "Paper ID : 8592e46a5435d18bba70557846f47290b34c1aa5 \tArticle : Learning and relearning in Boltzmann machines\n",
            "Author(s) : ['Geoffrey E. Hinton', 'T. Sejnowski']\n",
            "Year : 1986 \n",
            "Abstract : This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References\n",
            "\n",
            "Paper ID : 085589ca53509ad27edd95827f7198b3ce107024 \tArticle : A framework for variation discovery and genotyping using next-generation DNA sequencing data\n",
            "Author(s) : ['M. DePristo', 'E. Banks', 'R. Poplin', 'K. Garimella', 'J. Maguire', 'C. Hartl', 'A. Philippakis', 'G. Del Angel', 'M. Rivas', 'M. Hanna', 'A. McKenna', 'T. Fennell', 'A. Kernytsky', 'A. Sivachenko', 'K. Cibulskis', 'S. Gabriel', 'D. Altshuler', 'M. Daly']\n",
            "Year : 2011 \n",
            "Abstract : Recent advances in sequencing technology make it possible to comprehensively catalog genetic variation in population samples, creating a foundation for understanding human disease, ancestry and evolution. The amounts of raw data produced are prodigious, and many computational steps are required to translate this output into high-quality variant calls. We present a unified analytic framework to discover and genotype variation among multiple samples simultaneously that achieves sensitive and specific results across five sequencing technologies and three distinct, canonical experimental designs. Our process includes (i) initial read mapping; (ii) local realignment around indels; (iii) base quality score recalibration; (iv) SNP discovery and genotyping to find all potential variants; and (v) machine learning to separate true segregating variation from machine artifacts common to next-generation sequencing technologies. We here discuss the application of these tools, instantiated in the Genome Analysis Toolkit, to deep whole-genome, whole-exome capture and multi-sample low-pass (∼4×) 1000 Genomes Project datasets.\n",
            "\n",
            "Paper ID : 82635fb63640ae95f90ee9bdc07832eb461ca881 \tArticle : The Pascal Visual Object Classes (VOC) Challenge\n",
            "Author(s) : ['M. Everingham', 'L. Gool', 'Christopher K. I. Williams', 'J. Winn', 'Andrew Zisserman']\n",
            "Year : 2009 \n",
            "Abstract : The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.\n",
            "\n",
            "Paper ID : 0a8149fb5aa8a5684e7d530c264451a5cb9250f5 \tArticle : Recent Advances in Hierarchical Reinforcement Learning\n",
            "Author(s) : ['A. Barto', 'S. Mahadevan']\n",
            "Year : 2003 \n",
            "Abstract : Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.\n",
            "\n",
            "Paper ID : b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b \tArticle : Adversarial examples in the physical world\n",
            "Author(s) : ['A. Kurakin', 'Ian J. Goodfellow', 'Samy Bengio']\n",
            "Year : 2017 \n",
            "Abstract : Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.\n",
            "------------------------------------Extracting Page #77------------------------------------\n",
            "\n",
            "Paper ID : a0858dd960c635c3e6294908b794fae52f66e42f \tArticle : Spark: Cluster Computing with Working Sets\n",
            "Author(s) : ['M. Zaharia', 'M. Chowdhury', 'M. Franklin', 'S. Shenker', 'I. Stoica']\n",
            "Year : 2010 \n",
            "Abstract : MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.\n",
            "\n",
            "Paper ID : 38f23fe236b152cd4983c8f30d305a568afd0d3e \tArticle : A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI\n",
            "Author(s) : ['Erico Tjoa', 'Cuntai Guan']\n",
            "Year : 2021 \n",
            "Abstract : Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.\n",
            "\n",
            "Paper ID : 41f762a455ff8c26aa99be58fe6398722864f06e \tArticle : Learning from Data\n",
            "Author(s) : ['M. Kantardzic']\n",
            "Year : 2011 \n",
            "Abstract : This chapter contains sections titled: Learning Machine Statistical Learning Theory Types of Learning Methods Common Learning Tasks Model Estimation Review Questions and Problems References for further study\n",
            "\n",
            "Paper ID : 5ded2b8c64491b4a67f6d39ce473d4b9347a672e \tArticle : A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\n",
            "Author(s) : ['Adina Williams', 'Nikita Nangia', 'Samuel R. Bowman']\n",
            "Year : 2018 \n",
            "Abstract : This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.\n",
            "\n",
            "Paper ID : 13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986 \tArticle : Random Forests\n",
            "Author(s) : ['L. Breiman']\n",
            "Year : 2004 \n",
            "Abstract : Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.\n",
            "\n",
            "Paper ID : dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63 \tArticle : Rapid object detection using a boosted cascade of simple features\n",
            "Author(s) : ['Paul A. Viola', 'Michael J. Jones']\n",
            "Year : 2001 \n",
            "Abstract : This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the \"integral image\" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a \"cascade\" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.\n",
            "\n",
            "Paper ID : 04b23f577c20d1a0e2a67aadda555f58e6d23d6e \tArticle : Support Vector Machines\n",
            "Author(s) : ['Ingo Steinwart', 'A. Christmann']\n",
            "Year : 2008 \n",
            "Abstract : This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.\n",
            "\n",
            "Paper ID : 5151d6cb3a4eaec14a56944d58338251fca344ab \tArticle : Overcoming catastrophic forgetting in neural networks\n",
            "Author(s) : ['J. Kirkpatrick', 'Razvan Pascanu', 'Neil C. Rabinowitz', 'J. Veness', 'Guillaume Desjardins', 'Andrei A. Rusu', 'K. Milan', 'John Quan', 'Tiago Ramalho', 'Agnieszka Grabska-Barwinska', 'D. Hassabis', 'C. Clopath', 'D. Kumaran', 'R. Hadsell']\n",
            "Year : 2017 \n",
            "Abstract : Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.\n",
            "\n",
            "Paper ID : b84276fe751ca4f1389549281383b151a746107b \tArticle : Statistical language learning\n",
            "Author(s) : ['Eugene Charniak']\n",
            "Year : 1993 \n",
            "Abstract : From the Publisher: \n",
            "Eugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background. \n",
            "New, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises. \n",
            "Charniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: \"one simply gathers statistics.\" \n",
            "Language, Speech, and Communication\n",
            "\n",
            "Paper ID : d516daff247f7157fccde6649ace91d969cd1973 \tArticle : The Mythos of Model Interpretability\n",
            "Author(s) : ['Zachary Chase Lipton']\n",
            "Year : 2018 \n",
            "Abstract : Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?\n",
            "------------------------------------Extracting Page #78------------------------------------\n",
            "\n",
            "Paper ID : 8f1408d33858a78f90f9000a34856664fc639ae4 \tArticle : Statistical Comparisons of Classifiers over Multiple Data Sets\n",
            "Author(s) : ['J. Demšar']\n",
            "Year : 2006 \n",
            "Abstract : While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.\n",
            "\n",
            "Paper ID : 806c2c4327a31fded64a5d673ab82b133194c234 \tArticle : Introduction to information retrieval\n",
            "Author(s) : ['Christopher D. Manning', 'P. Raghavan', 'Hinrich Schütze']\n",
            "Year : 2005 \n",
            "Abstract : Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.\n",
            "\n",
            "Paper ID : e044a4b5be1563fccc46e8f7552935b99365f90a \tArticle : Learning with Support Vector Machines\n",
            "Author(s) : ['C. Campbell', 'Yiming Ying']\n",
            "Year : 2011 \n",
            "Abstract : Support Vectors Machines have become a well established tool within machine learning. They work well in practice and have now been used across a wide range of applications from recognizing hand-written digits, to face identification, text categorisation, bioinformatics, and database marketing. In this book we give an introductory overview of this subject. We start with a simple Support Vector Machine for performing binary classification before considering multi-class classification and learning in the presence of noise. We show that this framework can be extended to many other scenarios such as prediction with real-valued outputs, novelty detection and the handling of complex output structures such as parse trees. Finally, we give an overview of the main types of kernels which are used in practice and how to learn and make predictions from multiple types of input data. Table of Contents: Support Vector Machines for Classification / Kernel-based Models / Learning with Kernels\n",
            "\n",
            "Paper ID : 68c1bfe375dde46777fe1ac8f3636fb651e3f0f8 \tArticle : Experiments with a New Boosting Algorithm\n",
            "Author(s) : ['Y. Freund', 'R. Schapire']\n",
            "Year : 1996 \n",
            "Abstract : In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.\n",
            "\n",
            "Paper ID : 98f8a0055bb28133efcff359a92937324d0e6f51 \tArticle : A Perspective View and Survey of Meta-Learning\n",
            "Author(s) : ['R. Vilalta', 'Youssef Drissi']\n",
            "Year : 2005 \n",
            "Abstract : Different researchers hold different views of what the term meta-learning exactlymeans. The first part of this paper provides our own perspective view in which the goal isto build self-adaptive learners (i.e. learning algorithms that improve their bias dynamicallythrough experience by accumulating meta-knowledge). The second part provides a survey ofmeta-learning as reported by the machine-learning literature. We find that, despite differentviews and research lines, a question remains constant: how can we exploit knowledge aboutlearning (i.e. meta-knowledge) to improve the performance of learning algorithms? Clearlythe answer to this question is key to the advancement of the field and continues being thesubject of intensive research.\n",
            "\n",
            "Paper ID : f216444d4f2959b4520c61d20003fa30a199670a \tArticle : Siamese Neural Networks for One-Shot Image Recognition\n",
            "Author(s) : ['Gregory R. Koch']\n",
            "Year : 2015 \n",
            "Abstract : The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.\n",
            "\n",
            "Paper ID : 0b2a2ed870d9e947aca41daf751d987ab3163d74 \tArticle : Introduction to Statistical Pattern Recognition\n",
            "Author(s) : ['P. Pudil', 'P. Somol', 'M. Haindl']\n",
            "Year : 2006 \n",
            "Abstract : Annotation : Pattern recognition problem is briefly characterized as a process of machine learning. Its main stages (dimensionality reduction and classifier design) are stated. Statistical approach is given priority here. Two approaches to dimensionality reduction, namely feature selection (FS) and feature extraction (FE) are specified. Though FS is a special case of FE, they are very different from a practical viewpoint and thus must be considered separately.\n",
            "\n",
            "Paper ID : 5a05b9d5de7d1d11d1aa03826ded577d4acf4f49 \tArticle : Genetic Algorithms + Data Structures = Evolution Programs\n",
            "Author(s) : ['Z. Michalewicz']\n",
            "Year : 1996 \n",
            "Abstract : 1 GAs: What Are They?.- 2 GAs: How Do They Work?.- 3 GAs: Why Do They Work?.- 4 GAs: Selected Topics.- 5 Binary or Float?.- 6 Fine Local Tuning.- 7 Handling Constraints.- 8 Evolution Strategies and Other Methods.- 9 The Transportation Problem.- 10 The Traveling Salesman Problem.- 11 Evolution Programs for Various Discrete Problems.- 12 Machine Learning.- 13 Evolutionary Programming and Genetic Programming.- 14 A Hierarchy of Evolution Programs.- 15 Evolution Programs and Heuristics.- 16 Conclusions.- Appendix A.- Appendix B.- Appendix C.- Appendix D.- References.\n",
            "\n",
            "Paper ID : 88816ae492956f3004daa41357166f1181c0c1bf \tArticle : Laplacian Eigenmaps for Dimensionality Reduction and Data Representation\n",
            "Author(s) : ['Mikhail Belkin', 'P. Niyogi']\n",
            "Year : 2003 \n",
            "Abstract : One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.\n",
            "\n",
            "Paper ID : 7bb6bdf4ed609e5e72d4206d1b308323e73dceec \tArticle : An Introduction to Genetic Algorithms.\n",
            "Author(s) : ['D. Heiss-Czedik']\n",
            "Year : 1997 \n",
            "Abstract : An Introduction to Genetic Algorithms is one of the rare examples of a book in which every single page is worth reading. The author, Melanie Mitchell, manages to describe in depth many fascinating examples as well as important theoretical issues, yet the book is concise (200 pages) and readable. Although Mitchell explicitly states that her aim is not a complete survey, the essentials of genetic algorithms (GAs) are contained: theory and practice, problem solving and scientific models, a \"Brief History\" and \"Future Directions.\" Her book is both an introduction for novices interested in GAs and a collection of recent research, including hot topics such as coevolution (interspecies and intraspecies), diploidy and dominance, encapsulation, hierarchical regulation, adaptive encoding, interactions of learning and evolution, self-adapting GAs, and more. Nevertheless, the book focused more on machine learning, artificial life, and modeling evolution than on optimization and engineering.\n",
            "------------------------------------Extracting Page #79------------------------------------\n",
            "\n",
            "Paper ID : 6adf016e7531c91100d3cf4a74f5d4c87b26b528 \tArticle : Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks\n",
            "Author(s) : ['Nicolas Papernot', 'P. Mcdaniel', 'Xi Wu', 'S. Jha', 'A. Swami']\n",
            "Year : 2016 \n",
            "Abstract : Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.\n",
            "\n",
            "Paper ID : 6b570069f14c7588e066f7138e1f21af59d62e61 \tArticle : Theano: A Python framework for fast computation of mathematical expressions\n",
            "Author(s) : ['Rami Al-Rfou', 'Guillaume Alain', 'Amjad Almahairi', 'Christof Angermüller', 'Dzmitry Bahdanau', 'Nicolas Ballas', 'Frédéric Bastien', 'Justin Bayer', 'A. Belikov', 'A. Belopolsky', 'Yoshua Bengio', 'Arnaud Bergeron', 'J. Bergstra', 'Valentin Bisson', 'Josh Bleecher Snyder', 'Nicolas Bouchard', 'Nicolas Boulanger-Lewandowski', 'Xavier Bouthillier', 'A. D. Brébisson', 'Olivier Breuleux', 'P. Carrier', 'Kyunghyun Cho', 'J. Chorowski', 'P. Christiano', 'Tim Cooijmans', 'Marc-Alexandre Côté', 'Myriam Côté', 'Aaron C. Courville', 'Yann Dauphin', 'Olivier Delalleau', 'Julien Demouth', 'Guillaume Desjardins', 'S. Dieleman', 'Laurent Dinh', 'Mélanie Ducoffe', 'Vincent Dumoulin', 'S. Kahou', 'D. Erhan', 'Ziye Fan', 'Orhan Firat', 'M. Germain', 'Xavier Glorot', 'Ian J. Goodfellow', 'M. Graham', 'Çaglar Gülçehre', 'P. Hamel', 'Iban Harlouchet', 'J. Heng', 'Balázs Hidasi', 'Sina Honari', 'Arjun Jain', 'Sébastien Jean', 'Kai Jia', 'Mikhail Korobov', 'Vivek Kulkarni', 'Alex Lamb', 'Pascal Lamblin', 'Eric Larsen', 'César Laurent', 'S. Lee', 'S. Lefrançois', 'S. Lemieux', 'Nicholas Léonard', 'Zhouhan Lin', 'J. Livezey', 'C. Lorenz', 'J. Lowin', 'Qianli Ma', 'Pierre-Antoine Manzagol', 'Olivier Mastropietro', 'R. McGibbon', 'R. Memisevic', 'Bart van Merrienboer', 'Vincent Michalski', 'Mehdi Mirza', 'A. Orlandi', 'C. Pal', 'Razvan Pascanu', 'M. Pezeshki', 'Colin Raffel', 'D. Renshaw', 'M. Rocklin', 'Adriana Romero', 'Markus Roth', 'Peter Sadowski', 'J. Salvatier', 'F. Savard', 'Jan Schlüter', 'J. Schulman', 'Gabriel Schwartz', 'Iulian Serban', 'Dmitriy Serdyuk', 'Samira Shabanian', 'Étienne Simon', 'Sigurd Spieckermann', 'S. Subramanyam', 'Jakub Sygnowski', 'Jérémie Tanguay', 'Gijs van Tulder', 'Joseph P. Turian', 'S. Urban', 'Pascal Vincent', 'Francesco Visin', 'Harm de Vries', 'David Warde-Farley', 'Dustin J. Webb', 'M. Willson', 'Kelvin Xu', 'Lijun Xue', 'Li Yao', 'Saizheng Zhang', 'Ying Zhang']\n",
            "Year : 2016 \n",
            "Abstract : Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. \n",
            "The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.\n",
            "\n",
            "Paper ID : 34d405eaecab40a932108a7ff97e92fb8fd1ae4e \tArticle : A review of feature selection techniques in bioinformatics\n",
            "Author(s) : ['Y. Saeys', 'Iñaki Inza', 'P. Larrañaga']\n",
            "Year : 2007 \n",
            "Abstract : Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.\n",
            "\n",
            "Paper ID : 7365f887c938ca21a6adbef08b5a520ebbd4638f \tArticle : Model Cards for Model Reporting\n",
            "Author(s) : ['Margaret Mitchell', 'Simone Wu', 'Andrew Zaldivar', 'Parker Barnes', 'Lucy Vasserman', 'B. Hutchinson', 'Elena Spitzer', 'Inioluwa Deborah Raji', 'Timnit Gebru']\n",
            "Year : 2019 \n",
            "Abstract : Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.\n",
            "\n",
            "Paper ID : 08b43d84e6747e370ef307e2ada50675b414514a \tArticle : Survey of clustering algorithms\n",
            "Author(s) : ['R. Xu', 'D. Wunsch']\n",
            "Year : 2005 \n",
            "Abstract : Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.\n",
            "\n",
            "Paper ID : a1874aafa8730bdd4b28f29d025141c13ee28b58 \tArticle : From Data Mining to Knowledge Discovery in Databases\n",
            "Author(s) : ['U. Fayyad', 'G. Piatetsky-Shapiro', 'Padhraic Smyth']\n",
            "Year : 1996 \n",
            "Abstract : ■ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.\n",
            "\n",
            "Paper ID : b54bcfca3fddc26b8889739a247a25e445818149 \tArticle : Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition\n",
            "Author(s) : ['Dan Jurafsky', 'James H. Martin']\n",
            "Year : 2000 \n",
            "Abstract : From the Publisher: \n",
            "This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.\n",
            "\n",
            "Paper ID : 167e1359943b96b9e92ee73db1df69a1f65d731d \tArticle : A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts\n",
            "Author(s) : ['B. Pang', 'Lillian Lee']\n",
            "Year : 2004 \n",
            "Abstract : Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.\n",
            "\n",
            "Paper ID : a3461eaf51016f9d6e85ea47173b27e019e801c4 \tArticle : State of the Art\n",
            "Author(s) : ['Markus Voelter']\n",
            "Year : 1997 \n",
            "Abstract : We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H ∈ L H induces a mapping h : X → O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C ∈ L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H ∈ L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H ≡ C (i.e. h(x) = c(x) ∀x ∈ X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n → ∞. Such a requirement is however very rigid and does not comply to the …\n",
            "\n",
            "Paper ID : a34e35dbbc6911fa7b94894dffdc0076a261b6f0 \tArticle : Neural Networks and the Bias/Variance Dilemma\n",
            "Author(s) : ['S. Geman', 'E. Bienenstock', 'R. Doursat']\n",
            "Year : 1992 \n",
            "Abstract : Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.\n",
            "------------------------------------Extracting Page #80------------------------------------\n",
            "\n",
            "Paper ID : a3461eaf51016f9d6e85ea47173b27e019e801c4 \tArticle : State of the Art\n",
            "Author(s) : ['Markus Voelter']\n",
            "Year : 1997 \n",
            "Abstract : We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H ∈ L H induces a mapping h : X → O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C ∈ L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H ∈ L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H ≡ C (i.e. h(x) = c(x) ∀x ∈ X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n → ∞. Such a requirement is however very rigid and does not comply to the …\n",
            "\n",
            "Paper ID : 7a59fde27461a3ef4a21a249cc403d0d96e4a0d7 \tArticle : Random Features for Large-Scale Kernel Machines\n",
            "Author(s) : ['A. Rahimi', 'B. Recht']\n",
            "Year : 2007 \n",
            "Abstract : To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.\n",
            "\n",
            "Paper ID : ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be \tArticle : Text Classification Algorithms: A Survey\n",
            "Author(s) : ['Kamran Kowsari', 'K. Meimandi', 'Mojtaba Heidarysafa', 'Sanjana Mendu', 'Laura E. Barnes', 'Donald E. Brown']\n",
            "Year : 2019 \n",
            "Abstract : In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.\n",
            "\n",
            "Paper ID : 09879f7956dddc2a9328f5c1472feeb8402bcbcf \tArticle : Density estimation using Real NVP\n",
            "Author(s) : ['Laurent Dinh', 'Jascha Narain Sohl-Dickstein', 'Samy Bengio']\n",
            "Year : 2017 \n",
            "Abstract : Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.\n",
            "\n",
            "Paper ID : e95d3934e51107da7610acd0b1bcb6551671f9f1 \tArticle : A Practical Guide to Training Restricted Boltzmann Machines\n",
            "Author(s) : ['Geoffrey E. Hinton']\n",
            "Year : 2012 \n",
            "Abstract : Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.\n",
            "\n",
            "Paper ID : 43d2ed5c3c55c1100450cd74dc1031afa24d37b2 \tArticle : Collective Classification in Network Data\n",
            "Author(s) : ['P. Sen', 'Galileo Namata', 'M. Bilgic', 'L. Getoor', 'B. Gallagher', 'Tina Eliassi-Rad']\n",
            "Year : 2008 \n",
            "Abstract : Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.\n",
            "\n",
            "Paper ID : f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556 \tArticle : The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions\n",
            "Author(s) : ['P. Tschandl', 'C. Rosendahl', 'H. Kittler']\n",
            "Year : 2018 \n",
            "Abstract : Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.\n",
            "\n",
            "Paper ID : 7ef26fb13e7b8d7555b6f58af141609535f7e28e \tArticle : A review of classification algorithms for EEG-based brain–computer interfaces: a 10 year update\n",
            "Author(s) : ['F. Lotte', 'L. Bougrain', 'A. Cichocki', 'M. Clerc', 'M. Congedo', 'A. Rakotomamonjy', 'F. Yger']\n",
            "Year : 2018 \n",
            "Abstract : Objective. Most current electroencephalography (EEG)-based brain–computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs. Approach. We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons. Main results. We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods. Significance. This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI.\n",
            "\n",
            "Paper ID : 7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f \tArticle : Sequence Transduction with Recurrent Neural Networks\n",
            "Author(s) : ['A. Graves']\n",
            "Year : 2012 \n",
            "Abstract : Many machine learning tasks can be expressed as the transformation---or \\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \\emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.\n",
            "\n",
            "Paper ID : d1b9a3b11e6c9571a1553556f82b605b2b4baec3 \tArticle : Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures\n",
            "Author(s) : ['Matt Fredrikson', 'S. Jha', 'T. Ristenpart']\n",
            "Year : 2015 \n",
            "Abstract : Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.\n",
            "------------------------------------Extracting Page #81------------------------------------\n",
            "\n",
            "Paper ID : 3a84214cb69ea0b34352285029f368b75718c32b \tArticle : Understanding of a convolutional neural network\n",
            "Author(s) : ['Saad Albawi', 'T. Mohammed', 'Saad Al-Zawi']\n",
            "Year : 2017 \n",
            "Abstract : The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.\n",
            "\n",
            "Paper ID : a42ca00fc188beb5586ad4c7108b70aeb5317da0 \tArticle : Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms\n",
            "Author(s) : ['C. Thornton', 'F. Hutter', 'H. Hoos', 'Kevin Leyton-Brown']\n",
            "Year : 2013 \n",
            "Abstract : Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.\n",
            "\n",
            "Paper ID : a85e512d8845bd007b0866b4a97e8341463f8190 \tArticle : Scalable Nearest Neighbor Algorithms for High Dimensional Data\n",
            "Author(s) : ['Marius Muja', 'D. Lowe']\n",
            "Year : 2014 \n",
            "Abstract : For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.\n",
            "\n",
            "Paper ID : 44bb1605c4ab8a8ce2764fa20424f6a148101ca4 \tArticle : ROC Graphs: Notes and Practical Considerations for Researchers\n",
            "Author(s) : ['Tom Fawcett']\n",
            "Year : 2007 \n",
            "Abstract : Receiver Operating Characteristics (ROC) graphs are a useful technique for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been increasingly adopted in the machine learning and data mining research communities. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. This article serves both as a tutorial introduction to ROC graphs and as a practical guide for using them in research.\n",
            "\n",
            "Paper ID : 31af4b8793e93fd35e89569ccd663ae8777f0072 \tArticle : The Netflix Prize\n",
            "Author(s) : ['J. Bennett', 'S. Lanning']\n",
            "Year : 2007 \n",
            "Abstract : Netflix released a dataset containing 100 million anonymous movie ratings and challenged the data mining, machine learning and computer science communities to develop systems that could beat the accuracy of its recommendation system, Cinematch. We briefly describe the challenge itself, review related work and efforts, and summarize visible progress to date. Other potential uses of the data are outlined, including its application to the KDD Cup 2007.\n",
            "\n",
            "Paper ID : e4a85af3f5dc41e13dc2cae9ee851953709b764e \tArticle : Solving the quantum many-body problem with artificial neural networks\n",
            "Author(s) : ['G. Carleo', 'M. Troyer']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning and quantum physics Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.\n",
            "\n",
            "Paper ID : 427b168f490b56716f22b129ac93aba5425ea08f \tArticle : Training linear SVMs in linear time\n",
            "Author(s) : ['T. Joachims']\n",
            "Year : 2006 \n",
            "Abstract : Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets.\n",
            "\n",
            "Paper ID : ccf6a69a7f33bcf052aa7def176d3b9de495beb7 \tArticle : Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n",
            "Author(s) : ['Tolga Bolukbasi', 'Kai-Wei Chang', 'James Y. Zou', 'Venkatesh Saligrama', 'A. Kalai']\n",
            "Year : 2016 \n",
            "Abstract : The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.\n",
            "\n",
            "Paper ID : a96e508a94c37ef847b172e7d31b5bbe25629cbb \tArticle : Data Mining: Concepts and Techniques, 3rd edition\n",
            "Author(s) : ['Jiawei Han', 'M. Kamber', 'J. Pei']\n",
            "Year : 2006 \n",
            "Abstract : The book Knowledge Discovery in Databases, edited by Piatetsky-Shapiro and Frawley [PSF91], is an early collection of research papers on knowledge discovery from data. The book Advances in Knowledge Discovery and Data Mining, edited by Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy [FPSSe96], is a collection of later research results on knowledge discovery and data mining. There have been many data mining books published in recent years, including Predictive Data Mining by Weiss and Indurkhya [WI98], Data Mining Solutions: Methods and Tools for Solving Real-World Problems by Westphal and Blaxton [WB98], Mastering Data Mining: The Art and Science of Customer Relationship Management by Berry and Linofi [BL99], Building Data Mining Applications for CRM by Berson, Smith, and Thearling [BST99], Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank [WF05], Principles of Data Mining (Adaptive Computation and Machine Learning) by Hand, Mannila, and Smyth [HMS01], The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman [HTF01], Data Mining: Introductory and Advanced Topics by Dunham, and Data Mining: Multimedia, Soft Computing, and Bioinformatics by Mitra and Acharya [MA03]. There are also books containing collections of papers on particular aspects of knowledge discovery, such as Machine Learning and Data Mining: Methods and Applications edited by Michalski, Brakto, and Kubat [MBK98], and Relational Data Mining edited by Dzeroski and Lavrac [De01], as well as many tutorial notes on data mining in major database, data mining and machine learning conferences.\n",
            "\n",
            "Paper ID : fc7a5018c590332b02e077ca056bdf8597d4cc9a \tArticle : DATA CLASSIFICATION USING SUPPORT VECTOR MACHINE\n",
            "Author(s) : ['D. Srivastava', 'L. Bhambhu']\n",
            "Year : 2009 \n",
            "Abstract : Classification is one of the most important tasks for different application such as text categorization, tone recognition, image classification, micro-array gene expression, proteins structure predictions, data Classification etc. Most of the existing supervised classification methods are based on traditional statistics, which can provide ideal results when sample size is tending to infinity. However, only finite samples can be acquired in practice. In this paper, a novel learning method, Support Vector Machine (SVM), is applied on different data (Diabetes data, Heart Data, Satellite Data and Shuttle data) which have two or multi class. SVM, a powerful machine method developed from statistical learning and has made significant achievement in some field. Introduced in the early 90’s, they led to an explosion of interest in machine learning. The foundations of SVM have been developed by Vapnik and are gaining popularity in field of machine learning due to many attractive features and promising empirical performance. SVM method does not suffer the limitations of data dimensionality and limited samples [1] & [2]. In our experiment, the support vectors, which are critical for classification, are obtained by learning from the training samples. In this paper we have shown the comparative results using different kernel functions for all data samples.\n",
            "------------------------------------Extracting Page #82------------------------------------\n",
            "\n",
            "Paper ID : b57c54350769ffa59ff57f79ee5aad918844d298 \tArticle : Differentially Private Empirical Risk Minimization\n",
            "Author(s) : ['Kamalika Chaudhuri', 'C. Monteleoni', 'A. Sarwate']\n",
            "Year : 2011 \n",
            "Abstract : Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.\n",
            "\n",
            "Paper ID : b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a \tArticle : Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution\n",
            "Author(s) : ['Lei Yu', 'Huan Liu']\n",
            "Year : 2003 \n",
            "Abstract : Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality\n",
            "\n",
            "Paper ID : 2d86e239a9e9741f22be1d8c1feed7a44da1bdc1 \tArticle : An introduction to Support Vector Machines\n",
            "Author(s) : ['N. Cristianini', 'J. Shawe-Taylor']\n",
            "Year : 2000 \n",
            "Abstract : This book is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. The book also introduces Bayesian analysis of learning and relates SVMs to Gaussian Processes and other kernel based learning methods. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc. Their first introduction in the early 1990s lead to a recent explosion of applications and deepening theoretical analysis, that has now established Support Vector Machines along with neural networks as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and application of these techniques. The concepts are introduced gradually in accessible and self-contained stages, though in each stage the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally the book will equip the practitioner to apply the techniques and an associated web site will provide pointers to updated literature, new applications, and on-line software.\n",
            "\n",
            "Paper ID : d079a2f877f554e00f71a6975435d8325987bdf5 \tArticle : Return of Frustratingly Easy Domain Adaptation\n",
            "Author(s) : ['Baochen Sun', 'Jiashi Feng', 'Kate Saenko']\n",
            "Year : 2016 \n",
            "Abstract : \n",
            " \n",
            " Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.\n",
            " \n",
            "\n",
            "\n",
            "Paper ID : 885af28a751553be48a25b411a5d492767d4cf65 \tArticle : Ensemble Classifiers for Steganalysis of Digital Media\n",
            "Author(s) : ['Jan Kodovský', 'J. Fridrich', 'V. Holub']\n",
            "Year : 2012 \n",
            "Abstract : Today, the most accurate steganalysis methods for digital media are built as supervised classifiers on feature vectors extracted from the media. The tool of choice for the machine learning seems to be the support vector machine (SVM). In this paper, we propose an alternative and well-known machine learning tool-ensemble classifiers implemented as random forests-and argue that they are ideally suited for steganalysis. Ensemble classifiers scale much more favorably w.r.t. the number of training examples and the feature dimensionality with performance comparable to the much more complex SVMs. The significantly lower training complexity opens up the possibility for the steganalyst to work with rich (high-dimensional) cover models and train on larger training sets-two key elements that appear necessary to reliably detect modern steganographic algorithms. Ensemble classification is portrayed here as a powerful developer tool that allows fast construction of steganography detectors with markedly improved detection accuracy across a wide range of embedding methods. The power of the proposed framework is demonstrated on three steganographic methods that hide messages in JPEG images.\n",
            "\n",
            "Paper ID : ebfa3fb595ca0299ab5249c69374c98b74bb62b3 \tArticle : A Tutorial on the Cross-Entropy Method\n",
            "Author(s) : ['P. Boer', 'Dirk P. Kroese', 'Shie Mannor', 'R. Rubinstein']\n",
            "Year : 2005 \n",
            "Abstract : The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning.\n",
            "\n",
            "Paper ID : 993d736b0174abf2f713bea9d9642b85a2313cae \tArticle : Estimating Attributes: Analysis and Extensions of RELIEF\n",
            "Author(s) : ['I. Kononenko']\n",
            "Year : 1994 \n",
            "Abstract : In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem.\n",
            "\n",
            "Paper ID : 9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746 \tArticle : The class imbalance problem: A systematic study\n",
            "Author(s) : ['N. Japkowicz', 'Shaju Stephen']\n",
            "Year : 2002 \n",
            "Abstract : In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.\n",
            "\n",
            "Paper ID : 03bf05bcb5fc5ea98c737c3aeaa330e047901b34 \tArticle : Experimental design\n",
            "Author(s) : ['J. Morgan', 'Xinwei Deng']\n",
            "Year : 2012 \n",
            "Abstract : Maximizing data information requires careful selection, termed design, of the points at which data are observed. Experimental design is reviewed here for broad classes of data collection and analysis problems, including: fractioning techniques based on orthogonal arrays, Latin hypercube designs and their variants for computer experimentation, efficient design for data mining and machine learning applications, and sequential design for active learning. © 2012 Wiley Periodicals, Inc.\n",
            "\n",
            "Paper ID : bae3eda9605700b14237f4d04652ab6759c68eef \tArticle : Artificial intelligence - a modern approach, 2nd Edition\n",
            "Author(s) : ['Stuart J. Russell', 'Peter Norvig']\n",
            "Year : 2003 \n",
            "Abstract : Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach\n",
            "------------------------------------Extracting Page #83------------------------------------\n",
            "\n",
            "Paper ID : 5194b668c67aa83c037e71599a087f63c98eb713 \tArticle : A sequential algorithm for training text classifiers\n",
            "Author(s) : ['D. Lewis', 'W. Gale']\n",
            "Year : 1994 \n",
            "Abstract : The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.\n",
            "\n",
            "Paper ID : 44e915a220ce74badf755aae870fa0b69ee2b82a \tArticle : Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval\n",
            "Author(s) : ['D. Lewis']\n",
            "Year : 1998 \n",
            "Abstract : The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.\n",
            "\n",
            "Paper ID : 043f084e379a44608c470059c2aa174a323e9774 \tArticle : Counterfactual Fairness\n",
            "Author(s) : ['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.\n",
            "\n",
            "Paper ID : 28cf6da1cdcfc1f95b8c31b13d975837257766f2 \tArticle : Coordinate descent algorithms\n",
            "Author(s) : ['Stephen J. Wright']\n",
            "Year : 2015 \n",
            "Abstract : Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.\n",
            "\n",
            "Paper ID : f46714d200d69eb9cb5cce176297b89a3f5e3a2c \tArticle : An Introduction to Convolutional Neural Networks\n",
            "Author(s) : ['K. O’Shea', 'Ryan Nash']\n",
            "Year : 2015 \n",
            "Abstract : The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. \n",
            "This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.\n",
            "\n",
            "Paper ID : 9257779eed46107bcdce9f4dc86298572ff466ce \tArticle : Automated learning of decision rules for text categorization\n",
            "Author(s) : ['C. Apté', 'Fred J. Damerau', 'S. Weiss']\n",
            "Year : 1994 \n",
            "Abstract : We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.\n",
            "\n",
            "Paper ID : 317794c81f54371dda5950a5ee7a41ed10298ab2 \tArticle : How to Grow a Mind: Statistics, Structure, and Abstraction\n",
            "Author(s) : ['J. Tenenbaum', 'Charles Kemp', 'T. Griffiths', 'Noah D. Goodman']\n",
            "Year : 2011 \n",
            "Abstract : In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?\n",
            "\n",
            "Paper ID : e86f71ca2948d17b003a5f068db1ecb2b77827f7 \tArticle : Concrete Problems in AI Safety\n",
            "Author(s) : ['Dario Amodei', 'C. Olah', 'J. Steinhardt', 'P. Christiano', 'J. Schulman', 'Dandelion Mané']\n",
            "Year : 2016 \n",
            "Abstract : Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.\n",
            "\n",
            "Paper ID : 7f57e9939560562727344c1c987416285ef76cda \tArticle : Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition\n",
            "Author(s) : ['Mahmood Sharif', 'Sruti Bhagavatula', 'Lujo Bauer', 'M. Reiter']\n",
            "Year : 2016 \n",
            "Abstract : Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.\n",
            "\n",
            "Paper ID : 455d9a4ff96561d543acbcb2aa81d6cd8fcd20df \tArticle : Trends & Controversies: Support Vector Machines\n",
            "Author(s) : ['Marti A. Hearst']\n",
            "Year : 1998 \n",
            "Abstract : My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently.\n",
            "------------------------------------Extracting Page #84------------------------------------\n",
            "\n",
            "Paper ID : a5e8141d4e323ff7f1f49dbbce293b0d6f739464 \tArticle : Very Simple Classification Rules Perform Well on Most Commonly Used Datasets\n",
            "Author(s) : ['R. Holte']\n",
            "Year : 2004 \n",
            "Abstract : This article reports an empirical investigation of the accuracy of rules that classify examples on the basis of a single attribute. On most datasets studied, the best of these very simple rules is as accurate as the rules induced by the majority of machine learning systems. The article explores the implications of this finding for machine learning research and applications.\n",
            "\n",
            "Paper ID : 0a289dd59b8043c0fbe3a4d1ae59055476ddb3ff \tArticle : Ensemble Methods: Foundations and Algorithms\n",
            "Author(s) : ['Zhi-Hua Zhou']\n",
            "Year : 2012 \n",
            "Abstract : An up-to-date, self-contained introduction to a state-of-the-art machine learning approach, Ensemble Methods: Foundations and Algorithms shows how these accurate methods are used in real-world tasks. It gives you the necessary groundwork to carry out further research in this evolving field. After presenting background and terminology, the book covers the main algorithms and theories, including Boosting, Bagging, Random Forest, averaging and voting schemes, the Stacking method, mixture of experts, and diversity measures. It also discusses multiclass extension, noise tolerance, error-ambiguity and bias-variance decompositions, and recent progress in information theoretic diversity. Moving on to more advanced topics, the author explains how to achieve better performance through ensemble pruning and how to generate better clustering results by combining multiple clusterings. In addition, he describes developments of ensemble methods in semi-supervised learning, active learning, cost-sensitive learning, class-imbalance learning, and comprehensibility enhancement.\n",
            "\n",
            "Paper ID : 5e095981ebf4d389e9356bd56e59e0ade1b42e88 \tArticle : 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text\n",
            "Author(s) : ['Özlem Uzuner', 'B. South', 'Shuying Shen', 'S. Duvall']\n",
            "Year : 2011 \n",
            "Abstract : The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.\n",
            "\n",
            "Paper ID : 7547fd7c5e4bc3b8b8bf714583684ff187e8a382 \tArticle : An assessment of support vector machines for land cover classification\n",
            "Author(s) : ['Chengquan Huang', 'L. Davis', 'J. Townshend']\n",
            "Year : 2002 \n",
            "Abstract : The support vector machine (SVM) is a group of theoretically superior machine learning algorithms. It was found competitive with the best available machine learning algorithms in classifying high-dimensional data sets. This paper gives an introduction to the theoretical development of the SVM and an experimental evaluation of its accuracy, stability and training speed in deriving land cover classifications from satellite images. The SVM was compared to three other popular classifiers, including the maximum likelihood classifier (MLC), neural network classifiers (NNC) and decision tree classifiers (DTC). The impacts of kernel configuration on the performance of the SVM and of the selection of training data and input variables on the four classifiers were also evaluated in this experiment.\n",
            "\n",
            "Paper ID : 28f7cc049f3f4a4db81f0d0a608a4f57636cc35b \tArticle : Quantum-chemical insights from deep tensor neural networks\n",
            "Author(s) : ['Kristof T. Schütt', 'F. Arbabzadah', 'Stefan Chmiela', 'K. Müller', 'A. Tkatchenko']\n",
            "Year : 2017 \n",
            "Abstract : Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks, which leads to size-extensive and uniformly accurate (1 kcal mol−1) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the model reveals a classification of aromatic rings with respect to their stability. Further applications of our model for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the potential of machine learning for revealing insights into complex quantum-chemical systems.\n",
            "\n",
            "Paper ID : eec471897375942fd690b736c2753bb19d907273 \tArticle : Gradient boosting machines, a tutorial\n",
            "Author(s) : ['Alexey Natekin', 'A. Knoll']\n",
            "Year : 2013 \n",
            "Abstract : Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.\n",
            "\n",
            "Paper ID : ade03d0c772c35dc8e865bdb41d7bc54d5b782d1 \tArticle : kernlab - An S4 Package for Kernel Methods in R\n",
            "Author(s) : ['Alexandros Karatzoglou', 'A. Smola', 'K. Hornik', 'A. Zeileis']\n",
            "Year : 2004 \n",
            "Abstract : kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.\n",
            "\n",
            "Paper ID : 10f919b1a5161b560504c225cfb2d1b3a4768f80 \tArticle : Artificial intelligence in healthcare: past, present and future\n",
            "Author(s) : ['F. Jiang', 'Yong Jiang', 'Hui Zhi', 'Yi Dong', 'Hao Li', 'Sufeng Ma', 'Yilong Wang', 'Q. Dong', 'Haipeng Shen', 'Yongjun Wang']\n",
            "Year : 2017 \n",
            "Abstract : Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.\n",
            "\n",
            "Paper ID : ad3c82ada3ff848bb36bade1d90820c2e465b2d7 \tArticle : Feature Selection\n",
            "Author(s) : ['Jundong Li', 'Kewei Cheng', 'Suhang Wang', 'Fred Morstatter', 'Robert P. Trevino', 'Jiliang Tang', 'Huan Liu']\n",
            "Year : 2010 \n",
            "Abstract : Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.\n",
            "\n",
            "Paper ID : 5c89852b90a1e9e506d237749c745bf42ac0f737 \tArticle : A Survey of Text Classification Algorithms\n",
            "Author(s) : ['C. Aggarwal', 'ChengXiang Zhai']\n",
            "Year : 2012 \n",
            "Abstract : The problem of classification has been widely studied in the data mining, machine learning, database, and information retrieval communities with applications in a number of diverse domains, such as target marketing, medical diagnosis, news group filtering, and document organization. In this paper we will provide a survey of a wide variety of text classification algorithms.\n",
            "------------------------------------Extracting Page #85------------------------------------\n",
            "\n",
            "Paper ID : 938f6ef7eed095919e6a482c7f1836a01d62db4b \tArticle : Google Vizier: A Service for Black-Box Optimization\n",
            "Author(s) : ['D. Golovin', 'Benjamin Solnik', 'Subhodeep Moitra', 'G. Kochanski', 'J. Karro', 'D. Sculley']\n",
            "Year : 2017 \n",
            "Abstract : Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Google's Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.\n",
            "\n",
            "Paper ID : 561c3fa53d36405186da9cab02bd68635c3738aa \tArticle : Molecular graph convolutions: moving beyond fingerprints\n",
            "Author(s) : ['S. Kearnes', 'Kevin McCloskey', 'Marc Berndl', 'V. Pande', 'Patrick F. Riley']\n",
            "Year : 2016 \n",
            "Abstract : Molecular “fingerprints” encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph—atoms, bonds, distances, etc.—which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.\n",
            "\n",
            "Paper ID : 310ea531640728702fce6c743c1dd680a23d2ef4 \tArticle : Feature Selection for Classification: A Review\n",
            "Author(s) : ['Jiliang Tang', 'Salem Alelyani', 'Huan Liu']\n",
            "Year : 2014 \n",
            "Abstract : Nowadays, the growth of the high-throughput technologies has resulted in exponential growth in the harvested data with respect to both dimensionality and sample size. The trend of this growth of the UCI machine learning repository is shown in Figure 1. Efficient and effective management of these data becomes increasing challenging. Traditionally manual management of these datasets to be impractical. Therefore, data mining and machine learning techniques were developed to automatically discover knowledge and recognize ...\n",
            "\n",
            "Paper ID : 52b7bf3ba59b31f362aa07f957f1543a29a4279e \tArticle : Support-Vector Networks\n",
            "Author(s) : ['Corinna Cortes', 'V. Vapnik']\n",
            "Year : 2004 \n",
            "Abstract : The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.\n",
            "\n",
            "Paper ID : 759d9a6c9206c366a8d94a06f4eb05659c2bb7f2 \tArticle : Toward Open Set Recognition\n",
            "Author(s) : ['W. Scheirer', 'A. Rocha', 'Archana Sapkota', 'T. Boult']\n",
            "Year : 2013 \n",
            "Abstract : To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of “closed set” recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is “open set” recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel “1-vs-set machine,” which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.\n",
            "\n",
            "Paper ID : 93cb06180743fa648d844b9e7883b62468921c84 \tArticle : Pattern Recognition and Neural Networks\n",
            "Author(s) : ['B. Ripley']\n",
            "Year : 1996 \n",
            "Abstract : From the Publisher: \n",
            "Pattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.\n",
            "\n",
            "Paper ID : 80196cdfcd0c6ce2953bf65a7f019971e2026386 \tArticle : IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\n",
            "Author(s) : ['Lasse Espeholt', 'Hubert Soyer', 'R. Munos', 'K. Simonyan', 'Volodymyr Mnih', 'Tom Ward', 'Yotam Doron', 'Vlad Firoiu', 'Tim Harley', 'Iain Dunning', 'S. Legg', 'K. Kavukcuoglu']\n",
            "Year : 2018 \n",
            "Abstract : In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.\n",
            "\n",
            "Paper ID : 997bbde7333fb945e9086b36b54ff1a214a151a0 \tArticle : Introduction\n",
            "Author(s) : ['Satinder Singh']\n",
            "Year : 2002 \n",
            "Abstract : This is the third special issue of Machine Learning on the subject of reinforcement learning (the first and second special issues were edited by Richard Sutton in 1992 and Leslie Kaelbling in 1996 respectively). The field of reinforcement learning continues to grow, attracting ideas and participants not only from AI and machine learning but also from neuroscience, cognitive science, operations research, and control. More than a decade of the resulting research has led to great progress in the theoretical underpinnings of the field, much of it derived from the theory of dynamic programming and the associated frameworks of Markov Decision Processes (MDPs), semi-MDPs and partially observable MDPs (POMDPs) (see texts by Sutton and Barto (1998) and Bertsekas and Tsitsiklis (1996) for excellent overviews). \n",
            "\n",
            "Much has been accomplished, and yet, of course, much remains to be done. I will take advantage of this guest editorial to outline my general views on three of the open issues that are key to further rapid progress in reinforcement learning, and then turn to very briefly survey the papers in this special issue.\n",
            "\n",
            "Paper ID : e3d8a9d35232d7967780019f0fa93626ca1373cb \tArticle : Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation\n",
            "Author(s) : ['Marina Sokolova', 'N. Japkowicz', 'S. Szpakowicz']\n",
            "Year : 2006 \n",
            "Abstract : Different evaluation measures assess different characteristics of machine learning algorithms. The empirical evaluation of algorithms and classifiers is a matter of on-going debate among researchers. Most measures in use today focus on a classifier's ability to identify classes correctly. We note other useful properties, such as failure avoidance or class discrimination, and we suggest measures to evaluate such properties. These measures – Youden's index, likelihood, Discriminant power – are used in medical diagnosis. We show that they are interrelated, and we apply them to a case study from the field of electronic negotiations. We also list other learning problems which may benefit from the application of these measures.\n",
            "\n",
            "Paper ID : ecfdfcd1ef21449a01701122745a7be5ba135e7f \tArticle : Data Mining for Imbalanced Datasets: An Overview\n",
            "Author(s) : ['N. Chawla']\n",
            "Year : 2005 \n",
            "Abstract : A dataset is imbalanced if the classification categories are not approximately equally represented. Recent years brought increased interest in applying machine learning techniques to difficult “real-world” problems, many of which are characterized by imbalanced data. Additionally the distribution of the testing data may differ from that of the training data, and the true misclassification costs may be unknown at learning time. Predictive accuracy, a popular choice for evaluating performance of a classifier, might not be appropriate when the data is imbalanced and/or the costs of different errors vary markedly. In this Chapter, we discuss some of the sampling techniques used for balancing the datasets, and the performance measures more appropriate for mining imbalanced datasets.\n",
            "------------------------------------Extracting Page #86------------------------------------\n",
            "\n",
            "Paper ID : 6120cc252bc74239012f11b8b075cb7cb16bee26 \tArticle : An Introduction to Variational Methods for Graphical Models\n",
            "Author(s) : ['Michael I. Jordan', 'Zoubin Ghahramani', 'T. Jaakkola', 'L. Saul']\n",
            "Year : 2004 \n",
            "Abstract : This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.\n",
            "\n",
            "Paper ID : a9763afda62e960c35c80681f805ddecbef14a92 \tArticle : Images of Organization\n",
            "Author(s) : ['J. Alexander', 'G. Morgan']\n",
            "Year : 1986 \n",
            "Abstract : Preface Part I. An Overview Introduction Part II. Some Images of Organization 2. Mechanization Takes Command: Organizations as Machines Machines, Mechanical Thinking, and the Rise of Bureaucratic Organization The Origins of Mechanistic Organization Classical Management Theory: Designing bureaucratic organizations Scientific Management Strengths and Limitations of the Machine Metaphor 3. Nature Intervenes: Organizations as Organisms Discovering Organizational Needs Recognizing the Importance of Environment: Organizations as Open Systems Contingency Theory: Adapting Organization to Environment The Variety of the Species Contingency Theory: Promoting Organizational Health and Development Natural Selection: The Population-Ecology View of Organizations Organizational Ecology: The Creation of Shared Futures Strengths and Limitations of the Organismic Metaphor 4. Learning and Self-Organization: Organizations as Brains Images of the Brain Organizations as Information Processing Brains Creating Learning Organizations Cybernetics, Learning, and Learning to Learn Can Organizations Learn to Learn? Guidelines for \"Learning Organizations\" Organizations as Holographic Brains Principles of Holographic Design Strengths and Limitations of the Brain Metaphors 5. Creating Social Realty: Organizations as Cultures Culture and Organization Organization as a Cultural Phenomenon Organization and Cultural Context Corporate Cultures and Subcultures Creating Organizational Reality Culture: Rule Following or Enactment? Organization: The enactment of a Shared Reality Strengths and Limitations of the Cultural Metaphor 6. Interests, Conflict, and Power: Organizations as Political Systems Organizations as Systems of Government Organizations as Systems of Political Activity Analyzing Interests Understanding Conflict Exploring Power Managing Pluralist Organizations Strengths and Limitations of the Political Metaphor 7. Exploring Plato's Cave: Organizations as Psychic Prisons The Trap of Favored Ways of Thinking Organization and the Unconscious Organization and Repressed Sexuality Organization and the Patriarchal Family Organization, Death, and Immortality Organization and Anxiety Organization, Dolls, and Teddy Bears Organization, Shadow, and Archetype The Unconscious: A Creative and Destructive Force Strengths and Limitations of the Psychic Prison Metaphor 8. Unfolding Logics of Change: Organization as Flux and Transformation Autopoiesis: Rethinking Relations With the Environment Enactment as a Form of Narcissism: Organizations Interact With Projections of Themselves Identity and Closure: Egocentrism Versus Systemic Wisdom Shifting \"Attractors\": The Logic of Chaos and Complexity Managing in the Midst of Complexity Loops, Not Lines: The Logic of Mutual Causality Contradiction and Crisis: The Logic of Dialectical Change Dialectical Analysis: How Opposing Forces Drive Change The Dialectics of Management Strengths and Limitations of the Flux and Transformation Metaphor 9. The Ugly Face: Organizations as Instruments of Domination Organization as Domination How Organizations Use and Exploit Their Employees Organization, Class, and Control Work Hazards, Occupational Disease, and Industrial Accidents Workaholism and Social and Mental Stress Organizational Politics and the Radicalized Organization Multinationals and the World Economy The Multinationals as World Powers Multinationals: A Record of Exploitation? Strengths and Limitations of the Domination Metaphor Part III. Implications For Practice 10. The Challenge of Metaphor Metaphors Create Ways of Seeing and Shaping Organizational Life Seeing, Thinking, and Acting in New Ways 11. Reading and Shaping Organizational Life The Multicom Case Interpreting Multicom Developing and Detailed Reading and \"Storyline\" Multicom From Another View \"Reading\" and Emergent Intelligence 12. Postscript Bibliographic Notes Introduction The Machine Metaphor The Organismic Metaphor The Brain Metaphor The Culture Metaphor The Political Metaphor The Psychic Prison Metaphor The Flux and Transformation Metaphor The Domination Metaphor The Challenge of Metaphor Reading and Shaping Organizational Life Postscript Bibliography\n",
            "\n",
            "Paper ID : e211ec0fdaee8bec696475eaffae05af32222b9b \tArticle : Semantics derived automatically from language corpora contain human-like biases\n",
            "Author(s) : ['Aylin Caliskan', 'J. Bryson', 'A. Narayanan']\n",
            "Year : 2017 \n",
            "Abstract : Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.\n",
            "\n",
            "Paper ID : 0407b605b8f55db72e2545586bfe8e946b691b70 \tArticle : An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks\n",
            "Author(s) : ['Ian J. Goodfellow', 'Mehdi Mirza', 'Xia Da', 'Aaron C. Courville', 'Yoshua Bengio']\n",
            "Year : 2014 \n",
            "Abstract : Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.\n",
            "\n",
            "Paper ID : 0c91d5305ad34814b631d4a642bb0535a2e066ea \tArticle : Feature selection based on mutual information\n",
            "Author(s) : ['Muhammad Aliyu Sulaiman', 'J. Labadin']\n",
            "Year : 2015 \n",
            "Abstract : The application of machine learning models such as support vector machine (SVM) and artificial neural networks (ANN) in predicting reservoir properties has been effective in the recent years when compared with the traditional empirical methods. Despite that the machine learning models suffer a lot in the faces of uncertain data which is common characteristics of well log dataset. The reason for uncertainty in well log dataset includes a missing scale, data interpretation and measurement error problems. Feature Selection aimed at selecting feature subset that is relevant to the predicting property. In this paper a feature selection based on mutual information criterion is proposed, the strong point of this method relies on the choice of threshold based on statistically sound criterion for the typical greedy feedforward method of feature selection. Experimental results indicate that the proposed method is capable of improving the performance of the machine learning models in terms of prediction accuracy and reduction in training time.\n",
            "\n",
            "Paper ID : 79f2626046fdc56edfaca840874e355cac734b9a \tArticle : Ad click prediction: a view from the trenches\n",
            "Author(s) : ['H. B. McMahan', 'Gary Holt', 'D. Sculley', 'Michael Young', 'D. Ebner', 'Julian Grady', 'Lan Nie', 'Todd Phillips', 'Eugene Davydov', 'D. Golovin', 'S. Chikkerur', 'Dan Liu', 'M. Wattenberg', 'A. M. Hrafnkelsson', 'T. Boulos', 'J. Kubica']\n",
            "Year : 2013 \n",
            "Abstract : Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.\n",
            "\n",
            "Paper ID : 24e6cf0796237f21c780a3f0c996817f57b3a1bd \tArticle : Support-vector networks\n",
            "Author(s) : ['Corinna Cortes', 'V. Vapnik']\n",
            "Year : 2004 \n",
            "Abstract : Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.\n",
            "\n",
            "Paper ID : 864e7db59f2ccfec1ee9f6eba79566ac7b0634df \tArticle : Convolutional Pose Machines\n",
            "Author(s) : ['Shih-En Wei', 'V. Ramakrishna', 'T. Kanade', 'Yaser Sheikh']\n",
            "Year : 2016 \n",
            "Abstract : Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.\n",
            "\n",
            "Paper ID : 4f51a64793d3b2a60e9e5846c31dae023cf5c69a \tArticle : Unmasking Clever Hans predictors and assessing what machines really learn\n",
            "Author(s) : ['S. Lapuschkin', 'S. Wäldchen', 'Alexander Binder', 'G. Montavon', 'W. Samek', 'K. Müller']\n",
            "Year : 2019 \n",
            "Abstract : Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.Nonlinear machine learning methods have good predictive ability but the lack of transparency of the algorithms can limit their use. Here the authors investigate how these methods approach learning in order to assess the dependability of their decision making.\n",
            "\n",
            "Paper ID : 9dc134b18c06577354d50c12a8972b965d3bbacd \tArticle : Learning to Decode Cognitive States from Brain Images\n",
            "Author(s) : ['Tom Michael Mitchell', 'R. Hutchinson', 'R. Niculescu', 'Francisco Pereira', 'Xuerui Wang', 'M. Just', 'S. Newman']\n",
            "Year : 2004 \n",
            "Abstract : Over the past decade, functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain. A typical fMRI experiment can produce a three-dimensional image related to the human subject's brain activity every half second, at a spatial resolution of a few millimeters. As in other modern empirical sciences, this new instrumentation has led to a flood of new data, and a corresponding need for new data analysis methods. We describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on fRMI data observed over a single time interval. In particular, we present case studies in which we have successfully trained classifiers to distinguish cognitive states such as (1) whether the human subject is looking at a picture or a sentence, (2) whether the subject is reading an ambiguous or non-ambiguous sentence, and (3) whether the word the subject is viewing is a word describing food, people, buildings, etc. This learning problem provides an interesting case study of classifier learning from extremely high dimensional (105 features), extremely sparse (tens of training examples), noisy data. This paper summarizes the results obtained in these three case studies, as well as lessons learned about how to successfully apply machine learning methods to train classifiers in such settings.\n",
            "------------------------------------Extracting Page #87------------------------------------\n",
            "\n",
            "Paper ID : b8012351bc5ebce4a4b3039bbbba3ce393bc3315 \tArticle : An empirical evaluation of deep architectures on problems with many factors of variation\n",
            "Author(s) : ['H. Larochelle', 'D. Erhan', 'Aaron C. Courville', 'J. Bergstra', 'Yoshua Bengio']\n",
            "Year : 2007 \n",
            "Abstract : Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.\n",
            "\n",
            "Paper ID : 33e46a618fdb22d46951f548d6ceeb384e7f1687 \tArticle : Pedestrian Detection via Classification on Riemannian Manifolds\n",
            "Author(s) : ['Oncel Tuzel', 'F. Porikli', 'P. Meer']\n",
            "Year : 2008 \n",
            "Abstract : We present a new algorithm to detect pedestrian in still images utilizing covariance matrices as object descriptors. Since the descriptors do not form a vector space, well known machine learning techniques are not well suited to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the space. The algorithm is tested on INRIA and DaimlerChrysler pedestrian datasets where superior detection rates are observed over the previous approaches.\n",
            "\n",
            "Paper ID : 6d5965a76f88a8ebab4fc9c43a3ae2630628966a \tArticle : Learning and evaluating classifiers under sample selection bias\n",
            "Author(s) : ['B. Zadrozny']\n",
            "Year : 2004 \n",
            "Abstract : Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.\n",
            "\n",
            "Paper ID : 3bc4736f9b8512043ed47357a81f26b93a1204b6 \tArticle : Semi-supervised learning with graphs\n",
            "Author(s) : ['Xiaojin Zhu', 'J. Lafferty', 'R. Rosenfeld']\n",
            "Year : 2005 \n",
            "Abstract : In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. \n",
            "We present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end.\n",
            "\n",
            "Paper ID : 0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff \tArticle : Solving large scale linear prediction problems using stochastic gradient descent algorithms\n",
            "Author(s) : ['Tong Zhang']\n",
            "Year : 2004 \n",
            "Abstract : Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.\n",
            "\n",
            "Paper ID : b8e37682c847844a4b5c4851239fdc3357d5577b \tArticle : Lecture Notes in Artificial Intelligence\n",
            "Author(s) : ['P. Brézillon', 'P. Bouquet']\n",
            "Year : 1999 \n",
            "Abstract : LNAI was established in the mid-1980s as a topical subseries of LNCS focusing on artificial intelligence. This subseries is devoted to the publication of state-of-the-art research results in artificial intelligence, at a high level and in both printed and electronic versions making use of the well-established LNCS publication machinery. As with the LNCS mother series, proceedings and postproceedings are at the core of LNAI; however, all other sublines are available for LNAI as well. The topics in LNAI include automated reasoning, automated programming, algorithms, knowledge representation, agent-based systems, intelligent systems, expert systems, machine learning, natural-language processing, machine vision, robotics, search systems, knowledge discovery, data mining, and related programming languages.\n",
            "\n",
            "Paper ID : ec76f55da5c6df30f6e4c9e4945bd3304d508ef7 \tArticle : Fuzzy support vector machines\n",
            "Author(s) : ['Chun-fu Lin', 'Sheng-de Wang']\n",
            "Year : 2002 \n",
            "Abstract : A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).\n",
            "\n",
            "Paper ID : b3b3c562a45d7710d6f62ad8f210ebca9a47d23f \tArticle : Who should fix this bug?\n",
            "Author(s) : ['J. Anvik', 'L. Hiew', 'G. Murphy']\n",
            "Year : 2006 \n",
            "Abstract : Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.\n",
            "\n",
            "Paper ID : 6c55c7afd86bdfb80d57ee9f939d03d342152bb5 \tArticle : Designing neural networks through neuroevolution\n",
            "Author(s) : ['Kenneth O. Stanley', 'J. Clune', 'J. Lehman', 'R. Miikkulainen']\n",
            "Year : 2019 \n",
            "Abstract : Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.Deep neural networks have become very successful at certain machine learning tasks partly due to the widely adopted method of training called backpropagation. An alternative way to optimize neural networks is by using evolutionary algorithms, which, fuelled by the increase in computing power, offers a new range of capabilities and modes of learning.\n",
            "\n",
            "Paper ID : 29650544fded20dd5b2fc49f60f9a3ad30d0e275 \tArticle : Speech Recognition Using Deep Neural Networks: A Systematic Review\n",
            "Author(s) : ['A. B. Nassif', 'I. Shahin', 'Imtinan B. Attili', 'Mohammad Azzeh', 'K. Shaalan']\n",
            "Year : 2019 \n",
            "Abstract : Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.\n",
            "------------------------------------Extracting Page #88------------------------------------\n",
            "\n",
            "Paper ID : a4c940cbe96ce4f181b7c1bb03caacf7efcad5b2 \tArticle : Robot Programming by Demonstration\n",
            "Author(s) : ['A. Billard', 'S. Calinon', 'R. Dillmann', 'S. Schaal']\n",
            "Year : 2008 \n",
            "Abstract : Also referred to as learning by imitation, tutelage, or apprenticeship learning, Programming by Demonstration (PbD) develops methods by which new skills can be transmitted to a robot. This book examines methods by which robots learn new skills through human guidance. Taking a practical perspective, it covers a broad range of applications, including service robots. The text addresses the challenges involved in investigating methods by which PbD is used to provide robots with a generic and adaptive model of control. Drawing on findings from robot control, human-robot interaction, applied machine learning, artificial intelligence, and developmental and cognitive psychology, the book contains a large set of didactic and illustrative examples. Practical and comprehensive machine learning source codes are available on the books companion website: http://www.programming-by-demonstration.org\n",
            "\n",
            "Paper ID : 10b496ad48513f8585aa56f2c682159357858960 \tArticle : Understanding Data Augmentation for Classification: When to Warp?\n",
            "Author(s) : ['Sebastien C. Wong', 'A. Gatt', 'V. Stamatescu', 'M. McDonnell']\n",
            "Year : 2016 \n",
            "Abstract : In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.\n",
            "\n",
            "Paper ID : e75b3c12da067552fda910a5bbed8b4d0e82dbcb \tArticle :  Neural Network Methods for Natural Language Processing\n",
            "Author(s) : ['Yoav Goldberg']\n",
            "Year : 2017 \n",
            "Abstract : Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.\n",
            "\n",
            "The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.\n",
            "\n",
            "Paper ID : 4e11ebdc3e4f30d774458fab9e4b45ff0d0aa971 \tArticle : Logical and relational learning\n",
            "Author(s) : ['L. D. Raedt']\n",
            "Year : 2008 \n",
            "Abstract : I use the term logical and relational learning (LRL) to refer to the subfield of machine learning and data mining that is concerned with learning in expressive logical or relational representations. It is the union of inductive logic programming, (statistical) relational learning and multi-relational data mining and constitutes a general class of techniques and methodology for learning from structured data (such as graphs, networks, relational databases) and background knowledge. During the course of its existence, logical and relational learning has changed dramatically. Whereas early work was mainly concerned with logical issues (and even program synthesis from examples), in the 90s its focus was on the discovery of new and interpretable knowledge from structured data, often in the form of rules or patterns. Since then the range of tasks to which logical and relational learning has been applied has significantly broadened and now covers almost all machine learning problems and settings. Today, there exist logical and relational learning methods for reinforcement learning, statistical learning, distance- and kernel-based learning in addition to traditional symbolic machine learning approaches. At the same time, logical and relational learning problems are appearing everywhere. Advances in intelligent systems are enabling the generation of high-level symbolic and structured data in a wide variety of domains, including the semantic web, robotics, vision, social networks, and the life sciences, which in turn raises new challenges and opportunities for logical and relational learning. These developments have led to a new view on logical and relational learning and its role in machine learning and artificial intelligence. In this talk, I shall reflect on this view by identifying some of the lessons learned in logical and relational learning and formulating some challenges for future developments.\n",
            "\n",
            "Paper ID : 77703a2783f64dfceb638aa9eebd9c9c501bb835 \tArticle : The Case against Accuracy Estimation for Comparing Induction Algorithms\n",
            "Author(s) : ['F. Provost', 'Tom Fawcett', 'Ron Kohavi']\n",
            "Year : 1998 \n",
            "Abstract : We analyze critically the use of classi cation accuracy to compare classi ers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classi ers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scienti c conclusions.\n",
            "\n",
            "Paper ID : f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10 \tArticle : Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.\n",
            "Author(s) : ['Xin Yang', 'Yifei Wang', 'R. Byrne', 'G. Schneider', 'Sheng-yong Yang']\n",
            "Year : 2019 \n",
            "Abstract : Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.\n",
            "\n",
            "Paper ID : 0a5ff7336879c99513dca6fce6ef44984ebf3f55 \tArticle : Clipper: A Low-Latency Online Prediction Serving System\n",
            "Author(s) : ['D. Crankshaw', 'Xin Wang', 'Giulio Zhou', 'M. Franklin', 'Joseph E. Gonzalez', 'I. Stoica']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. \n",
            "In this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the TensorFlow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.\n",
            "\n",
            "Paper ID : 8010e480ad33f6f95e80f7361ba8928f07f80a13 \tArticle : Inference for the Generalization Error\n",
            "Author(s) : ['C. Nadeau', 'Yoshua Bengio']\n",
            "Year : 2004 \n",
            "Abstract : In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.\n",
            "\n",
            "Paper ID : cc1cad12521b5aab43fdda5b4dec67586aef1f87 \tArticle : Kernel Methods for Relation Extraction\n",
            "Author(s) : ['D. Zelenko', 'Chinatsu Aone', 'A. Richardella']\n",
            "Year : 2003 \n",
            "Abstract : We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.\n",
            "\n",
            "Paper ID : dd135a89b5075af5cbef5becaf419457cdd77cc9 \tArticle : An Introduction to Restricted Boltzmann Machines\n",
            "Author(s) : ['Asja Fischer', 'C. Igel']\n",
            "Year : 2012 \n",
            "Abstract : Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.\n",
            "------------------------------------Extracting Page #89------------------------------------\n",
            "\n",
            "Paper ID : 55e36d6b45c91a0daa49234bd47b856470d6825c \tArticle : Identifying Sarcasm in Twitter: A Closer Look\n",
            "Author(s) : ['Roberto I. González-Ibáñez', 'S. Muresan', 'N. Wacholder']\n",
            "Year : 2011 \n",
            "Abstract : Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.\n",
            "\n",
            "Paper ID : ac12c9b9e35e58b55d85a97c47886a7371c14afa \tArticle : Data mining in bioinformatics using Weka\n",
            "Author(s) : ['Eibe Frank', 'M. Hall', 'Leonard E. Trigg', 'G. Holmes', 'I. Witten']\n",
            "Year : 2004 \n",
            "Abstract : UNLABELLED\n",
            "The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.\n",
            "\n",
            "\n",
            "AVAILABILITY\n",
            "http://www.cs.waikato.ac.nz/ml/weka.\n",
            "\n",
            "Paper ID : c4a422669ec9b6a60b05d2d2595314008a5fb419 \tArticle : Comparing support vector machines with Gaussian kernels to radial basis function classifiers\n",
            "Author(s) : ['B. Schölkopf', 'K. Sung', 'C. Burges', 'F. Girosi', 'P. Niyogi', 'T. Poggio', 'V. Vapnik']\n",
            "Year : 1997 \n",
            "Abstract : The support vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights, and threshold that minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by X-means clustering, and the weights are computed using error backpropagation. We consider three machines, namely, a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the United States postal service database of handwritten digits, the SV machine achieves the highest recognition accuracy, followed by the hybrid system. The SV approach is thus not only theoretically well-founded but also superior in a practical application.\n",
            "\n",
            "Paper ID : 96abcdded2985bd44b9514e28f5b8da4fa1e4371 \tArticle : The mythos of model interpretability\n",
            "Author(s) : ['Zachary Chase Lipton']\n",
            "Year : 2018 \n",
            "Abstract : In machine learning, the concept of interpretability is both important and slippery.\n",
            "\n",
            "Paper ID : 86e5827087e11dc929d592ee7b3d7581fc48265e \tArticle : Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN\n",
            "Author(s) : ['Weiwei Hu', 'Ying Tan']\n",
            "Year : 2017 \n",
            "Abstract : Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms. Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples' malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.\n",
            "\n",
            "Paper ID : 3335c340c20609b4e6de481c9eaf67ecd6c960dc \tArticle : Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science\n",
            "Author(s) : ['Randal S. Olson', 'Nathan Bartley', 'R. Urbanowicz', 'J. Moore']\n",
            "Year : 2016 \n",
            "Abstract : As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.\n",
            "\n",
            "Paper ID : b69df93991a1f5a712b20e832f5b0281acb3153b \tArticle : Kernel Methods in Computational Biology\n",
            "Author(s) : ['B. Schölkopf', 'K. Tsuda', 'Jean-Philippe Vert']\n",
            "Year : 2003 \n",
            "Abstract : Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology.Following three introductory chapters -- an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology -- the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods.\n",
            "\n",
            "Paper ID : db68a79e59291b85e10300b79c43843b651aa195 \tArticle : Support vector machines for classification in remote sensing\n",
            "Author(s) : ['M. Pal', 'P. Mather']\n",
            "Year : 2005 \n",
            "Abstract : Support vector machines (SVM) represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of two experiments in which multi‐class SVMs are compared with maximum likelihood (ML) and artificial neural network (ANN) methods in terms of classification accuracy. The two land cover classification experiments use multispectral (Landsat‐7 ETM+) and hyperspectral (DAIS) data, respectively, for test areas in eastern England and central Spain. Our results show that the SVM achieves a higher level of classification accuracy than either the ML or the ANN classifier, and that the SVM can be used with small training datasets and high‐dimensional data.\n",
            "\n",
            "Paper ID : 0ea2f1f5c470c4947b48bbd21245fb327282f3b4 \tArticle : Stock market's price movement prediction with LSTM neural networks\n",
            "Author(s) : ['David M. Q. Nelson', 'Adriano M. Pereira', 'Renato A. de Oliveira']\n",
            "Year : 2017 \n",
            "Abstract : Predictions on stock market prices are a great challenge due to the fact that it is an immensely complex, chaotic and dynamic environment. There are many studies from various areas aiming to take on that challenge and Machine Learning approaches have been the focus of many of them. There are many examples of Machine Learning algorithms been able to reach satisfactory results when doing that type of prediction. This article studies the usage of LSTM networks on that scenario, to predict future trends of stock prices based on the price history, alongside with technical analysis indicators. For that goal, a prediction model was built, and a series of experiments were executed and theirs results analyzed against a number of metrics to assess if this type of algorithm presents and improvements when compared to other Machine Learning methods and investment strategies. The results that were obtained are promising, getting up to an average of 55.9% of accuracy when predicting if the price of a particular stock is going to go up or not in the near future.\n",
            "\n",
            "Paper ID : bd5d4f760e8856d51fef2da900a763cca4cbce96 \tArticle : Metalearning - Applications to Data Mining\n",
            "Author(s) : ['P. Brazdil', 'C. Giraud-Carrier', 'Carlos Soares', 'R. Vilalta']\n",
            "Year : 2009 \n",
            "Abstract : Metalearning is the study of principled methods that exploit metaknowledge to obtain efficient models and solutions by adapting machine learning and data mining processes. While the variety of machine learning and data mining techniques now available can, in principle, provide good model solutions, a methodology is still needed to guide the search for the most appropriate model in an efficient way. Metalearning provides one such methodology that allows systems to become more effective through experience. This book discusses several approaches to obtaining knowledge concerning the performance of machine learning and data mining algorithms. It shows how this knowledge can be reused to select, combine, compose and adapt both algorithms and models to yield faster, more effective solutions to data mining problems. It can thus help developers improve their algorithms and also develop learning systems that can improve themselves. The book will be of interest to researchers and graduate students in the areas of machine learning, data mining and artificial intelligence.\n",
            "------------------------------------Extracting Page #90------------------------------------\n",
            "\n",
            "Paper ID : 8e9e7b8084082e599e16d65cc9539b415ee94738 \tArticle : Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF\n",
            "Author(s) : ['I. Kononenko', 'E. Simec', 'M. Robnik-Sikonja']\n",
            "Year : 2004 \n",
            "Abstract : Current inductive machine learning algorithms typically use greedy search with limited lookahead. This prevents them to detect significant conditional dependencies between the attributes that describe training objects. Instead of myopic impurity functions and lookahead, we propose to use RELIEFF, an extension of RELIEF developed by Kira and Rendell [10, 11], for heuristic guidance of inductive learning algorithms. We have reimplemented Assistant, a system for top down induction of decision trees, using RELIEFF as an estimator of attributes at each selection step. The algorithm is tested on several artificial and several real world problems and the results are compared with some other well known machine learning algorithms. Excellent results on artificial data sets and two real world problems show the advantage of the presented approach to inductive learning.\n",
            "\n",
            "Paper ID : 784220a5a2ad452282f8af006a6cb5715d54d0ed \tArticle : Link-Based Classification\n",
            "Author(s) : ['Qing Lu', 'L. Getoor']\n",
            "Year : 2003 \n",
            "Abstract : A key challenge for machine learning is tackling the problem of mining richly structured data sets, where the objects are linked in some way due to either an explicit or implicit relationship that exists between the objects. Links among the objects demonstrate certain patterns, which can be helpful for many machine learning tasks and are usually hard to capture with traditional statistical models. Recently there has been a surge of interest in this area, fueled largely by interest in web and hypertext mining, but also by interest in mining social networks, bibliographic citation data, epidemiological data and other domains best described using a linked or graph structure. In this paper we propose a framework for modeling link distributions, a link-based model that supports discriminative models describing both the link distributions and the attributes of linked objects. We use a structured logistic regression model, capturing both content and links. We systematically evaluate several variants of our link-based model on a range of data sets including both web and citation collections. In all cases, the use of the link distribution improves classification accuracy.\n",
            "\n",
            "Paper ID : 2323173a0bddac0dd2586b17a2f3ac33f401c45c \tArticle : Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)\n",
            "Author(s) : ['Gregory Shakhnarovich', 'Trevor Darrell', 'P. Indyk']\n",
            "Year : 2005 \n",
            "Abstract : Regression and classification methods based on similarity of the input to stored examples have not been widely used in applications involving very large sets of high-dimensional data. Recent advances in computational geometry and machine learning, however, may alleviate the problems in using these methods on large data sets. This volume presents theoretical and practical discussions of nearest-neighbor (NN) methods in machine learning and examines computer vision as an application domain in which the benefit of these advanced methods is often dramatic. It brings together contributions from researchers in theory of computation, machine learning, and computer vision with the goals of bridging the gaps between disciplines and presenting state-of-the-art methods for emerging applications.The contributors focus on the importance of designing algorithms for NN search, and for the related classification, regression, and retrieval tasks, that remain efficient even as the number of points or the dimensionality of the data grows very large. The book begins with two theoretical chapters on computational geometry and then explores ways to make the NN approach practicable in machine learning applications where the dimensionality of the data and the size of the data sets make the naive methods for NN search prohibitively expensive. The final chapters describe successful applications of an NN algorithm, locality-sensitive hashing (LSH), to vision tasks.\n",
            "\n",
            "Paper ID : fd0934a6008ecb9035b5b1e0b91ac890e2e5bd3d \tArticle : A connectionist machine for genetic hillclimbing\n",
            "Author(s) : ['D. Ackley']\n",
            "Year : 1987 \n",
            "Abstract : 1. Introduction.- 1.1. Satisfying hidden strong constraints.- 1.2. Function optimization.- 1.2.1. The methodology of heuristic search.- 1.2.2. The shape of function spaces.- 1.3. High-dimensional binary vector spaces.- 1.3.1. Graph partitioning.- 1.4. Dissertation overview.- 1.5. Summary.- 2. The model.- 2.1. Design goal: Learning while searching.- 2.1.1. Knowledge representation.- 2.1.2. Point-based search strategies.- 2.1.3. Population-based search strategies.- 2.1.4. Combination rules.- 2.1.5. Election rules.- 2.1.6. Summary: Learning while searching.- 2.2. Design goal: Sustained exploration.- 2.2.1. Searching broadly.- 2.2.2. Convergence and divergence.- 2.2.3. Mode transitions.- 2.2.4. Resource allocation via taxation.- 2.2.5. Summary: Sustained exploration.- 2.3. Connectionist computation.- 2.3.1. Units and links.- 2.3.2. A three-state stochastic unit.- 2.3.3. Receptive fields.- 2.4. Stochastic iterated genetic hillclimbing.- 2.4.1. Knowledge representation in SIGH.- 2.4.2. The SIGH control algorithm.- 2.4.3. Formal definition.- 2.5. Summary.- 3. Empirical demonstrations.- 3.1. Methodology.- 3.1.1. Notation.- 3.1.2. Parameter tuning.- 3.1.3. Non-termination.- 3.2. Seven algorithms.- 3.2.1. Iterated hillclimbing-steepest ascent (IHC-SA).- 3.2.2. Iterated hillclimbing-next ascent (IHC-NA).- 3.2.3. Stochastic hillclimbing (SHC).- 3.2.4. Iterated simulated annealing (ISA).- 3.2.5. Iterated genetic search-Uniform combination (IGS-U).- 3.2.6. Iterated genetic search-Ordered combination (IGS-O).- 3.2.7. Stochastic iterated genetic hillclimbing (SIGH).- 3.3. Six functions.- 3.3.1. A linear space-\"One Max\".- 3.3.2. A local maximum-\"Two Max\".- 3.3.3. A large local maximum-\"Trap\".- 3.3.4. Fine-grained local maxima-\"Porcupine\".- 3.3.5. Flat areas-\"Plateaus\".- 3.3.6. A combination space-\"Mix\".- 4. Analytic properties.- 4.1. Problem definition.- 4.2. Energy functions.- 4.3. Basic properties of the learning algorithm.- 4.3.1. Motivating the approach.- 4.3.2. Defining reinforcement signals.- 4.3.3. Defining similarity measures.- 4.3.4. The equilibrium distribution.- 4.4. Convergence.- 4.5. Divergence.- 5. Graph partitioning.- 5.1. Methodology.- 5.1.1. Problems.- 5.1.2. Algorithms.- 5.1.3. Data collection.- 5.1.4. Parameter tuning.- 5.2. Adding a linear component.- 5.3. Experiments on random graphs.- 5.4. Experiments on multilevel graphs.- 6. Related work.- 6.1. The problem space formulation.- 6.2. Search and learning.- 6.2.1. Learning while searching.- 6.2.2. Symbolic learning.- 6.2.3. Hillclimbing.- 6.2.4. Stochastic hillclimbing and simulated annealing.- 6.2.5. Genetic algorithms.- 6.3. Connectionist modelling.- 6.3.1. Competitive learning.- 6.3.2. Back propagation.- 6.3.3. Boltzmann machines.- 6.3.4. Stochastic iterated genetic hillclimbing.- 6.3.5. Harmony theory.- 6.3.6. Reinforcement models.- 7. Limitations and variations.- 7.1. Current limitations.- 7.1.1. The problem.- 7.1.2. The SIGH model.- 7.2. Possible variations.- 7.2.1. Exchanging parameters.- 7.2.2. Beyond symmetric connections.- 7.2.3. Simultaneous optimization.- 7.2.4. Widening the bottleneck.- 7.2.5. Temporal credit assignment.- 7.2.6. Learning a function.- 8. Discussion and conclusions.- 8.1. Stability and change.- 8.2. Architectural goals.- 8.2.1 High potential parallelism.- 8.2.2 Highly incremental.- 8.2.3 \"Generalized Hebbian\" learning.- 8.2.4 Unsupervised learning.- 8.2.5 \"Closed loop\" interactions.- 8.2.6 Emergent properties.- 8.3. Discussion.- 8.3.1 The processor/memory distinction.- 8.3.2 Physical computation systems.- 8.3.3 Between mind and brain.- 8.4. Conclusions.- 8.4.1. Recapitulation.- 8.4.2. Contributions.- References.\n",
            "\n",
            "Paper ID : 1b286f0984301793f711899aa3294974f60ffed9 \tArticle : PRoNTo: Pattern Recognition for Neuroimaging Toolbox\n",
            "Author(s) : ['J. Schrouff', 'M. J. Rosa', 'J. Rondina', 'A. Marquand', 'C. Chu', 'J. Ashburner', 'C. Phillips', 'J. Richiardi', 'J. Miranda']\n",
            "Year : 2013 \n",
            "Abstract : In the past years, mass univariate statistical analyses of neuroimaging data have been complemented by the use of multivariate pattern analyses, especially based on machine learning models. While these allow an increased sensitivity for the detection of spatially distributed effects compared to univariate techniques, they lack an established and accessible software framework. The goal of this work was to build a toolbox comprising all the necessary functionalities for multivariate analyses of neuroimaging data, based on machine learning models. The “Pattern Recognition for Neuroimaging Toolbox” (PRoNTo) is open-source, cross-platform, MATLAB-based and SPM compatible, therefore being suitable for both cognitive and clinical neuroscience research. In addition, it is designed to facilitate novel contributions from developers, aiming to improve the interaction between the neuroimaging and machine learning communities. Here, we introduce PRoNTo by presenting examples of possible research questions that can be addressed with the machine learning framework implemented in PRoNTo, and cannot be easily investigated with mass univariate statistical analysis.\n",
            "\n",
            "Paper ID : 8198e70878c907e1bd05e7a3fa4280d8c338df60 \tArticle : Semi-Supervised Support Vector Machines\n",
            "Author(s) : ['Kristin P. Bennett', 'A. Demiriz']\n",
            "Year : 1998 \n",
            "Abstract : We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach.\n",
            "\n",
            "Paper ID : ec7c68427a26f812532b1c913c68fcf84b7de58e \tArticle : Beyond the point cloud: from transductive to semi-supervised learning\n",
            "Author(s) : ['V. Sindhwani', 'P. Niyogi', 'Mikhail Belkin']\n",
            "Year : 2005 \n",
            "Abstract : Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.\n",
            "\n",
            "Paper ID : f1d73520d51a0f19c686d9f4bf7af1db33763a2a \tArticle : A support vector machine approach for detection of microcalcifications\n",
            "Author(s) : ['I. El-Naqa', 'Yongyi Yang', 'M. Wernick', 'N. Galatsanos', 'R. Nishikawa']\n",
            "Year : 2002 \n",
            "Abstract : We investigate an approach based on support vector machines (SVMs) for detection of microcalcification (MC) clusters in digital mammograms, and propose a successive enhancement learning scheme for improved performance. SVM is a machine-learning method, based on the principle of structural risk minimization, which performs well when applied to data outside the training set. We formulate MC detection as a supervised-learning problem and apply SVM to develop the detection algorithm. We use the SVM to detect at each location in the image whether an MC is present or not. We tested the proposed method using a database of 76 clinical mammograms containing 1120 MCs. We use free-response receiver operating characteristic curves to evaluate detection performance, and compare the proposed algorithm with several existing methods. In our experiments, the proposed SVM framework outperformed all the other methods tested. In particular, a sensitivity as high as 94% was achieved by the SVM method at an error rate of one false-positive cluster per image. The ability of SVM to outperform several well-known methods developed for the widely studied problem of MC detection suggests that SVM is a promising technique for object detection in a medical imaging application.\n",
            "\n",
            "Paper ID : 63be611dc8cf09ed081b4ad2b1756420b782b0be \tArticle : A Perspective on Deep Imaging\n",
            "Author(s) : ['Ge Wang']\n",
            "Year : 2016 \n",
            "Abstract : The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.\n",
            "\n",
            "Paper ID : f620ec7bc0632be5518718cb81e2bfb57c81e950 \tArticle : Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data\n",
            "Author(s) : ['Miloš Radovanović', 'A. Nanopoulos', 'M. Ivanović']\n",
            "Year : 2010 \n",
            "Abstract : Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent \"popular\" nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families.\n",
            "------------------------------------Extracting Page #91------------------------------------\n",
            "\n",
            "Paper ID : 1cef66e9f54dd3a855b8bcc53dbbfa6b7d642b93 \tArticle : 구글 TensorFlow 소개\n",
            "Author(s) : ['김종영']\n",
            "Year : 2015 \n",
            "Abstract : TensorFlow 2.0 in ActionTensorFlow 1.x Deep Learning CookbookMachine Learning with TensorFlow 1.xMachine Learning with TensorFlow, Second EditionTensorFlow 2 Pocket PrimerProgramming with TensorFlowTensorFlow Machine Learning ProjectsHands-On Neural Networks with TensorFlow 2.0TensorFlow for Deep LearningTensor Flow Pocket PrimerNatural Language Processing with TensorFlowTensorFlow: Powerful Predictive Analytics with TensorFlowHands-On Convolutional Neural Networks with TensorFlowTensorFlow 2.0 Computer Vision CookbookIntelligent Mobile Projects with TensorFlowLearning TensorFlow.jsDeep Learning with TensorFlow 2 and KerasLearning TensorFlowTensorFlow 2 Pocket ReferenceMachine Learning Using TensorFlow CookbookTensorFlow 2.0 Quick Start GuideTensorFlow Machine Learning CookbookLearn TensorFlow 2.0Learn TensorFlow in 24 HoursHands-On Computer Vision with TensorFlow 2Mastering Computer Vision with TensorFlow 2.xPro Deep Learning with TensorFlowHands-On Machine Learning with TensorFlow.jsTensorFlow for Deep LearningTinyMLLearning TensorFlow.jsDeep Learning with TensorFlow 2 and Keras Second EditionDeep Learning with TensorFlowMastering TensorFlow 1.xAdopting TensorFlow for Real-World AITensorFlow For DummiesArtificial Intelligence with PythonHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlowLearn TensorFlow EnterpriseThe TensorFlow Workshop\n",
            "\n",
            "Paper ID : 898b028e96884f667dd518b2d2038fb48adc353f \tArticle : A Review of Feature Reduction Techniques in Neuroimaging\n",
            "Author(s) : ['B. Mwangi', 'T. Tian', 'J. Soares']\n",
            "Year : 2013 \n",
            "Abstract : Machine learning techniques are increasingly being used in making relevant predictions and inferences on individual subjects neuroimaging scan data. Previous studies have mostly focused on categorical discrimination of patients and matched healthy controls and more recently, on prediction of individual continuous variables such as clinical scores or age. However, these studies are greatly hampered by the large number of predictor variables (voxels) and low observations (subjects) also known as the curse-of-dimensionality or small-n-large-p problem. As a result, feature reduction techniques such as feature subset selection and dimensionality reduction are used to remove redundant predictor variables and experimental noise, a process which mitigates the curse-of-dimensionality and small-n-large-p effects. Feature reduction is an essential step before training a machine learning model to avoid overfitting and therefore improving model prediction accuracy and generalization ability. In this review, we discuss feature reduction techniques used with machine learning in neuroimaging studies.\n",
            "\n",
            "Paper ID : 08745f22d0abbe66e486f0985c985ecf1eab4e9e \tArticle : Question classification using support vector machines\n",
            "Author(s) : ['Dell Zhang', 'Wee Sun Lee']\n",
            "Year : 2003 \n",
            "Abstract : Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors (NN), Naive Bayes (NB), Decision Tree (DT), Sparse Network of Winnows (SNoW), and Support Vector Machines (SVM) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the SVM outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the SVM to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the TREC QA track.\n",
            "\n",
            "Paper ID : 7b5440285956e5d2ff848583775de35c089cc12f \tArticle : Incremental Learning with Support Vector Machines\n",
            "Author(s) : ['S. Rüping']\n",
            "Year : 2001 \n",
            "Abstract : Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.\n",
            "\n",
            "Paper ID : 796d70a6eb0428ae19f1187ae1c81185d4ae6701 \tArticle : Automating Biomedical Data Science Through Tree-Based Pipeline Optimization\n",
            "Author(s) : ['Randal S. Olson', 'R. Urbanowicz', 'Peter C. Andrews', 'Nicole A. Lavender', 'L. C. Kidd', 'J. Moore']\n",
            "Year : 2016 \n",
            "Abstract : Over the past decade, data science and machine learning has grown from a mysterious art form to a staple tool across a variety of fields in academia, business, and government. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning—pipeline design. We implement a Tree-based Pipeline Optimization Tool (TPOT) and demonstrate its effectiveness on a series of simulated and real-world genetic data sets. In particular, we show that TPOT can build machine learning pipelines that achieve competitive classification accuracy and discover novel pipeline operators—such as synthetic feature constructors—that significantly improve classification accuracy on these data sets. We also highlight the current challenges to pipeline optimization, such as the tendency to produce pipelines that overfit the data, and suggest future research paths to overcome these challenges. As such, this work represents an early step toward fully automating machine learning pipeline design.\n",
            "\n",
            "Paper ID : 831edc3d67457db83da40d260e93bfd7559347ae \tArticle : Dyna, an integrated architecture for learning, planning, and reacting\n",
            "Author(s) : ['R. Sutton']\n",
            "Year : 1991 \n",
            "Abstract : Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.\n",
            "\n",
            "Paper ID : 356125478f5d06b564b420755a4944254045bbbe \tArticle : Support vector learning\n",
            "Author(s) : ['B. Schölkopf']\n",
            "Year : 1997 \n",
            "Abstract : Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology.\n",
            "\n",
            "Paper ID : 9e5e82a07998ce355b65fe5d43b5c3138fd767d9 \tArticle : The power of amnesia: Learning probabilistic automata with variable memory length\n",
            "Author(s) : ['D. Ron', 'Y. Singer', 'Naftali Tishby']\n",
            "Year : 2004 \n",
            "Abstract : We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E.coli DNA.\n",
            "\n",
            "Paper ID : e72fdc7471e78adec50a6b41792254ce966ce70f \tArticle : Learning to detect malicious executables in the wild\n",
            "Author(s) : ['J. Z. Kolter', 'M. Maloof']\n",
            "Year : 2004 \n",
            "Abstract : In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.\n",
            "\n",
            "Paper ID : 949a68c4d0513574a4a504ce6f867b1330f8a947 \tArticle : A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features\n",
            "Author(s) : ['R. S. Cost', 'S. Salzberg']\n",
            "Year : 2004 \n",
            "Abstract : In the past, nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values. In such domains, the examples can be treated as points and distance metrics can use standard definitions. In symbolic domains, a more sophisticated treatment of the feature space is required. We introduce a nearest neighbor algorithm for learning in domains with symbolic features. Our algorithm calculates distance tables that allow it to produce real-valued distances between instances, and attaches weights to the instances to further modify the structure of feature space. We show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text. Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains. In addition, our algorithm has advantages in training speed, simplicity, and perspicuity. We conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here.\n",
            "------------------------------------Extracting Page #92------------------------------------\n",
            "\n",
            "Paper ID : 218a899ba7ff3094c0fc871b9605d8ff4f529336 \tArticle : Learning Logical Definitions from Relations\n",
            "Author(s) : ['J. R. Quinlan']\n",
            "Year : 2005 \n",
            "Abstract : This paper describes FOIL, a system that learns Horn clauses from data expressed as relations. FOIL is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature.\n",
            "\n",
            "Paper ID : 23d2d8b687d31b11573473a7c7792b7ec08d0745 \tArticle : Learning Kernel Classifiers - Theory and Algorithms\n",
            "Author(s) : ['R. Herbrich']\n",
            "Year : 2002 \n",
            "Abstract : From the Publisher: \n",
            "Linear classifiers in kernel spaces have emerged as a major topic within the field of machine learning. The kernel technique takes the linear classifier--a limited, but well-established and comprehensively studied model--and extends its applicability to a wide range of nonlinear pattern-recognition tasks such as natural language processing, machine vision, and biological sequence analysis. This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments. It begins by describing the major algorithmic advances: kernel perceptron learning, kernel Fisher discriminants, support vector machines, relevance vector machines, Gaussian processes, and Bayes point machines. Then follows a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds. Throughout, the book emphasizes the interaction between theory and algorithms: how learning algorithms work and why. The book includes many examples, complete pseudo code of the algorithms presented, and an extensive source code library.\n",
            "\n",
            "Paper ID : 1889b9c3e8bc1118448b95fca38d6eff0bfca64d \tArticle : Learning the Kernel with Hyperkernels\n",
            "Author(s) : ['Cheng Soon Ong', 'Alex Smola', 'R. C. Williamson']\n",
            "Year : 2005 \n",
            "Abstract : This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional.We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach.\n",
            "\n",
            "Paper ID : 04f67e55a636b9053ddc30f55def0ee3fc1c8e2b \tArticle : Supervised tensor learning\n",
            "Author(s) : ['D. Tao', 'Xuelong Li', 'Xindong Wu', 'Weiming Hu', 'S. Maybank']\n",
            "Year : 2005 \n",
            "Abstract : Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis, and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis, and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By comparing with minimax probability machine, the tensor version reduces the overfitting problem.\n",
            "\n",
            "Paper ID : c8b5825b8994ce3c4dc7e603423d7d43a5ead15c \tArticle : Ranking Learning Algorithms: Using IBL and Meta-Learning on Accuracy and Time Results\n",
            "Author(s) : ['P. Brazdil', 'Carlos Soares', 'J. Costa']\n",
            "Year : 2004 \n",
            "Abstract : We present a meta-learning method to support selection of candidate learning algorithms. It uses a k-Nearest Neighbor algorithm to identify the datasets that are most similar to the one at hand. The distance between datasets is assessed using a relatively small set of data characteristics, which was selected to represent properties that affect algorithm performance. The performance of the candidate algorithms on those datasets is used to generate a recommendation to the user in the form of a ranking. The performance is assessed using a multicriteria evaluation measure that takes not only accuracy, but also time into account. As it is not common in Machine Learning to work with rankings, we had to identify and adapt existing statistical techniques to devise an appropriate evaluation methodology. Using that methodology, we show that the meta-learning method presented leads to significantly better rankings than the baseline ranking method. The evaluation methodology is general and can be adapted to other ranking problems. Although here we have concentrated on ranking classification algorithms, the meta-learning framework presented can provide assistance in the selection of combinations of methods or more complex problem solving strategies.\n",
            "\n",
            "Paper ID : a584e4b607e972783cea22daaaf1114ea94a8035 \tArticle : Support Vector Learning for Semantic Argument Classification\n",
            "Author(s) : ['Sameer Pradhan', 'K. Hacioglu', 'Valerie Krugler', 'Wayne H. Ward', 'James H. Martin', 'Dan Jurafsky']\n",
            "Year : 2005 \n",
            "Abstract : The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing—the process of assigning a Who did What to Whom, When, Where, Why, How etc. structure to plain text. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them. It could play a key role in NLP tasks like Information Extraction, Question Answering and Summarization. We propose a machine learning algorithm for semantic role parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers. We show performance improvements through a number of new features designed to improve generalization to unseen data, such as automatic clustering of verbs. We also report on various analytic studies examining which features are most important, comparing our classifier to other machine learning algorithms in the literature, and testing its generalization to new test set from different genre. On the task of assigning semantic labels to the PropBank (Kingsbury, Palmer, & Marcus, 2002) corpus, our final system has a precision of 84% and a recall of 75%, which are the best results currently reported for this task. Finally, we explore a completely different architecture which does not requires a deep syntactic parse. We reformulate the task as a combined chunking and classification problem, thus allowing our algorithm to be applied to new languages or genres of text for which statistical syntactic parsers may not be available.\n",
            "\n",
            "Paper ID : 1bd6e929ed8384ea2212d50ab3c103ec018cc9fd \tArticle : A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling\n",
            "Author(s) : ['Jonathan Baxter']\n",
            "Year : 2004 \n",
            "Abstract : A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks.\n",
            "\n",
            "Paper ID : b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b \tArticle : Steps toward Artificial Intelligence\n",
            "Author(s) : ['M. Minsky']\n",
            "Year : 1961 \n",
            "Abstract : The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.\n",
            "\n",
            "Paper ID : 84f6eeeac62804de0502a54c444bacfad0f64730 \tArticle : Ontology Learning and Its Application to Automated Terminology Translation\n",
            "Author(s) : ['R. Navigli', 'P. Velardi', 'Aldo Gangemi']\n",
            "Year : 2003 \n",
            "Abstract : Our OntoLearn system is an infrastructure for automated ontology learning from domain text. It is the only system, as far as we know, that uses natural language processing and machine learning techniques, and is part of a more general ontology engineering architecture. We describe the system and an experiment in which we used a machine-learned tourism ontology to automatically translate multiword terms from English to Italian. The method can apply to other domains without manual adaptation.\n",
            "\n",
            "Paper ID : a9565f70db3a6b5e4ff1df272238b8d7a84a1337 \tArticle : Experience with a learning personal assistant\n",
            "Author(s) : ['Tom Michael Mitchell', 'R. Caruana', 'D. Freitag', 'J. McDermott', 'David Zabowski']\n",
            "Year : 1994 \n",
            "Abstract : Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user. For example, an assistant that helps schedule a particular user’s calendar will have to know that user’s scheduling preferences. This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants. We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience. Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users. Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants.\n",
            "------------------------------------Extracting Page #93------------------------------------\n",
            "\n",
            "Paper ID : 05c08de1bf91cd52ea1d22e7238e33958b574a23 \tArticle : The responsibility gap: Ascribing responsibility for the actions of learning automata\n",
            "Author(s) : ['A. Matthias']\n",
            "Year : 2004 \n",
            "Abstract : Traditionally, the manufacturer/operator of a machine is held (morally and legally) responsible for the consequences of its operation. Autonomous, learning machines, based on neural networks, genetic algorithms and agent architectures, create a new situation, where the manufacturer/operator of the machine is in principle not capable of predicting the future machine behaviour any more, and thus cannot be held morally responsible or liable for it. The society must decide between not using this kind of machine any more (which is not a realistic option), or facing a responsibility gap, which cannot be bridged by traditional concepts of responsibility ascription.\n",
            "\n",
            "Paper ID : 6b0966c51d66e3097fc9f9d704bc43fdd963e90e \tArticle : Learning in Humans and Machines: Towards an Interdisciplinary Learning Science\n",
            "Author(s) : ['H. Spada', 'Reimann', 'P. Reimann']\n",
            "Year : 1995 \n",
            "Abstract : Chapter headings: Towards an Interdisciplinary Learning Science (P. Reimann, H. Spada). A Cognitive Psychological Approach to Learning (S. Vosniadou). Learning to Do and Learning to Understand: A Lesson and a Challenge for Cognitive Modeling (S. Ohlsson). Machine Learning: Case Studies of an Interdisciplinary Approach (W. Emde). Mental and Physical Artifacts in Cognitive Practices (R. Saljo). Learning Theory and Instructional Science (E. De Corte). Knowledge Representation Changes in Humans and Machines (L. Saitta and Task Force 1). Multi-Objective Learning with Multiple Representations (M. Van Someren, P. Reimann). Order Effects in Incremental Learning (P. Langley). Situated Learning and Transfer (H. Gruber et al.). The Evolution of Research on Collaborative Learning (P. Dillenbourg et al.). A Developmental Case Study on Sequential Learning: The Day-Night Cycle (K. Morik, S. Vosniadou). Subject index. Author index.\n",
            "\n",
            "Paper ID : d18f64aa830075ed3e10206907f32c8fb2aa189d \tArticle : INTRODUCTION TO STATISTICAL LEARNING THEORY AND SUPPORT VECTOR MACHINES\n",
            "Author(s) : ['Zhang Xuegong']\n",
            "Year : 2000 \n",
            "Abstract : Data based machine learning covers a wide range of topics from pattern recognition to function regression and density estimation. Most of the existing methods are based on traditional statistics, which provides conclusion only for the situation where sample size is tending to infinity. So they may not work in practical cases of limited samples. Statistical Learning Theory or SLT is a small sample statistics by Vapnik et al., which concerns mainly the statistic principles when samples are limited, especially the properties of learning procedure in such cases. SLT provides us a new framework for the general learning problem, and a novel powerful learning method called Support Vector Machine or SVM, which can solve small sample learning problems better. It is believed that the study of SLT and SVM is becoming a new hot area in the field of machine learning. This review introduces the basic ideas of SLT and SVM, their major characteristics and some current research trends.\n",
            "\n",
            "Paper ID : 01a61d9b9183ce11c89e36d9e1f24614c98f3ee8 \tArticle : Query Learning with Large Margin Classifiers\n",
            "Author(s) : ['C. Campbell', 'N. Cristianini', 'Alex Smola']\n",
            "Year : 2000 \n",
            "Abstract : The active selection of instances can significantly improve the generalisation performance of a learning machine. Large margin classifiers such as support vector machines classify data using the most informative instances (the support vectors). This makes them natural candidates for instance selection strategies. In this paper we propose an algorithm for the training of support vector machines using instance selection. We give a theoretical justification for the strategy and experimental results on real and artificial data demonstrating its effectiveness. The technique is most efficient when the data set can be learnt using few support vectors.\n",
            "\n",
            "Paper ID : 5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398 \tArticle : Hidden Markov Support Vector Machines\n",
            "Author(s) : ['Y. Altun', 'Ioannis Tsochantaridis', 'Thomas Hofmann']\n",
            "Year : 2003 \n",
            "Abstract : This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach.\n",
            "\n",
            "Paper ID : 62f40a5d8f099caaecdbb91fefd7bdfde3f32cf2 \tArticle : Toward Practical Smile Detection\n",
            "Author(s) : ['J. Whitehill', 'G. Littlewort', 'I. Fasel', 'M. Bartlett', 'J. Movellan']\n",
            "Year : 2009 \n",
            "Abstract : Machine learning approaches have produced some of the highest reported performances for facial expression recognition. However, to date, nearly all automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled lighting conditions on a relatively small number of subjects. This paper explores whether current machine learning methods can be used to develop an expression recognition system that operates reliably in more realistic conditions. We explore the necessary characteristics of the training data set, image registration, feature representation, and machine learning algorithms. A new database, GENKI, is presented which contains pictures, photographed by the subjects themselves, from thousands of different people in many different real-world imaging conditions. Results suggest that human-level expression recognition accuracy in real-life illumination conditions is achievable with machine learning technology. However, the data sets currently used in the automatic expression recognition literature to evaluate progress may be overly constrained and could potentially lead research into locally optimal algorithmic solutions.\n",
            "\n",
            "Paper ID : e5ecac3aa41e359aa3a5d25943d0af72ff91df25 \tArticle : Support vector machines : theory and applications\n",
            "Author(s) : ['Lipo Wang']\n",
            "Year : 2005 \n",
            "Abstract : From the contents: Support Vector Machines - An Introduction.- Multiple Model Estimation for Nonlinear Classification.- Componentwise Least Squares Support Vector Machines.- Active Support Vector Learning with Statistical Queries.- Local Learning vs. Global Learning: An Introduction to Maxi-Min Margin Machine.- Active-Set Methods for Support Vector Machines.- Theoretical and Practical Model Selection Methods for Support Vector Classifiers.- Adaptive Discriminant and Quasiconformal Kernel Nearest Neighbor Classification.- Improving the Performance of the Support Vector Machine: Two Geometrical Scaling Methods.- An Accelerated Robust Support Vector Machine Algorithm.- Fuzzy Support Vector Machines with Automatic Membership Setting.- Iterative Single Data Algorithm for Training Kernel Machines from Huge Data Sets: Theory and Performance.- Kernel Discriminant Learning with Application to Face Recognition.- Fast Color Texture-based Object Detection in Images: Application to License Plate Localization.\n",
            "\n",
            "Paper ID : dde691805cfa7d6f1bb88c7411c1c3377b6cdc67 \tArticle : Lifelong Learning Algorithms\n",
            "Author(s) : ['S. Thrun']\n",
            "Year : 1998 \n",
            "Abstract : Machine learning has not yet succeeded in the design of robust learning algorithms that generalize well from very small datasets. In contrast, humans often generalize correctly from only a single training example, even if the number of potentially relevant features is large. To do so, they successfully exploit knowledge acquired in previous learning tasks, to bias subsequent learning.\n",
            "\n",
            "Paper ID : b7d89441fcf28ca1a365af4d739709a7075a5db2 \tArticle : Knowledge Discovery with Support Vector Machines\n",
            "Author(s) : ['L. Hamel']\n",
            "Year : 2009 \n",
            "Abstract : An easy-to-follow introduction to support vector machines This book provides an in-depth, easy-to-follow introduction to support vector machines drawing only from minimal, carefully motivated technical and mathematical background material. It begins with a cohesive discussion of machine learning and goes on to cover: Knowledge discovery environments Describing data mathematically Linear decision surfaces and functions Perceptron learning Maximum margin classifiers Support vector machines Elements of statistical learning theory Multi-class classification Regression with support vector machines Novelty detection Complemented with hands-on exercises, algorithm descriptions, and data sets, Knowledge Discovery with Support Vector Machines is an invaluable textbook for advanced undergraduate and graduate courses. It is also an excellent tutorial on support vector machines for professionals who are pursuing research in machine learning and related areas.\n",
            "\n",
            "Paper ID : 9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb \tArticle : WEKA: The Waikato Environment for Knowledge Analysis\n",
            "Author(s) : ['Stephen R. Garner']\n",
            "Year : 1996 \n",
            "Abstract : WEKA is a workbench designed to aid in the application of machine learning technology to real world data sets, in particular, data sets from New Zealand’s agricultural sector. In order to do this a range of machine learning techniques are presented to the user in such a way as to hide the idiosyncrasies of input and output formats, as well as allow an exploratory approach in applying the technology. The system presented is a component based one that also has application in machine learning research and education.\n",
            "------------------------------------Extracting Page #94------------------------------------\n",
            "\n",
            "Paper ID : e6140a793a4554806eb39d15c018d8f782d2ac1e \tArticle : Learning to Parse Natural Language with Maximum Entropy Models\n",
            "Author(s) : ['A. Ratnaparkhi']\n",
            "Year : 2004 \n",
            "Abstract : This paper presents a machine learning system for parsing natural language that learns from manually parsed example sentences, and parses unseen data at state-of-the-art accuracies. Its machine learning technology, based on the maximum entropy framework, is highly reusable and not specific to the parsing problem, while the linguistic hints that it uses to learn can be specified concisely. It therefore requires a minimal amount of human effort and linguistic knowledge for its construction. In practice, the running time of the parser on a test sentence is linear with respect to the sentence length. We also demonstrate that the parser can train from other domains without modification to the modeling framework or the linguistic hints it uses to learn. Furthermore, this paper shows that research into rescoring the top 20 parses returned by the parser might yield accuracies dramatically higher than the state-of-the-art.\n",
            "\n",
            "Paper ID : 6b945d00f77367422965531dd3d01694822c52c1 \tArticle : Ensembles of Learning Machines\n",
            "Author(s) : ['G. Valentini', 'F. Masulli']\n",
            "Year : 2002 \n",
            "Abstract : Ensembles of learning machines constitute one of the main current directions in machine learning research, and have been applied to a wide range of real problems. Despite of the absence of an unified theory on ensembles, there are many theoretical reasons for combining multiple learners, and an empirical evidence of the effectiveness of this approach. In this paper we present a brief overview of ensemble methods, explaining the main reasons why they are able to outperform any single classifier within the ensemble, and proposing a taxonomy based on the main ways base classifiers can be generated or combined together.\n",
            "\n",
            "Paper ID : 2e75ad22a6865c3402925f12317dd0897c67ebe6 \tArticle : Predicting Structured Data\n",
            "Author(s) : ['G. Bakir', 'T. Hofmann', 'B. Schölkopf', 'Taskar B Smola Aj', 'S. Vishwanathan']\n",
            "Year : 2007 \n",
            "Abstract : Machine learning develops intelligent computer systems that are able to generalize from previously seen examples. A new domain of machine learning, in which the prediction must satisfy the additional constraints found in structured data, poses one of machine learning's greatest challenges: learning functional dependencies between arbitrary input and output domains. This volume presents and analyzes the state of the art in machine learning algorithms and theory in this novel field. The contributors discuss applications as diverse as machine translation, document markup, computational biology, and information extraction, among others, providing a timely overview of an exciting field. Contributors Yasemin Altun, Gokhan Bakir [no dot over i], Olivier Bousquet, Sumit Chopra, Corinna Cortes, Hal Daume III, Ofer Dekel, Zoubin Ghahramani, Raia Hadsell, Thomas Hofmann, Fu Jie Huang, Yann LeCun, Tobias Mann, Daniel Marcu, David McAllester, Mehryar Mohri, William Stafford Noble, Fernando Perez-Cruz, Massimiliano Pontil, Marc'Aurelio Ranzato, Juho Rousu, Craig Saunders, Bernhard Scholkopf, Matthias W. Seeger, Shai Shalev-Shwartz, John Shawe-Taylor, Yoram Singer, Alexander J. Smola, Sandor Szedmak, Ben Taskar, Ioannis Tsochantaridis, S.V.N Vishwanathan, Jason Weston Gokhan Bakir [no dot over i] is Research Scientist at the Max Planck Institute for Biological Cybernetics in Tubingen, Germany. Thomas Hofmann is a Director of Engineering at Google's Engineering Center in Zurich and Adjunct Associate Professor of Computer Science at Brown University. Bernhard Scholkopf is Director of the Max Planck Institute for Biological Cybernetics and Professor at the Technical University Berlin. Alexander J. Smola is Senior Principal Researcher and Machine Learning Program Leader at National ICT Australia/Australian National University, Canberra. Ben Taskar is Assistant Professor in the Computer and Information Science Department at the University of Pennsylvania. S. V. N. Vishwanathan is Senior Researcher in the Statistical Machine Learning Program, National ICT Australia with an adjunct appointment at the Research School for Information Sciences and Engineering, Australian National University.\n",
            "\n",
            "Paper ID : e22d10b9e55bb6ed3adfd2f5136a5972fc683565 \tArticle : Connectionist expert systems\n",
            "Author(s) : ['S. I. Gallant']\n",
            "Year : 1988 \n",
            "Abstract : Connectionist networks can be used as expert system knowledge bases. Furthermore, such networks can be constructed from training examples by machine learning techniques. This gives a way to automate the generation of expert systems for classification problems.\n",
            "\n",
            "Paper ID : 008abebf4a9404db9050c9d2fbca769f4faf3ca6 \tArticle : Learning by Transduction\n",
            "Author(s) : ['A. Gammerman', 'V. Vovk', 'V. Vapnik']\n",
            "Year : 1998 \n",
            "Abstract : We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.\n",
            "\n",
            "Paper ID : 3e06c979b01b1c235017495d7d3a2769bb6a81bc \tArticle : Learning to Learn: Introduction and Overview\n",
            "Author(s) : ['S. Thrun', 'L. Pratt']\n",
            "Year : 1998 \n",
            "Abstract : Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).\n",
            "\n",
            "Paper ID : 886b9cd28b77e1a73da7112b95196e661ae8d5a1 \tArticle : Explanation-based learning: An alternative view\n",
            "Author(s) : ['G. DeJong', 'R. Mooney']\n",
            "Year : 2004 \n",
            "Abstract : In the last issue of this journal Mitchell, Keller, and Kedar-Cabelli presented a unifying framework for the explanation-based approach to machine learning. While it works well for a number of systems, the framework does not adequately capture certain aspects of the systems under development by the explanation-based learning group at Illinois. The primary inadequacies arise in the treatment of concept operationality, organization of knowledge into schemata, and learning from observation. This paper outlines six specific problems with the previously proposed framework and presents an alternative generalization method to perform explanation-based learning of new concepts.\n",
            "\n",
            "Paper ID : 572d3c5fe5a2908d308701fc88dc4484d44fba14 \tArticle : Applications of Support Vector Machines in Chemistry\n",
            "Author(s) : ['O. Ivanciuc']\n",
            "Year : 2007 \n",
            "Abstract : Kernel-based techniques (such as support vector machines, Bayes point machines, kernel principal component analysis, and Gaussian processes) represent a major development in machine learning algorithms. Support vector machines (SVM) are a group of supervised learning methods that can be applied to classification or regression. In a short period of time, SVM found numerous applications in chemistry, such as in drug design (discriminating between ligands and nonligands, inhibitors and noninhibitors, etc.), quantitative structure-activity relationships (QSAR, where SVM regression is used to predict various physical, chemical, or biological properties), chemometrics (optimization of chromatographic separation or compound concentration prediction from spectral data as examples), sensors (for qualitative and quantitative prediction from sensor data), chemical engineering (fault detection and modeling of industrial processes), and text mining (automatic recognition of scientific information). Support vector machines represent an extension to nonlinear models of the generalized portrait algorithm developed by Vapnik and Lerner. The SVM algorithm is based on the statistical learning theory and the Vapnik–Chervonenkis\n",
            "\n",
            "Paper ID : 3b0c8c67d17634f6f1f7a74d43b46e1782c1271e \tArticle : The class imbalance problem: A systematic study\n",
            "Author(s) : ['JapkowiczNathalie', 'StephenShaju']\n",
            "Year : 2002 \n",
            "Abstract : In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. T...\n",
            "\n",
            "Paper ID : a1f94bd5954fbbdbf8aebfabe2760fc73c05eab8 \tArticle : Learning to Predict Rare Events in Event Sequences\n",
            "Author(s) : ['Gary M. Weiss', 'H. Hirsh']\n",
            "Year : 1998 \n",
            "Abstract : Learning to predict rare events from sequences of events with categorical features is an important, real-world, problem that existing statistical and machine learning methods are not well suited to solve. This paper describes timeweaver, a genetic algorithm based machine learning system that predicts rare events by identifying predictive temporal and sequential patterns. Timeweaver is applied to the task of predicting telecommunication equipment failures from 110,000 alarm messages and is shown to outperform existing learning methods.\n",
            "------------------------------------Extracting Page #95------------------------------------\n",
            "\n",
            "Paper ID : e316ada2fd1717d49d4c3c84681325ae8665785d \tArticle : Learning with genetic algorithms: An overview\n",
            "Author(s) : ['K. A. Jong']\n",
            "Year : 2004 \n",
            "Abstract : Genetic algorithms represent a class of adaptive search techniques that have been intensively studied in recent years. Much of the interest in genetic algorithms is due to the fact that they provide a set of efficient domain-independent search heuristics which are a significant improvement over traditional “weak methods” without the need for incorporating highly domain-specific knowledge. There is now considerable evidence that genetic algorithms are useful for global function optimization and NP-hard problems. Recently, there has been a good deal of interest in using genetic algorithms for machine learning problems. This paper provides a brief overview of how one might use genetic algorithms as a key element in learning systems.\n",
            "\n",
            "Paper ID : c26bdb2c2b62212c26b4f2e91ea5f9011656a56c \tArticle : Handling concept drifts in incremental learning with support vector machines\n",
            "Author(s) : ['N. Syed', 'Huan Liu', 'K. Sung']\n",
            "Year : 1999 \n",
            "Abstract : With the increase in the size of real-world databases, there is an ever-increasing need to scale up inductive learning algorithms. Incremental learning techniques are one possible solution to the scalability problem. In this paper, we propose three ctiteria to evaluate the robustness and reliability of incremental learning methods, and use them to study the robustness of an incremental training method for Support Vector Machines. We provide empirical results using benchmark machine learning datasets to show that support vectors form a svccdnct and suficient set for block-by-block incremental learning.\n",
            "\n",
            "Paper ID : d08b933458716a85888956165d07243505d4e4b2 \tArticle : Model Induction with Support Vector Machines: Introduction and Applications\n",
            "Author(s) : ['Y. Dibike', 'S. Velickov', 'D. Solomatine', 'M. Abbott']\n",
            "Year : 2001 \n",
            "Abstract : The rapid advance in information processing systems in recent decades had directed engineering research towards the development of intelligent systems that can evolve models of natural phenomena automatically—“by themselves,” so to speak. In this respect, a wide range of machine learning techniques like decision trees, artificial neural networks (ANNs), Bayesian methods, fuzzy-rule based systems, and evolutionary algorithms have been successfully applied to model different civil engineering systems. In this study, the possibility of using yet another machine learning paradigm that is firmly based on the theory of statistical learning, namely that of the support vector machine (SVM), is investigated. An interesting property of this approach is that it is an approximate implementation of a structural risk minimization (SRM) induction principle that aims at minimizing a bound on the generalization error of a model, rather than minimizing only the mean square error over the data set. In this paper, the basic ...\n",
            "\n",
            "Paper ID : 62ec643e5415b6ab89f7186d3631f5549fd8a0cc \tArticle : Support Vector Machines and Kernel Methods: The New Generation of Learning Machines\n",
            "Author(s) : ['N. Cristianini', 'B. Schölkopf']\n",
            "Year : 2002 \n",
            "Abstract : Kernel methods, a new generation of learning algorithms, utilize techniques from optimization, statistics, and functional analysis to achieve maximal generality, flexibility, and performance. These algorithms are different from earlier techniques used in machine learning in many respects: For example, they are explicitly based on a theoretical model of learning rather than on loose analogies with natural learning systems or other heuristics. They come with theoretical guarantees about their performance and have a modular design that makes it possible to separately implement and analyze their components. They are not affected by the problem of local minima because their training amounts to convex optimization. In the last decade, a sizable community of theoreticians and practitioners has formed around these methods, and a number of practical applications have been realized. Although the research is not concluded, already now kernel methods are considered the state of the art in several machine learning tasks. Their ease of use, theoretical appeal, and remarkable performance have made them the system of choice for many learning problems. Successful applications range from text categorization to handwriting recognition to classification of gene-expression data.\n",
            "\n",
            "Paper ID : b6b743de242a2987b3ed0349d90971fdf7ed2faf \tArticle : Algebraic Analysis for Nonidentifiable Learning Machines\n",
            "Author(s) : ['Sumio Watanabe']\n",
            "Year : 2001 \n",
            "Abstract : This article clarifies the relation between the learning curve and the algebraic geometrical structure of a nonidentifiable learning machine such as a multilayer neural network whose true parameter set is an analytic set with singular points. By using a concept in algebraic analysis, we rigorously prove that the Bayesian stochastic complexity or the free energy is asymptotically equal to 1 logn (m1 1) loglogn + constant, where n is the number of training samples and 1 and m1 are the rational number and the natural number, which are determined as the birational invariant values of the singularities in the parameter space. Also we show an algorithm to calculate 1 and m1 based on the resolution of singularities in algebraic geometry. In regular statistical models, 21 is equal to the number of parameters and m1 = 1, whereas in nonregular models, such as multilayer networks, 21 is not larger than the number of parameters and m1 1. Since the increase of the stochastic complexity is equal to the learning curve or the generalization error, the nonidentifiable learning machines are better models than the regular ones if Bayesian ensemble learning is applied.\n",
            "\n",
            "Paper ID : ee732251c34c222370428c9db8386269ce089b95 \tArticle : A Comparative Evaluation of Sequential Feature Selection Algorithms\n",
            "Author(s) : ['D. Aha', 'R. Bankert']\n",
            "Year : 1995 \n",
            "Abstract : Several recent machine learning publications demonstrate the utility of using feature selection algorithms in supervised learning tasks. Among these, sequential feature selection algorithms are receiving attention. The most frequently studied variants of these algorithms are forward and backward sequential selection. Many studies on supervised learning with sequential feature selection report applications of these algorithms, but do not consider variants of them that might be more appropriate for some performance tasks. This paper reports positive empirical results on such variants, and argues for their serious consideration in similar learning tasks.\n",
            "\n",
            "Paper ID : 8a8576d0184b48fabb5dbe76ce04213ee50a8986 \tArticle : Theoretical Models of Learning to Learn\n",
            "Author(s) : ['Jonathan Baxter']\n",
            "Year : 1998 \n",
            "Abstract : A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an environment of related tasks, then it can learn its own bias by learning sufficiently many tasks from the environment [Baxter, 1995b; Baxter, 1997]. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.\n",
            "\n",
            "Paper ID : 4d0183d8644aabcdf01f2f741249df718de0609b \tArticle : Behavioral and Brain Sciences\n",
            "Author(s) : ['G. C.B.']\n",
            "Year : 1999 \n",
            "Abstract : HCS 5314 (ACN 5314) Computational Modeling Methods in Behavioral and Brain Sciences (3 semester credit hours) Historical introduction to machine learning algorithms from a cognitive-neuroscience perspective. Includes an introduction to important and widely used computational modeling methodologies in psychology, neuroscience, and machine learning. No mathematical prerequisites and no computer programming prerequisites, but students will use the computer in simulation experiments. Prerequisites: BBSC majors only and department consent required. (3-0) T\n",
            "\n",
            "Paper ID : d215c7a1f7176c8a536e1ca79e56fcd405cfa770 \tArticle : Probabilistic finite-state machines - part I\n",
            "Author(s) : ['E. Vidal', 'F. Thollard', 'C. D. L. Higuera', 'F. Casacuberta', 'Rafael C. Carrasco']\n",
            "Year : 2005 \n",
            "Abstract : Probabilistic finite-state machines are used today in a variety of areas in pattern recognition, or in fields to which pattern recognition is linked: computational linguistics, machine learning, time series analysis, circuit testing, computational biology, speech recognition, and machine translation are some of them. In Part I of this paper, we survey these generative objects and study their definitions and properties. In Part II, we study the relation of probabilistic finite-state automata with other well-known devices that generate strings as hidden Markov models and n-grams and provide theorems, algorithms, and properties that represent a current state of the art of these objects.\n",
            "\n",
            "Paper ID : 7dddc25995ea77c1ec4de48e1291e6f9b21c877b \tArticle : An Overview on Theory and Algorithm of Support Vector Machines\n",
            "Author(s) : ['Qi Bing-juan']\n",
            "Year : 2011 \n",
            "Abstract : Statistical learning theory is the statistical theory of smallsample,and it focuses on the statistical law and the nature of learning of small samples.Support vector machine is a new machine learning method based on statistical learning theory,and it has become the research field of machine learning because of its excellent performance.This paper describes the theoretical basis of support vector machines(SVM) systematically,sums up the mainstream machine training algorithms of traditional SVM and some new learning models and algorithms detailedly,and finally points out the research and development prospects of support vector machine.\n",
            "------------------------------------Extracting Page #96------------------------------------\n",
            "\n",
            "Paper ID : 2d05d7b1bf024d2004d9ce13d42b462e961f4dc3 \tArticle : Intelligent Manufacturing Systems\n",
            "Author(s) : ['A. Kusiak', 'D. Dornfeld']\n",
            "Year : 1990 \n",
            "Abstract : Introduction - flexible machining and assembly systems components of knowledge-based systems machine learning design of mechanical parts and mechanisms process planning. KBSES - a knowledge-based system for equipment selection group technology - models and algorithms. KBGT - a knowledge-based system for group technology models and algorithms for machine layout. KBML - knowledge-based system for machine layout aggregate scheduling of machining and assembly systems scheduling models and algorithms. KBSS - a knowledge-based system for scheduling in automated manufacturing.\n",
            "\n",
            "Paper ID : 0d28ca03d6b993c0c9203957d052475f5425aa2c \tArticle : WHY SHOULD MACHINES LEARN\n",
            "Author(s) : ['H. Simon']\n",
            "Year : 1983 \n",
            "Abstract : When I agreed to write this chapter, I thought I could simply expand a paper that I wrote for the Carnegie Symposium on Cognition, since the topic of that symposium was also learning. The difficulty with plagiarizing that paper is that it was really about psychology, whereas this book is concerned with machine learning. Now although we all believe machines can simulate human thought—unless we’re vitalists, and there aren’t any of those around any more—still, I didn’t think that was what was intended by the title of the book. I didn’t think it was appropriate to write about psychology.\n",
            "\n",
            "Paper ID : b96a32ceb596a6894fc1a318ce1f8072991c7178 \tArticle : Classifier Systems and the Animat Problem\n",
            "Author(s) : ['Stewart W. Wilson']\n",
            "Year : 2005 \n",
            "Abstract : This paper characterizes and investigates, from the perspective of machine learning and, particularly, classifier systems, the learning problem faced by animals and autonomous robots (here collectively termed animats). We suggest that, to survive in their environments, animats must in effect learn multiple disjunctive concepts incrementally under payoff (needs-satisfying) feedback. A review of machine learning techniques indicates that most relax at least one of these constraints. In theory, classifier systems satisfy the constraints, but tests have been limited. We show how the standard classifier system model applies to the animat learning problem. Then, in the experimental part of the paper, we specialize the model and test it in a problem environment satisfying the constraints and consisting of a difficult, disjunctive Boolean function drawn from the machine learning literature. Results include: learning the function in significantly fewer trials than a neural-network method; learning under payoff regimes that include both noisy payoff and partial reward for suboptimal performance; demonstration, in a classifier system, of a theoretically predicted property of genetic algorithms: the superiority of crossovers to point mutations; and automatic control of variation (search) rate based on system entropy. We conclude that the results support the classifier system approach to the animat problem, but suggest work aimed at the emergence of behavioral hierarchies of classifiers to offset slower learning rates in larger problems.\n",
            "\n",
            "Paper ID : f70e3b10a6a72cda146ee48161e2d02054a4ad59 \tArticle : Leave One Out Error, Stability, and Generalization of Voting Combinations of Classifiers\n",
            "Author(s) : ['T. Evgeniou', 'M. Pontil', 'A. Elisseeff']\n",
            "Year : 2004 \n",
            "Abstract : We study the leave-one-out and generalization errors of voting combinations of learning machines. A special case considered is a variant of bagging. We analyze in detail combinations of kernel machines, such as support vector machines, and present theoretical estimates of their leave-one-out error. We also derive novel bounds on the stability of combinations of any classifiers. These bounds can be used to formally show that, for example, bagging increases the stability of unstable learning machines. We report experiments supporting the theoretical findings.\n",
            "\n",
            "Paper ID : 16b3c8f6f1dffd31271c59c11e17241e51377d68 \tArticle : An Introduction to Statistical Learning\n",
            "Author(s) : ['Gareth M. James', 'D. Witten', 'T. Hastie', 'R. Tibshirani']\n",
            "Year : 2021 \n",
            "Abstract : Statistics An Intduction to Stistical Lerning with Applications in R An Introduction to Statistical Learning provides an accessible overview of the fi eld of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fi elds ranging from biology to fi nance to marketing to astrophysics in the past twenty years. Th is book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classifi cation, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fi elds, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical soft ware platform. Two of the authors co-wrote Th e Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. Th is book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. Th e text assumes only a previous course in linear regression and no knowledge of matrix algebra.\n",
            "\n",
            "Paper ID : f50e54684086a91bb481f76f7180c1ef3c4cb312 \tArticle : Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data\n",
            "Author(s) : ['Liang Guo', 'Y. Lei', 'Saibo Xing', 'Tao Yan', 'Naipeng Li']\n",
            "Year : 2019 \n",
            "Abstract : The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.\n",
            "\n",
            "Paper ID : b5e5a7eee59dd740897c0c3d1ada96c2e2a7e0a7 \tArticle : An Introduction to Statistical Learning: with Applications in R\n",
            "Author(s) : ['D. Witten']\n",
            "Year : 2013 \n",
            "Abstract : An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.\n",
            "\n",
            "Paper ID : 818e627defcd36477b6d39ec5fc4d02a479e7c37 \tArticle : Trends in extreme learning machines: A review\n",
            "Author(s) : ['Gao Huang', 'G. Huang', 'Shiji Song', 'Keyou You']\n",
            "Year : 2015 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : 51886def908b16d11685ea23eb2124dfe961754f \tArticle : Semi-Supervised and Unsupervised Extreme Learning Machines\n",
            "Author(s) : ['Gao Huang', 'Shiji Song', 'J. Gupta', 'Cheng Wu']\n",
            "Year : 2014 \n",
            "Abstract : Extreme learning machines (ELMs) have proven to be efficient and effective learning mechanisms for pattern classification and regression. However, ELMs are primarily applied to supervised learning problems. Only a few existing research papers have used ELMs to explore unlabeled data. In this paper, we extend ELMs for both semi-supervised and unsupervised tasks based on the manifold regularization, thus greatly expanding the applicability of ELMs. The key advantages of the proposed algorithms are as follows: 1) both the semi-supervised ELM (SS-ELM) and the unsupervised ELM (US-ELM) exhibit learning capability and computational efficiency of ELMs; 2) both algorithms naturally handle multiclass classification or multicluster clustering; and 3) both algorithms are inductive and can handle unseen data at test time directly. Moreover, it is shown in this paper that all the supervised, semi-supervised, and unsupervised ELMs can actually be put into a unified framework. This provides new perspectives for understanding the mechanism of random feature mapping, which is the key concept in ELM theory. Empirical study on a wide range of data sets demonstrates that the proposed algorithms are competitive with the state-of-the-art semi-supervised or unsupervised learning algorithms in terms of accuracy and efficiency.\n",
            "\n",
            "Paper ID : d1208ac421cf8ff67b27d93cd19ae42b8d596f95 \tArticle : Deep learning with COTS HPC systems\n",
            "Author(s) : ['Adam Coates', 'Brody Huval', 'Tao Wang', 'David J. Wu', 'Bryan Catanzaro', 'A. Ng']\n",
            "Year : 2013 \n",
            "Abstract : Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.\n",
            "------------------------------------Extracting Page #97------------------------------------\n",
            "\n",
            "Paper ID : 5d90f06bb70a0a3dced62413346235c02b1aa086 \tArticle : Learning Multiple Layers of Features from Tiny Images\n",
            "Author(s) : ['A. Krizhevsky']\n",
            "Year : 2009 \n",
            "Abstract : Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.\n",
            "\n",
            "Paper ID : ad51e96bc42096840f674d8790a3397755967712 \tArticle : The Machine That Changed the World\n",
            "Author(s) : ['J. Womack', 'Daniel T. Jones', 'D. Roos']\n",
            "Year : 1991 \n",
            "Abstract : This book is based on the Massachusetts Institute of Technology (MIT) 5-million-dollar 5-year study on the future of the automobile. Designated the International Motor Vehicle Program (IMVP), the MIT study explored the differences between mass production and lean production in the automobile industry. Lean production, pioneered by Eiji Toyoda and Taiichi Ohno at the Toyota Motor Company in Japan, combines the advantages of craft and mass production, while avoiding the high cost of the former and the rigidity of the latter. Toward this end, lean producers employ teams of multiskilled workers at all levels of the organization and use highly flexible, increasingly automated machines to produce volumes of products in enormous variety. Lean production (a term coined by IMVP researcher John Krafcik) is \"lean\" because it uses less of everything compared with mass production. Also, it requires keeping far less than half the needed inventory on site, results in many fewer defects, and produces a greater and ever growing variety of products. Lean production changes how people work. Most will find their jobs more challenging and will become more productive, but, at the same time, they may find their work more stressful. Lean production calls for learning far more professional skills (than in mass production) and applying these creatively in a team setting (rather than a rigid hierarchy). This book is organized in three sections. The first, \"The Origins of Lean Production,\" traces the evolution of lean production. The second, \"The Elements of Lean Production,\" looks at how lean production works in factory operations, product development, supply-system coordination, customer relations and as a total lean enterprise. Finally, in the third section, \"Diffusing Lean Production,\" the authors examine how lean production is spreading across the world and to other industries and, in the process, is revolutionizing how people live and work. Also examined are the barriers that are preventing companies and countries from becoming lean. Creative ways leanness can be achieved are suggested.\n",
            "\n",
            "Paper ID : 8ff61b8e097ccdb784a35b466ba9e130c2502513 \tArticle : Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond\n",
            "Author(s) : ['A. Atiya']\n",
            "Year : 2005 \n",
            "Abstract : Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.\n",
            "\n",
            "Paper ID : 40212e9474c3ddf3d8c6ffd13dd3211ec9406c49 \tArticle : Text Categorization with Support Vector Machines: Learning with Many Relevant Features\n",
            "Author(s) : ['T. Joachims']\n",
            "Year : 1998 \n",
            "Abstract : This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.\n",
            "\n",
            "Paper ID : e60ff004dde5c13ec53087872cfcdd12e85beb57 \tArticle : Learning Deep Architectures for AI\n",
            "Author(s) : ['Yoshua Bengio']\n",
            "Year : 2007 \n",
            "Abstract : Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.\n",
            "\n",
            "Paper ID : a675fe5a7d99ac6f7ff91fa084462faefe616148 \tArticle : What video games have to teach us about learning and literacy\n",
            "Author(s) : ['J. Gee']\n",
            "Year : 2003 \n",
            "Abstract : Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.\n",
            "\n",
            "Paper ID : 19bb0dce99466077e9bc5a2ad4941607fc28b40c \tArticle : Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples\n",
            "Author(s) : ['Mikhail Belkin', 'P. Niyogi', 'V. Sindhwani']\n",
            "Year : 2006 \n",
            "Abstract : We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.\n",
            "\n",
            "Paper ID : 4609f6bdc3beab00c9beceaa12dd8101fefe6f1c \tArticle : An overview of statistical learning theory\n",
            "Author(s) : ['V. Vapnik']\n",
            "Year : 1999 \n",
            "Abstract : Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).\n",
            "\n",
            "Paper ID : 22feb6532228392457664becc48b3096d9858505 \tArticle : Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory\n",
            "Author(s) : ['Sumio Watanabe']\n",
            "Year : 2010 \n",
            "Abstract : In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.\n",
            "\n",
            "Paper ID : 033ca7d9f4302a9349424f56c297657dd5d98616 \tArticle : Extreme Learning Machines [Trends & Controversies]\n",
            "Author(s) : ['E. Cambria', 'G. Huang', 'L. L. C. Kasun', 'Hongming Zhou', 'C. Vong', 'Jiarun Lin', 'Jianping Yin', 'Zhiping Cai', 'Qiang Liu', 'Kuan Li', 'Victor C. M. Leung', 'Liang Feng', 'Y. Ong', 'M. Lim', 'Anton Akusok', 'A. Lendasse', 'F. Corona', 'Rui Nian', 'Y. Miché', 'P. Gastaldo', 'R. Zunino', 'S. Decherchi', 'Xuefeng Yang', 'K. Mao', 'B. Oh', 'Je-Hyoung Jeon', 'K. Toh', 'A. Teoh', 'Jaihie Kim', 'Hanchao Yu', 'Yiqiang Chen', 'Junfa Liu']\n",
            "Year : 2013 \n",
            "Abstract : This special issue includes eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In \"Representational Learning with ELMs for Big Data,\" Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In \"A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing,\" Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C.M. Leung propose a method for handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In \"ELM-Guided Memetic Computation for Vehicle Routing,\" Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In \"ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs,\" Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In \"Combining ELMs with Random Projections,\" Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In \"Reduced ELMs for Causal Relation Extraction from Unstructured Text,\" Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In \"A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures,\" Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry for touchless applications without the need for handheld devices. Finally, in \"An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System,\" Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for touchless human-machine interaction.\n",
            "------------------------------------Extracting Page #98------------------------------------\n",
            "\n",
            "Paper ID : d4599b177559dd5ede4dda9d6d96aa149fc71942 \tArticle : An Efficient Learning Procedure for Deep Boltzmann Machines\n",
            "Author(s) : ['R. Salakhutdinov', 'Geoffrey E. Hinton']\n",
            "Year : 2012 \n",
            "Abstract : We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.\n",
            "\n",
            "Paper ID : 1fcbefeb0beae4470cf40df74cd116b1d4bdcae4 \tArticle : An introduction to kernel-based learning algorithms\n",
            "Author(s) : ['K. Müller', 'S. Mika', 'Gunnar Rätsch', 'K. Tsuda', 'B. Schölkopf']\n",
            "Year : 2001 \n",
            "Abstract : This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.\n",
            "\n",
            "Paper ID : a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657 \tArticle : A Learning Algorithm for Boltzmann Machines\n",
            "Author(s) : ['D. Ackley', 'Geoffrey E. Hinton', 'T. Sejnowski']\n",
            "Year : 1985 \n",
            "Abstract : \n",
            "\n",
            "Paper ID : fd8ce955dc0c570b66305dfbc65e4ed5f37658d0 \tArticle : Induction: Processes of Inference, Learning, and Discovery\n",
            "Author(s) : ['J. H. Holland', 'K. Holyoak', 'R. Nisbett', 'P. Thagard', 'S. Smoliar']\n",
            "Year : 1987 \n",
            "Abstract : Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.\n",
            "\n",
            "Paper ID : 248a297d786228a183fcae64023092660550fcd2 \tArticle : Learning to classify text using support vector machines - methods, theory and algorithms\n",
            "Author(s) : ['T. Joachims']\n",
            "Year : 2002 \n",
            "Abstract : Based on ideas from Support Vector Machines (SVMs), Learning To Classify Text Using Support Vector Machines presents a new approach to generating text classifiers from examples. The approach combines high performance and efficiency with theoretical understanding and improved robustness. In particular, it is highly effective without greedy heuristic components. The SVM approach is computationally efficient in training and classification, and it comes with a learning theory that can guide real-world applications. Learning To Classify Text Using Support Vector Machines gives a complete and detailed description of the SVM approach to learning text classifiers, including training algorithms, transductive text classification, efficient performance estimation, and a statistical learning model of text classification. In addition, it includes an overview of the field of text classification, making it self-contained even for newcomers to the field. This book gives a concise introduction to SVMs for pattern recognition, and it includes a detailed description of how to formulate text-classification tasks for machine learning.\n",
            "\n",
            "Paper ID : e219a61354d972a28954e655a7c53373508a08b6 \tArticle : Regularized multi--task learning\n",
            "Author(s) : ['T. Evgeniou', 'M. Pontil']\n",
            "Year : 2004 \n",
            "Abstract : Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.\n",
            "\n",
            "Paper ID : 02adea3455cd7b09e1dac9ddf2637a1e7ae84005 \tArticle : Inductive learning algorithms and representations for text categorization\n",
            "Author(s) : ['S. Dumais', 'John C. Platt', 'David Hecherman', 'M. Sahami']\n",
            "Year : 1998 \n",
            "Abstract : 1. ABSTRACT Text categorization – the assignment of natural language texts to one or more predefined categories based on their content – is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1\n",
            "\n",
            "Paper ID : 12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4 \tArticle : Online learning with kernels\n",
            "Author(s) : ['Jyrki Kivinen', 'Alex Smola', 'R. C. Williamson']\n",
            "Year : 2004 \n",
            "Abstract : Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.\n",
            "\n",
            "Paper ID : 77e379fd57ea44638fc628623e383eccada82689 \tArticle : Kernel Methods for Deep Learning\n",
            "Author(s) : ['Youngmin Cho', 'L. Saul']\n",
            "Year : 2009 \n",
            "Abstract : We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.\n",
            "\n",
            "Paper ID : 00cd1dab559a9671b692f39f14c1573ab2d1416b \tArticle : Efficient Learning of Deep Boltzmann Machines\n",
            "Author(s) : ['R. Salakhutdinov', 'H. Larochelle']\n",
            "Year : 2010 \n",
            "Abstract : We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.\n",
            "------------------------------------Extracting Page #99------------------------------------\n",
            "\n",
            "Paper ID : 0af75728bec67f698a8c619645165de13780c2fa \tArticle : Learning Multiple Tasks with Kernel Methods\n",
            "Author(s) : ['T. Evgeniou', 'C. Micchelli', 'M. Pontil']\n",
            "Year : 2005 \n",
            "Abstract : We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.\n",
            "\n",
            "Paper ID : 609e5cc1da126d7f760d1444b43b4fae41602841 \tArticle : Less is More: Active Learning with Support Vector Machines\n",
            "Author(s) : ['Greg Schohn', 'David A. Cohn']\n",
            "Year : 2000 \n",
            "Abstract : We describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classification tasks. We observe a number of benefits, the most surprising of which is that a SVM trained on a wellchosen subset of the available corpus frequently performs better than one trained on all available data. The heuristic for choosing this subset is simple to compute, and makes no use of information about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better performance with fewer data, it frequently does so in less time than the naive approach of training on all available data.\n",
            "\n",
            "Paper ID : 4673a47a4f6719e350196f4086a65d08f946df25 \tArticle : Learning by Design: Good Video Games as Learning Machines\n",
            "Author(s) : ['J. Gee']\n",
            "Year : 2005 \n",
            "Abstract : This article asks how good video and computer game designers manage to get new players to learn long, complex and difficult games. The short answer is that designers of good games have hit on excellent methods for getting people to learn and to enjoy learning. The longer answer is more complex. Integral to this answer are the good principles of learning built into successful games. The author discusses 13 such principles under the headings of ‘Empowered Learners’, ‘Problem Solving’ and ‘Understanding’ and concludes that the main impediment to implementing these principles in formal education is cost. This, however, is not only (or even so much) monetary cost. It is, importantly, the cost of changing minds about how and where learning is done and of changing one of our most profoundly change-resistant institutions: the school.\n",
            "\n",
            "Paper ID : 506f516d8b60ba74e5ce811a458ec4fd72d714b2 \tArticle : An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods\n",
            "Author(s) : ['Tong Zhang']\n",
            "Year : 2001 \n",
            "Abstract : This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.\n",
            "\n",
            "Paper ID : 424a6e62084d919bfc2e39a507c263e5991ebdad \tArticle : Self-Normalizing Neural Networks\n",
            "Author(s) : ['G. Klambauer', 'Thomas Unterthiner', 'Andreas Mayr', 'S. Hochreiter']\n",
            "Year : 2017 \n",
            "Abstract : Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are \"scaled exponential linear units\" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL.\n",
            "\n",
            "Paper ID : 3127190433230b3dc1abd0680bb58dced4bcd90e \tArticle : Large Scale Distributed Deep Networks\n",
            "Author(s) : ['J. Dean', 'G. Corrado', 'R. Monga', 'Kai Chen', 'Matthieu Devin', 'Quoc V. Le', 'Mark Z. Mao', \"Marc'Aurelio Ranzato\", 'A. Senior', 'P. Tucker', 'Ke Yang', 'A. Ng']\n",
            "Year : 2012 \n",
            "Abstract : Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.\n",
            "\n",
            "Paper ID : 6bdb186ec4726e00a8051119636d4df3b94043b5 \tArticle : Caffe: Convolutional Architecture for Fast Feature Embedding\n",
            "Author(s) : ['Yangqing Jia', 'Evan Shelhamer', 'Jeff Donahue', 'Sergey Karayev', 'Jonathan Long', 'Ross B. Girshick', 'S. Guadarrama', 'Trevor Darrell']\n",
            "Year : 2014 \n",
            "Abstract : Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.\n",
            "\n",
            "Paper ID : 7260c0692f8d265e11c4e9c4c8ef4c185bd587ad \tArticle : Building machines that learn and think like people\n",
            "Author(s) : ['B. Lake', 'T. Ullman', 'J. Tenenbaum', 'S. Gershman']\n",
            "Year : 2016 \n",
            "Abstract : Abstract Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.\n",
            "\n",
            "Paper ID : e50f4d3316d13841c287dcdf5479d7820d593571 \tArticle : Factorization Machines with libFM\n",
            "Author(s) : ['Steffen Rendle']\n",
            "Year : 2012 \n",
            "Abstract : Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.\n",
            " Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.\n",
            "\n",
            "Paper ID : 52e2ac397f0c8d5f533959905df899bc328d9f85 \tArticle : Reinforcement Learning with Hierarchies of Machines\n",
            "Author(s) : ['Ronald E. Parr', 'Stuart J. Russell']\n",
            "Year : 1997 \n",
            "Abstract : We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and \"behavior-based\" or \"teleo-reactive\" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.\n",
            "------------------------------------Extracting Page #100------------------------------------\n",
            "\n",
            "Paper ID : a538b05ebb01a40323997629e171c91aa28b8e2f \tArticle : Rectified Linear Units Improve Restricted Boltzmann Machines\n",
            "Author(s) : ['Vinod Nair', 'Geoffrey E. Hinton']\n",
            "Year : 2010 \n",
            "Abstract : Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.\n",
            "\n",
            "Paper ID : 62ed272e0e8b7be356c7f7595f5b7a22797a1c3e \tArticle : Support vector machines for classification and regression.\n",
            "Author(s) : ['R. Brereton', 'G. Lloyd']\n",
            "Year : 2010 \n",
            "Abstract : The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.\n",
            "\n",
            "Paper ID : 85021c84383d18a7a4434d76dc8135fc6bdc0aa6 \tArticle : Deep Boltzmann Machines\n",
            "Author(s) : ['R. Salakhutdinov', 'Geoffrey E. Hinton']\n",
            "Year : 2009 \n",
            "Abstract : We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.\n",
            "\n",
            "Paper ID : c834bddd5e75a64ca9bb80c195cf84345c38bb9b \tArticle : A Short Introduction to Boosting\n",
            "Author(s) : ['Y. Freund', 'R. Schapire']\n",
            "Year : 1999 \n",
            "Abstract : Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting’s relationship to support-vector machines. Some examples of recent applications of boosting are also described.\n",
            "\n",
            "Paper ID : 74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8 \tArticle : Transductive Inference for Text Classification using Support Vector Machines\n",
            "Author(s) : ['T. Joachims']\n",
            "Year : 1999 \n",
            "Abstract : This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.\n",
            "\n",
            "Paper ID : ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36 \tArticle : Introduction to Support Vector Machines\n",
            "Author(s) : ['D. Boswell']\n",
            "Year : 2002 \n",
            "Abstract : Support Vector Machines (SVM’s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM’s introduce the notion of a “kernel induced feature space” which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM’s is that the higher-dimensional space doesn’t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system’s likelihood to perform well on unseen data) of SVM’s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM’s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM’s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than “yes/no” classification).\n",
            "\n",
            "Paper ID : 1626c940a64ad96a7ed53d7d6c0df63c6696956b \tArticle : Restricted Boltzmann machines for collaborative filtering\n",
            "Author(s) : ['R. Salakhutdinov', 'A. Mnih', 'Geoffrey E. Hinton']\n",
            "Year : 2007 \n",
            "Abstract : Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.\n",
            "\n",
            "Paper ID : ff2c2e3e83d1e8828695484728393c76ee07a101 \tArticle : Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations\n",
            "Author(s) : ['D. Rumelhart', 'James L. McClelland']\n",
            "Year : 1986 \n",
            "Abstract : The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.\n",
            "\n",
            "Paper ID : de39028ea1122d09649a42b12e04c8f349ebe8a6 \tArticle : Autopoiesis and Cognition : The Realization of the Living (Boston Studies in the Philosophy of Scie\n",
            "Author(s) : ['H. Maturana', 'F. Varela']\n",
            "Year : 1980 \n",
            "Abstract : Editorial Preface General Table Of Contents Foreword Introduction (by Professor Maturana) Biology Of Cognition Dedication Table of Contents I. Introduction II. The Problem III. Cognitive Function in General A. The Observer B. The Living System C. Evolution D. The Cognitive Process IV. Cognitive Function in Particular A. Nerve Cells B. Architecture C. Function D. Representation E. Description F. Thinking G. Natural Language H. Memory and Learning I. The Observer V. Problems in the Neurophysiology of Cognition VI. Conclusions VII. Post Scriptum Autopoiesis: The Organization Of The Living Preface (by Sir Stafford Beer) Introduction I. On Machines, living and Otherwise 1. Machines 2. Living Machines II. Dispensability of Teleonomy 1. Purposelessness 2. Individuality III. Embodiments of Autopoiesis 1. Descriptive and Causal Notions 2. Molecular Embodiments 3. Origin IV. Diversity of Autopoiesis 1. Subordination to the Condition of Unity 2. Plasticity of Ontogeny 3. Reproduction, a Complication of the Unity 4. Evolution, a Historical Network 5. Second and Third Order Autopoietic Systems V. Presence of Autopoiesis 1. Biological Implications 2. Epistemological Implications 3. Cognitive Implications Appendix: The Nervous System Glossary Bibliography Index Of Names\n",
            "\n",
            "Paper ID : 9008cdacbdcff8a218a6928e94fe7c6dfc237b24 \tArticle : Training support vector machines: an application to face detection\n",
            "Author(s) : ['E. Osuna', 'R. Freund', 'F. Girosi']\n",
            "Year : 1997 \n",
            "Abstract : We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.\n",
            "------------------------------------Extracting Page #101------------------------------------\n",
            "\n",
            "Paper ID : b24972552161cd9eda729e748762a73430983e3a \tArticle : Least Squares Support Vector Machines\n",
            "Author(s) : ['J. Suykens', 'T. V. Gestel', 'J. D. Brabanter', 'B. Moor', 'J. Vandewalle']\n",
            "Year : 2002 \n",
            "Abstract : Support Vector Machines Basic Methods of Least Squares Support Vector Machines Bayesian Inference for LS-SVM Models Robustness Large Scale Problems LS-SVM for Unsupervised Learning LS-SVM for Recurrent Networks and Control.\n",
            "\n",
            "Paper ID : 1b4cf74a86d8cd2023928f15d9091e11020ac72f \tArticle : Feature Extraction - Foundations and Applications\n",
            "Author(s) : ['I. Guyon', 'S. Gunn', 'M. Nikravesh', 'L. Zadeh']\n",
            "Year : 2006 \n",
            "Abstract : An Introduction to Feature Extraction.- An Introduction to Feature Extraction.- Feature Extraction Fundamentals.- Learning Machines.- Assessment Methods.- Filter Methods.- Search Strategies.- Embedded Methods.- Information-Theoretic Methods.- Ensemble Learning.- Fuzzy Neural Networks.- Feature Selection Challenge.- Design and Analysis of the NIPS2003 Challenge.- High Dimensional Classification with Bayesian Neural Networks and Dirichlet Diffusion Trees.- Ensembles of Regularized Least Squares Classifiers for High-Dimensional Problems.- Combining SVMs with Various Feature Selection Strategies.- Feature Selection with Transductive Support Vector Machines.- Variable Selection using Correlation and Single Variable Classifier Methods: Applications.- Tree-Based Ensembles with Dynamic Soft Feature Selection.- Sparse, Flexible and Efficient Modeling using L 1 Regularization.- Margin Based Feature Selection and Infogain with Standard Classifiers.- Bayesian Support Vector Machines for Feature Ranking and Selection.- Nonlinear Feature Selection with the Potential Support Vector Machine.- Combining a Filter Method with SVMs.- Feature Selection via Sensitivity Analysis with Direct Kernel PLS.- Information Gain, Correlation and Support Vector Machines.- Mining for Complex Models Comprising Feature Selection and Classification.- Combining Information-Based Supervised and Unsupervised Feature Selection.- An Enhanced Selective Naive Bayes Method with Optimal Discretization.- An Input Variable Importance Definition based on Empirical Data Probability Distribution.- New Perspectives in Feature Extraction.- Spectral Dimensionality Reduction.- Constructing Orthogonal Latent Features for Arbitrary Loss.- Large Margin Principles for Feature Selection.- Feature Extraction for Classification of Proteomic Mass Spectra: A Comparative Study.- Sequence Motifs: Highly Predictive Features of Protein Function.\n",
            "\n",
            "Paper ID : d2d13bc44e15fd93480e16305d37c025bc0818c2 \tArticle : Regularization Networks and Support Vector Machines\n",
            "Author(s) : ['T. Evgeniou', 'M. Pontil', 'T. Poggio']\n",
            "Year : 2000 \n",
            "Abstract : Regularization Networks and Support Vector Machines are techniques for solving certain problems of learning from examples – in particular, the regression problem of approximating a multivariate function from sparse data. Radial Basis Functions, for example, are a special case of both regularization and Support Vector Machines. We review both formulations in the context of Vapnik's theory of statistical learning which provides a general foundation for the learning problem, combining functional analysis and statistics. The emphasis is on regression: classification is treated as a special case.\n",
            "\n",
            "Paper ID : 709129219b47929e102cc7858660c5ee495f2fa3 \tArticle : Support Vector Machines for Pattern Classification\n",
            "Author(s) : ['S. Abe']\n",
            "Year : 2005 \n",
            "Abstract : A guide on the use of SVMs in pattern classification, including a rigorous performance comparison of classifiers and regressors. The book presents architectures for multiclass classification and function approximation problems, as well as evaluation criteria for classifiers and regressors. Features: Clarifies the characteristics of two-class SVMs; Discusses kernel methods for improving the generalization ability of neural networks and fuzzy systems; Contains ample illustrations and examples; Includes performance evaluation using publicly available data sets; Examines Mahalanobis kernels, empirical feature space, and the effect of model selection by cross-validation; Covers sparse SVMs, learning using privileged information, semi-supervised learning, multiple classifier systems, and multiple kernel learning; Explores incremental training based batch training and active-set training methods, and decomposition techniques for linear programming SVMs; Discusses variable selection for support vector regressors.\n",
            "\n",
            "Paper ID : a53da9916b87fa295837617c16ef2ca6462cafb8 \tArticle : Classification using discriminative restricted Boltzmann machines\n",
            "Author(s) : ['H. Larochelle', 'Yoshua Bengio']\n",
            "Year : 2008 \n",
            "Abstract : Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.\n",
            "\n",
            "Paper ID : ceb5e9c07f2d95a700c1ed0813dfbae8c3901c18 \tArticle : Support Vector Machines for Classification and Regression\n",
            "Author(s) : ['S. Gunn']\n",
            "Year : 1998 \n",
            "Abstract : The foundations of Support Vector Machines (SVM) have been developed by Vapnik and are gaining popularity due to many attractive features, and promising empirical performance. The formulation embodies the Structural Risk Minimisation (SRM) principle, which in our work has been shown to be superior to traditional Empirical Risk Minimisation (ERM) principle employed by conventional neural networks. SRM minimises an upper bound on the VC dimension (generalisation error), as opposed to ERM which minimises the error on the training data. It is this difference which equips SVMs with a greater ability to generalise, which is our goal in statistical learning. SVMs were developed to solve the classification problem, but recently they have been extended to the domain of regression problems.\n",
            "\n",
            "Paper ID : 43c99a08e6e84d6fb75b0c8d03977fbf8e9dc402 \tArticle : Simulated annealing and Boltzmann machines - a stochastic approach to combinatorial optimization and neural computing\n",
            "Author(s) : ['E. Aarts', 'J. Korst']\n",
            "Year : 1990 \n",
            "Abstract : SIMULATED ANNEALING. Combinatorial Optimization. Simulated Annealing. Asymptotic Convergence. Finite-Time Approximation. Simulated Annealing in Practice. Parallel Simulated Annealing Algorithms. BOLTZMANN MACHINES. Neural Computing. Boltzmann Machines. Combinatorial Optimization and Boltzmann Machines. Classification and Boltzmann Machines. Learning and Boltzmann Machines. Appendix. Bibliography. Indices.\n",
            "\n",
            "Paper ID : 2e124658d8a5888a434e83b183d46e28a12275cd \tArticle : Learning by design: Games as learning machines\n",
            "Author(s) : ['J. Gee']\n",
            "Year : 2004 \n",
            "Abstract : , are long, complex, and difficult, especially for beginners. People are not always eager to do difficult things. Faced with the challenge of getting them to do so, two choices are often available. We can force them, which is the solution schools use. Or, a temptation when profit is at stake, we can dumb down the product. Neither option is open to the game industry, at least for the moment. They can’t force people to play and most avid players don’t want their games dumbed down. For people interested in learning, this raises an interesting question. How do good game designers manage to get new players to learn their long, complex, and difficult games—not only learn them, but pay to do so? It won’t do simply to say games are “motivating”. That just begs the question of “Why?”. Why is a long, complex, and difficult game motivating? I believe it is something about how games are designed to trigger learning that makes them so deeply motivating. So the question is: How do good game designers manage to get new players to learn long, complex, and difficult games? Of course, there are some forces in the game industry that want to dumb games down. That is not a very interesting answer to our question. Another answer that is not interesting, at least initially, is that some good games appear to be made only for people who are already adept game players. These games can be uninviting or frustrating for newcomers. Some thoroughly excellent games that fall into this category are\n",
            "\n",
            "Paper ID : bdb8f3288cb65b463a8e7aad557d3f1bb3ff5e25 \tArticle : Style machines\n",
            "Author(s) : ['M. Brand', 'Aaron Hertzmann']\n",
            "Year : 2000 \n",
            "Abstract : We approach the problem of stylistic motion synthesis by learning motion patterns from a highly varied set of motion capture sequences. Each sequence may have a distinct choreography, performed in a distinct sytle. Learning identifies common choreographic elements across sequences, the different styles in which each element is performed, and a small number of stylistic degrees of freedom which span the many variations in the dataset. The learned model can synthesize novel motion data in any interpolation or extrapolation of styles. For example, it can convert novice ballet motions into the more graceful modern dance of an expert. The model can also be driven by video, by scripts or even by noise to generate new choreography and synthesize virtual motion-capture in many styles.\n",
            "\n",
            "Paper ID : 33d49e6fc8755a35ec909c884b8fe8c22b706a4c \tArticle : On the Synthesis of Finite-State Machines from Samples of Their Behavior\n",
            "Author(s) : ['A. Biermann', 'J. Feldman']\n",
            "Year : 1972 \n",
            "Abstract : The Nerode realization technique for synthesizing finite-state machines from their associated right-invariant equivalence relations is modified to give a method for synthesizing machines from finite subsets of their input-output behavior. The synthesis procedure includes a parameter that one may adjust to obtain machines that represent the desired behavior with varying degrees of accuracy and that consequently have varying complexities. We discuss some of the uses of the method, including an application to a sequential learning problem.\n",
            "------------------------------------Extracting Page #102------------------------------------\n",
            "------------------------------------Extracting Page #103------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-201846cc3bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'-' * 36}Extracting Page #{item}{'-' * 36}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.semanticscholar.org/api/1/search'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mju\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Iterating and Printing the information of the articles:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'results'"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import requests\n",
        "import json\n",
        "\n",
        "data = {\n",
        "    \"authors\": [],\n",
        "    \"coAuthors\": [],\n",
        "    \"externalContentTypes\": [],\n",
        "    \"pageSize\": 10,\n",
        "    \"publicationTypes\": [],\n",
        "    \"queryString\": \"Machine Learning\",\n",
        "    \"requireViewablePdf\": False,\n",
        "    \"sort\": \"relevance\",\n",
        "    \"venues\": [],\n",
        "    \"yearFilter\": None\n",
        "}\n",
        "\n",
        "\n",
        "# function to traverse the list of authors in a paper\n",
        "\n",
        "\n",
        "def information_retreival(x):\n",
        "    if isinstance(x, list):\n",
        "        for v in x:\n",
        "            information_retreival(v)\n",
        "    elif isinstance(x, dict):\n",
        "        if 'name' in x:\n",
        "            authors.append(x['name'])\n",
        "        else:\n",
        "            for v in x.values():\n",
        "                information_retreival(v)\n",
        "\n",
        "\n",
        "# Looping over the pages\n",
        "\n",
        "\n",
        "for item in range(1, 1000):  # increase the range size here to extract more pages\n",
        "    data['page'] = item\n",
        "    print(f\"{'-' * 36}Extracting Page #{item}{'-' * 36}\")\n",
        "    request = requests.post('https://www.semanticscholar.org/api/1/search', json=data).json()\n",
        "    ju = request['results']\n",
        "\n",
        "    # Iterating and Printing the information of the articles:\n",
        "    for values in ju:\n",
        "        print(\"\\nPaper ID :\", values['id'], \"\\tArticle :\", values['title']['text'])\n",
        "        authors = []\n",
        "        information_retreival(values['authors'])\n",
        "        print(\"Author(s) :\", authors)\n",
        "        print(\"Year :\", values['year']['text'], \"\\nAbstract :\", values['paperAbstract']['text'])\n",
        "\n",
        "\n",
        "# Pages info\n",
        "num_page = request['totalPages']\n",
        "print(num_page)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBwpHWVaLUqU"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "E0sjEahWLUqU"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run ./keys.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wr7cv8tuXr2",
        "outputId": "729beff7-3352-4a3b-fa7e-9af6736b0128"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:File `'./keys.ipynb.py'` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_token = ''\n",
        "access_token_secret = ''\n"
      ],
      "metadata": {
        "id": "1SigbWNEvL4l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
        "auth.set_access_token(access_token,access_token_secret)\n",
        "api = tweepy.API(auth)"
      ],
      "metadata": {
        "id": "tfD1IrsjudWH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_tweets = 1000\n",
        "tweets = []\n",
        "likes = []\n",
        "time = []\n",
        "\n",
        "for i in tweepy.Cursor(api.search,q=\"#unitedAIRLINES\",count=100,lang=\"en\",since=\"2017-04-03\").items(number_of_tweets):\n",
        "  tweets.append(i.full_text)\n",
        "  likes.append(i.favorite_count)\n",
        "  time.append(i.created_at)"
      ],
      "metadata": {
        "id": "-daCo_JDC53e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}