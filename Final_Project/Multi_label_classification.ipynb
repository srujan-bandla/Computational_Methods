{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srujan-bandla/srujan_INFO5731_Fall2022/blob/main/Final_Project/Multi_label_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f352d12",
      "metadata": {
        "id": "3f352d12",
        "outputId": "c9227339-3947-49b1-efe9-8562ce3e8fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No columns to parse from file\n",
            "(3137, 3)\n"
          ]
        }
      ],
      "source": [
        "# merging the dataset\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "annotation_list_df = []\n",
        "#annotation_df = pd.DataFrame()\n",
        "\n",
        "os.chdir(\"C:\\\\Users\\\\sruja\\\\OneDrive\\\\Desktop\\\\Annotated files\")\n",
        "\n",
        "for file in os.listdir('C:\\\\Users\\\\sruja\\\\OneDrive\\\\Desktop\\\\Annotated files'):\n",
        "    #print(file)\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        annotation_list_df.append(df)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "annotation_df = pd.concat(annotation_list_df)\n",
        "print(annotation_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a05a510",
      "metadata": {
        "id": "8a05a510",
        "outputId": "44bf605e-bf75-4a60-e70a-9998f6607398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3132, 3)\n"
          ]
        }
      ],
      "source": [
        "annotation_df = annotation_df.dropna()\n",
        "\n",
        "#annotation_df_1 = annotation_df.iloc[:10 ,]\n",
        "print(annotation_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36318ca9",
      "metadata": {
        "id": "36318ca9",
        "outputId": "7d854797-640a-4154-ad34-262dab21cea0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2721, 2)\n"
          ]
        }
      ],
      "source": [
        "annotation_df = annotation_df.drop(['id'], axis = 1)\n",
        "annotation_df = annotation_df.drop_duplicates()\n",
        "\n",
        "#annotation_df_1 = annotation_df.iloc[:10 ,]\n",
        "print(annotation_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bce704a",
      "metadata": {
        "id": "2bce704a"
      },
      "outputs": [],
      "source": [
        "Pre_war_background_to_the_incarceration = []\n",
        "Governments_decision_to_remove_ethnic_Japanese = []\n",
        "Life_after_removal_and_during_the_incarceration = []\n",
        "Military_services = []\n",
        "Returning_of_Japanese_American_after_WWII = []\n",
        "Legal_challenges = []\n",
        "Redress_movement = []\n",
        "\n",
        "\n",
        "for i in range(annotation_df.shape[0]):\n",
        "    \n",
        "    if \"Pre-war background to the incarceration\" in annotation_df.iloc[i, 1]:\n",
        "        #roopesh_df.loc[i, \"Pre_war_background_to_the_incarceration\"] = 1\n",
        "        Pre_war_background_to_the_incarceration.append(1)\n",
        "    else:\n",
        "        Pre_war_background_to_the_incarceration.append(0)\n",
        "    \n",
        "    if \"Government’s decision to remove ethnic Japanese\" in annotation_df.iloc[i, 1]:\n",
        "        Governments_decision_to_remove_ethnic_Japanese.append(1)\n",
        "    else:\n",
        "        Governments_decision_to_remove_ethnic_Japanese.append(0)        \n",
        "        \n",
        "    if \"Life after removal and during the incarceration\" in annotation_df.iloc[i, 1]:\n",
        "        Life_after_removal_and_during_the_incarceration.append(1)\n",
        "    else:\n",
        "        Life_after_removal_and_during_the_incarceration.append(0)\n",
        "        \n",
        "    if \"Military services\" in annotation_df.iloc[i, 1]:\n",
        "        Military_services.append(1)\n",
        "    else:\n",
        "        Military_services.append(0)\n",
        "        \n",
        "    if \"Returning of Japanese American after WWII\" in annotation_df.iloc[i, 1]:\n",
        "        Returning_of_Japanese_American_after_WWII.append(1)\n",
        "    else:\n",
        "        Returning_of_Japanese_American_after_WWII.append(0)\n",
        "        \n",
        "    if \"Legal challenges\" in annotation_df.iloc[i, 1]:\n",
        "        Legal_challenges.append(1)\n",
        "    else:\n",
        "        Legal_challenges.append(0)\n",
        "        \n",
        "    if \"Redress movement\" in annotation_df.iloc[i, 1]:\n",
        "        Redress_movement.append(1)\n",
        "    else:\n",
        "        Redress_movement.append(0)\n",
        "    \n",
        "    \n",
        "    \n",
        "annotation_df['Pre_war_background_to_the_incarceration'] = Pre_war_background_to_the_incarceration\n",
        "annotation_df[\"Government’s_decision_to_remove_ethnic_Japanese\"] = Governments_decision_to_remove_ethnic_Japanese\n",
        "annotation_df['Life_after_removal_and_during_the_incarceration'] = Life_after_removal_and_during_the_incarceration\n",
        "annotation_df['Military_services'] = Military_services\n",
        "annotation_df['Returning_of_Japanese_American_after_WWII'] = Returning_of_Japanese_American_after_WWII\n",
        "annotation_df['Legal_challenges'] = Legal_challenges\n",
        "annotation_df['Redress_movement'] = Redress_movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f99408",
      "metadata": {
        "id": "03f99408",
        "outputId": "019b5baf-4ead-4eba-d10e-95e9b4cf2cf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\sruja\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\sruja\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\sruja\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6905ec2",
      "metadata": {
        "id": "d6905ec2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "cleaned_text = []\n",
        "for i in annotation_df['text']:\n",
        "    review = re.sub('[^a-zA-Z]', ' ', i)\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [lemmatizer.lemmatize(word) for word in review if (not word in set(stopwords) and len(lemmatizer.lemmatize(word)) > 1)]\n",
        "    review = ' '.join(review)\n",
        "    cleaned_text.append(review)\n",
        "    \n",
        "annotation_df['Cleaned_text'] = cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0df825",
      "metadata": {
        "id": "db0df825"
      },
      "outputs": [],
      "source": [
        "x = annotation_df['Cleaned_text']\n",
        "y = annotation_df.drop(['text', 'label', 'Cleaned_text'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ee02dd",
      "metadata": {
        "id": "55ee02dd"
      },
      "outputs": [],
      "source": [
        "tf_idf = TfidfVectorizer(ngram_range=(1, 3))\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=42, test_size=0.25)\n",
        "\n",
        "X_train_tf = tf_idf.fit_transform(xtrain)\n",
        "X_test_tf = tf_idf.transform(xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd09e3ba",
      "metadata": {
        "id": "cd09e3ba"
      },
      "source": [
        "# BinaryRelevance model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d6f7a9",
      "metadata": {
        "id": "44d6f7a9",
        "outputId": "153518e3-e20e-487f-adc8-f9c78191736d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: scikit-multilearn in c:\\users\\sruja\\appdata\\roaming\\python\\python39\\site-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87a3095e",
      "metadata": {
        "id": "87a3095e",
        "outputId": "b83650f9-811e-4c36-82db-c89906f012e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.39500734214390604\n",
            "F1 score =  0.5699745547073791\n",
            "Hamming loss =  0.17726033144535347\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "br_classifier = BinaryRelevance(classifier = gnb)\n",
        "\n",
        "br_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = br_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568ed88b",
      "metadata": {
        "id": "568ed88b"
      },
      "source": [
        "# ClassifierChain model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b78746",
      "metadata": {
        "id": "06b78746",
        "outputId": "bcd8ae2f-0e4f-423b-d762-ad2923f14faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.3964757709251101\n",
            "F1 score =  0.5715741212429954\n",
            "Hamming loss =  0.17642122928466542\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "chain_classifier = ClassifierChain(classifier = gnb)\n",
        "\n",
        "chain_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = chain_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffafe684",
      "metadata": {
        "id": "ffafe684"
      },
      "source": [
        "# Label PowerSet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305c64f7",
      "metadata": {
        "id": "305c64f7",
        "outputId": "deef4b4a-bc2a-4807-cdda-23e2d8b084c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.434654919236417\n",
            "F1 score =  0.5649717514124294\n",
            "Hamming loss =  0.14537444933920704\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "labpower_classifier = LabelPowerset(DecisionTreeClassifier(max_depth = 200))\n",
        "\n",
        "labpower_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = labpower_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fbd784",
      "metadata": {
        "id": "80fbd784",
        "outputId": "9490ddc3-5f58-48f9-d061-6df95aafc3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.3891336270190896\n",
            "F1 score =  0.5307644499689248\n",
            "Hamming loss =  0.15838053282987205\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "labpower_classifier = LabelPowerset(DecisionTreeClassifier(max_depth = 200, criterion='entropy'))\n",
        "\n",
        "labpower_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = labpower_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434e4213",
      "metadata": {
        "id": "434e4213",
        "outputId": "d56803ad-9f22-405a-c858-b58f888d06d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy =  0.39500734214390604\n",
            "F1 score =  0.6169607285145134\n",
            "Hamming loss =  0.14117893853576674\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# initialize Label Powerset multi-label classifier\n",
        "# with a gaussian naive bayes base classifier\n",
        "labpower_classifier = LabelPowerset(GaussianNB())\n",
        "\n",
        "labpower_classifier.fit(X_train_tf, ytrain)\n",
        "ypred = labpower_classifier.predict(X_test_tf)\n",
        "\n",
        "print(\"Accuracy = \",accuracy_score(ytest, ypred))\n",
        "print(\"F1 score = \",f1_score(ytest, ypred, average=\"micro\"))\n",
        "print(\"Hamming loss = \",hamming_loss(ytest, ypred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}